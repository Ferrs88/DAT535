{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a8c8933",
   "metadata": {},
   "source": [
    "# Predicting Salary Trends Using Multi-Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d03754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e950ad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions that we use futher in the code\n",
    "\n",
    "def safe_make_dir(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def year_from_path(path, base_dir):\n",
    "    rel = os.path.relpath(path, start=base_dir)\n",
    "    parts = rel.split(os.sep)\n",
    "    for part in parts:\n",
    "        if len(part) == 4 and part.isdigit():\n",
    "            return int(part)\n",
    "    for part in parts:\n",
    "        if len(part) >= 4 and part[:4].isdigit():\n",
    "            return int(part[:4])\n",
    "    return None\n",
    "\n",
    "\n",
    "def load_gender_dict(gender_from_names_file):\n",
    "    if not os.path.exists(gender_from_names_file):\n",
    "        return {}\n",
    "    with open(gender_from_names_file, \"r\") as f:\n",
    "        gender_data = json.load(f)\n",
    "    return {item[\"name\"].strip().lower(): item[\"gender\"] for item in gender_data}\n",
    "\n",
    "\n",
    "def get_gender_from_name_factory(gender_dict):\n",
    "    def _inner(name):\n",
    "        if not isinstance(name, str) or not name:\n",
    "            return \"unknown\"\n",
    "        return gender_dict.get(name.strip().lower(), \"unknown\").capitalize()\n",
    "    return _inner\n",
    "\n",
    "\n",
    "def load_country_iso_dict(country_iso_file):\n",
    "    if not os.path.exists(country_iso_file):\n",
    "        return {}, []\n",
    "    with open(country_iso_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        country_iso_data = data.get(\"3166-1\", [])\n",
    "    country_iso_dict = {\n",
    "        item[\"name\"].strip().lower(): item[\"alpha_2\"]\n",
    "        for item in country_iso_data\n",
    "        if \"name\" in item and \"alpha_2\" in item\n",
    "    }\n",
    "    alpha3_codes = [item[\"alpha_3\"] for item in country_iso_data if \"alpha_3\" in item]\n",
    "    return country_iso_dict, alpha3_codes\n",
    "\n",
    "\n",
    "def get_country_iso_factory(country_iso_dict):\n",
    "    def _inner(name):\n",
    "        if not isinstance(name, str) or not name:\n",
    "            return \"unknown\"\n",
    "        return country_iso_dict.get(name.strip().lower(), \"unknown\")\n",
    "    return _inner\n",
    "\n",
    "\n",
    "def get_random_country_iso_factory(country_iso_data):\n",
    "    def _inner():\n",
    "        if not country_iso_data:\n",
    "            return \"unknown\"\n",
    "        return random.choice(country_iso_data).get(\"alpha_2\", \"unknown\")\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e53293",
   "metadata": {},
   "source": [
    "Convert XLS to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677adec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = \"data/PART_0_xls/\"\n",
    "output_directory = \"data/PART_1_xls_to_csv/\"\n",
    "scan_depth = 300  # how many rows to scan to find the header\n",
    "\n",
    "safe_make_dir(output_directory)\n",
    "\n",
    "for root, _, files in os.walk(input_directory):\n",
    "    for file in files:\n",
    "        if not (file.lower().endswith(\".xls\") or file.lower().endswith(\".xlsx\")):\n",
    "            continue\n",
    "        if file.lower() in [\"field_descriptions.xls\", \"field_descriptions.xlsx\"]:\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(root, file)\n",
    "        save_dir = os.path.join(output_directory, os.path.relpath(root, input_directory))\n",
    "        safe_make_dir(save_dir)\n",
    "\n",
    "        engine = \"openpyxl\" if file.lower().endswith(\".xlsx\") else \"xlrd\"\n",
    "        xls = pd.ExcelFile(file_path, engine=engine)\n",
    "\n",
    "        for sheet in xls.sheet_names:\n",
    "            if sheet.lower() in [\"field descriptions\", \"updatetime\"]:\n",
    "                continue\n",
    "\n",
    "            df_raw = pd.read_excel(file_path, sheet_name=sheet, header=None, engine=engine)\n",
    "\n",
    "            if df_raw.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            header_row = 0\n",
    "            for i in range(min(scan_depth, df_raw.shape[0])):\n",
    "                row_values = [str(x).strip().lower() for x in df_raw.iloc[i]]\n",
    "                if \"occ_code\" in row_values:\n",
    "                    header_row = i\n",
    "                    break\n",
    "\n",
    "            header = df_raw.iloc[header_row].astype(str).str.strip()\n",
    "            df = df_raw.iloc[header_row + 1 :].copy()\n",
    "            df.columns = [str(x).strip() for x in header]\n",
    "\n",
    "            suffix = f\"_{sheet}\" if len(xls.sheet_names) > 1 else \"\"\n",
    "            out_path = os.path.join(save_dir, f\"{os.path.splitext(file)[0]}{suffix}.csv\")\n",
    "\n",
    "            df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"Conversion completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf684fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"data/PART_1_xls_to_csv/\"\n",
    "out_dir = \"data/PART_2_xls_to_csv_combined_header/\"\n",
    "\n",
    "safe_make_dir(out_dir)\n",
    "\n",
    "def collect_csv_files(base_dir, ext=\".csv\", **kwargs):\n",
    "    file_infos = []\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if not file.lower().endswith(ext.lower()):\n",
    "                continue\n",
    "            path = os.path.join(root, file)\n",
    "            try:\n",
    "                df = pd.read_csv(path, **kwargs)\n",
    "                df.columns = [col.strip().lower() for col in df.columns]\n",
    "                header = tuple(df.columns)\n",
    "                year = year_from_path(path, base_dir)\n",
    "                file_infos.append({\"path\": path, \"df\": df, \"header\": header, \"year\": year})\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {path}: {e}\")\n",
    "    return file_infos\n",
    "\n",
    "\n",
    "\n",
    "def check_header_differences(file_infos):\n",
    "    if not file_infos:\n",
    "        print(\"No CSV files found.\")\n",
    "        return\n",
    "    header_map = {}\n",
    "    for info in file_infos:\n",
    "        header_map.setdefault(info[\"header\"], []).append(info[\"path\"])\n",
    "    headers = list(header_map.keys())\n",
    "    base = headers[0]\n",
    "    if len(headers) == 1:\n",
    "        print(\"All CSV files have the same (lowercased) header format\")\n",
    "        return\n",
    "    print(\"Differences compared to the first header:\\n\")\n",
    "    print(\"Baseline header:\", base, \"\\n\")\n",
    "    base_set = set(base)\n",
    "    for i, hdr in enumerate(headers[1:], start=2):\n",
    "        missing = sorted(base_set - set(hdr))\n",
    "        extra = sorted(set(hdr) - base_set)\n",
    "        print(f\"Header format #{i}: {hdr}\")\n",
    "        if missing:\n",
    "            print(f\"  Missing columns: {missing}\")\n",
    "        if extra:\n",
    "            print(f\"  Extra columns:   {extra}\")\n",
    "        print(\"Files:\")\n",
    "        for p in header_map[hdr]:\n",
    "            print(f\" - {p}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "\n",
    "def combine_same_format_files(file_infos, out_dir, base_dir, add_source=True):\n",
    "    safe_make_dir(out_dir)\n",
    "    groups = {}\n",
    "    for info in file_infos:\n",
    "        df = info[\"df\"]\n",
    "        header = list(info[\"header\"])\n",
    "        cols_with_year = list(header)\n",
    "        if \"year\" not in cols_with_year:\n",
    "            cols_with_year.append(\"year\")\n",
    "        key = frozenset(cols_with_year)\n",
    "        if key not in groups:\n",
    "            order = list(header)\n",
    "            if \"year\" not in order:\n",
    "                order.append(\"year\")\n",
    "            groups[key] = {\"order\": order, \"files\": []}\n",
    "        groups[key][\"files\"].append(info)\n",
    "    if not groups:\n",
    "        print(\"No CSV files found.\")\n",
    "        return {}\n",
    "    outputs = {}\n",
    "    for idx, (key, info) in enumerate(groups.items(), start=1):\n",
    "        order = info[\"order\"]\n",
    "        out_path = os.path.join(out_dir, f\"group_{idx}.csv\")\n",
    "        frames = []\n",
    "        for file_info in info[\"files\"]:\n",
    "            df = file_info[\"df\"].copy()\n",
    "            if \"year\" not in df.columns:\n",
    "                yr = file_info[\"year\"]\n",
    "                df[\"year\"] = yr if yr is not None else pd.NA\n",
    "            if add_source:\n",
    "                df[\"__source_file\"] = os.path.relpath(file_info[\"path\"], start=base_dir)\n",
    "            cols = order + ([\"__source_file\"] if add_source else [])\n",
    "            df = df.reindex(columns=cols)\n",
    "            frames.append(df)\n",
    "        if not frames:\n",
    "            print(f\"(skip) No readable files for group #{idx}\")\n",
    "            continue\n",
    "        combined = pd.concat(frames, ignore_index=True)\n",
    "        combined.to_csv(out_path, index=False)\n",
    "        outputs[out_path] = [fi[\"path\"] for fi in info[\"files\"]]\n",
    "        print(f\"Wrote {len(combined)} rows from {len(info['files'])} files -> {out_path}\")\n",
    "    return outputs\n",
    "\n",
    "file_infos = collect_csv_files(data)\n",
    "check_header_differences(file_infos)\n",
    "combine_same_format_files(file_infos, out_dir, base_dir=data, add_source=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f00db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = []\n",
    "input_directory = 'data/PART_2_1_CSV_EMPLOYEE_SALARIES'\n",
    "output_file = 'data/PART_3_csv/combined_employee_salaries.csv'\n",
    "\n",
    "for filename in os.listdir(input_directory):\n",
    "    if not filename.endswith('.csv'):\n",
    "        continue\n",
    "    file_path = os.path.join(input_directory, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # 2020 Overtime Pay; 2021 Overtime Pay; 2019 Overtime Pay; 2024 Overtime Pay; Overtime Pay; Overtime_Pay\n",
    "    # 2024 Longevity Pay; 2019 Longevity Pay; 2021 Longevity Pay; 2020 Longevity Pay; Longevity_Pay\n",
    "\n",
    "    for col in df.columns:\n",
    "        column = col.lower()\n",
    "        if column in ['2020 overtime pay', '2021 overtime pay', '2019 overtime pay', '2024 overtime pay', 'overtime pay', 'overtime_pay']: # rename to unified column\n",
    "            df.rename(columns={col: 'OvertimePay'}, inplace=True)\n",
    "        if column in ['2024 longevity pay', '2019 longevity pay', '2021 longevity pay', '2020 longevity pay', 'longevity pay', 'longevity_pay']:\n",
    "            df.rename(columns={col: 'LongevityPay'}, inplace=True)\n",
    "        if column in ['department name', 'department_name']:\n",
    "            df.rename(columns={col: 'DepartmentName'}, inplace=True)\n",
    "        if column in ['base salary', 'base_salary']:\n",
    "            df.rename(columns={col: 'BaseSalary'}, inplace=True)\n",
    "        if 'year' not in df.columns: # TODO: perhaps remake it so that it's not hardcoded\n",
    "            if '2019' in filename:\n",
    "                df['Year'] = 2019\n",
    "            elif '2020' in filename:\n",
    "                df['Year'] = 2020\n",
    "            elif '2021' in filename:\n",
    "                df['Year'] = 2021\n",
    "            elif '2022' in filename:\n",
    "                df['Year'] = 2022\n",
    "            elif '2023' in filename:\n",
    "                df['Year'] = 2023\n",
    "            elif '2024' in filename:\n",
    "                df['Year'] = 2024\n",
    "            else:\n",
    "                df['Year'] = None\n",
    "    dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "print (\"Summary of combined data:\")\n",
    "print (combined_df.info())\n",
    "combined_df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c58779",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = []\n",
    "input_directory = 'data/PART_2_2_SALARIES'\n",
    "output_file = 'data/PART_3_csv/combined_salaries.csv'\n",
    "\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(input_directory, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "print (\"Summary of combined data:\")\n",
    "print (combined_df.info())\n",
    "combined_df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e31c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"data/PART_2_xls_to_csv_combined_header/\"\n",
    "combined_output_path = \"data/PART_3_csv/all_combined_data.csv\"\n",
    "\n",
    "def _coalesce_into(df, target, src, changes): # helper to coalesced and rename columns\n",
    "    if src not in df.columns:\n",
    "        return\n",
    "    if target in df.columns:\n",
    "        df[target] = df[target].where(df[target].notna(), df[src])\n",
    "        df.drop(columns=[src], inplace=True)\n",
    "        changes.append(f\"{src} → {target} (coalesced)\")\n",
    "    else:\n",
    "        df.rename(columns={src: target}, inplace=True)\n",
    "        changes.append(f\"{src} → {target} (renamed)\")\n",
    "\n",
    "\n",
    "def unify_schema_in_df(df):\n",
    "    changes = []\n",
    "    df = df.copy()\n",
    "\n",
    "    _coalesce_into(df, \"occ_title\", \"occ_titl\", changes) # combine similar columns\n",
    "    _coalesce_into(df, \"group\", \"o_group\", changes)\n",
    "    _coalesce_into(df, \"group\", \"occ_group\", changes)\n",
    "\n",
    "    cols = list(df.columns)\n",
    "    for c in cols:\n",
    "        if c.startswith(\"a_wpct\"):\n",
    "            target = \"a_pct\" + c[len(\"a_wpct\"):]\n",
    "            _coalesce_into(df, target, c, changes)\n",
    "    cols = list(df.columns)\n",
    "    for c in cols:\n",
    "        if c.startswith(\"h_wpct\"):\n",
    "            target = \"h_pct\" + c[len(\"h_wpct\"):]\n",
    "            _coalesce_into(df, target, c, changes)\n",
    "\n",
    "    return df, changes\n",
    "\n",
    "\n",
    "def collect_csv_files(base_dir, **read_csv_kwargs):\n",
    "    file_infos = []\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if not file.lower().endswith(\".csv\"):\n",
    "                continue\n",
    "            path = os.path.join(root, file)\n",
    "            try:\n",
    "                df = pd.read_csv(path, **read_csv_kwargs)\n",
    "                df.columns = [col.strip().lower() for col in df.columns]\n",
    "                header = tuple(df.columns)\n",
    "                file_infos.append({\"path\": path, \"df\": df, \"header\": header})\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {path}: {e}\")\n",
    "    return file_infos\n",
    "\n",
    "\n",
    "def build_header_map(file_infos):\n",
    "    header_map = {}\n",
    "    for info in file_infos:\n",
    "        header_map.setdefault(info[\"header\"], []).append(info[\"path\"]) # group files by header\n",
    "    return header_map\n",
    "\n",
    "\n",
    "def check_header_differences(header_map):\n",
    "    if not header_map:\n",
    "        print(\"No CSV files found.\")\n",
    "        return\n",
    "\n",
    "    headers = list(header_map.keys())\n",
    "    base = headers[0]\n",
    "    if len(headers) == 1:\n",
    "        print(\"--> All CSV files have the same (lowercased) header format.\")\n",
    "        return\n",
    "\n",
    "    print(\"--> Differences compared to the first header:\\n\")\n",
    "    print(\"Baseline header:\", base, \"\\n\")\n",
    "    base_set = set(base)\n",
    "\n",
    "    for i, hdr in enumerate(headers[1:], start=2):\n",
    "        missing = sorted(base_set - set(hdr)) # find missing columns compared to baseline\n",
    "        extra = sorted(set(hdr) - base_set) # find extra columns compared to baseline\n",
    "        print(f\"Header format #{i}: {hdr}\")\n",
    "        if missing:\n",
    "            print(f\"  Missing columns: {missing}\")\n",
    "        if extra:\n",
    "            print(f\"  Extra columns:   {extra}\")\n",
    "        print(\"Files:\")\n",
    "        for p in header_map[hdr]:\n",
    "            print(f\" - {p}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "def expand_to_include_extra_columns(file_infos):\n",
    "    if not file_infos:\n",
    "        return\n",
    "\n",
    "    headers = [info[\"header\"] for info in file_infos]\n",
    "    base = list(headers[0]) # use the first header as the base\n",
    "    base_set = set(base) # convert base header to set\n",
    "\n",
    "    any_missing_from_base = False\n",
    "    extra_cols_in_order = []\n",
    "    extra_seen = set()\n",
    "\n",
    "    for hdr in headers[1:]:\n",
    "        hdr_list = list(hdr)\n",
    "        hdr_set = set(hdr_list) # convert current header to set\n",
    "\n",
    "        missing = base_set - hdr_set # find missing columns compared to baseline\n",
    "        if missing:\n",
    "            any_missing_from_base = True\n",
    "\n",
    "        for c in hdr_list:\n",
    "            if c not in base_set and c not in extra_seen:\n",
    "                extra_cols_in_order.append(c)\n",
    "                extra_seen.add(c)\n",
    "\n",
    "    if any_missing_from_base or not extra_cols_in_order:\n",
    "        return\n",
    "\n",
    "    full_header = base + extra_cols_in_order\n",
    "\n",
    "    for info in file_infos:\n",
    "        df = info[\"df\"].copy()\n",
    "        for col in full_header:\n",
    "            if col not in df.columns:\n",
    "                df[col] = pd.NA\n",
    "        df = df[full_header]\n",
    "        info[\"df\"] = df\n",
    "        info[\"header\"] = tuple(full_header)\n",
    "\n",
    "\n",
    "def unify_dirs(dirs, **read_csv_kwargs):\n",
    "    if isinstance(dirs, str):\n",
    "        dirs = [dirs]\n",
    "\n",
    "    all_infos = []\n",
    "\n",
    "    for base_dir in dirs:\n",
    "        print(f\"\\n=== Directory: {base_dir} ===\")\n",
    "        file_infos = collect_csv_files(base_dir, **read_csv_kwargs)\n",
    "\n",
    "        print(\"Before:\")\n",
    "        pre_map = build_header_map(file_infos)\n",
    "        check_header_differences(pre_map)\n",
    "\n",
    "        for info in file_infos:\n",
    "            path = info[\"path\"]\n",
    "            df = info[\"df\"]\n",
    "            try:\n",
    "                new_df, changes = unify_schema_in_df(df)\n",
    "                if not changes:\n",
    "                    continue\n",
    "                print(f\"→ {os.path.relpath(path)}\")\n",
    "                for c in changes:\n",
    "                    print(f\"   - {c}\")\n",
    "                info[\"df\"] = new_df\n",
    "                info[\"header\"] = tuple(new_df.columns)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {path}: {e}\")\n",
    "\n",
    "        expand_to_include_extra_columns(file_infos)\n",
    "\n",
    "        print(\"\\nAfter:\")\n",
    "        post_map = build_header_map(file_infos)\n",
    "        check_header_differences(post_map)\n",
    "\n",
    "        all_infos.extend(file_infos)\n",
    "    return all_infos\n",
    "\n",
    "\n",
    "def combine_unified_files(file_infos, output_path):\n",
    "    if not file_infos:\n",
    "        print(\"No files to combine.\")\n",
    "        return\n",
    "\n",
    "    frames = [info[\"df\"] for info in file_infos]\n",
    "    combined = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    for col in combined.columns:\n",
    "        if col == \"occ_title\":\n",
    "            continue\n",
    "        s = combined[col]\n",
    "        mask = s.notna()\n",
    "        combined[col] = s.where(~mask, s[mask].astype(str).str.replace(r\"\\s+\", \"\", regex=True)) # clean whitespace in string columns\n",
    "\n",
    "    out_dir = os.path.dirname(output_path)\n",
    "    if out_dir:\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "    combined.to_csv(output_path, index=False)\n",
    "    print(f\"Combined {len(file_infos)} files, {len(combined)} rows -> {output_path}\")\n",
    "\n",
    "infos = unify_dirs(data_directory)\n",
    "combine_unified_files(infos, combined_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b44413",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data/PART_3_csv/'\n",
    "all_files = os.listdir(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fb1ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Found {len(all_files)} files in the directory.\")\n",
    "\n",
    "dataframes = []\n",
    "for file in all_files:\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(data_folder, file)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['source_file'] = file # add source file so that we can use it later\n",
    "            dataframes.append(df)\n",
    "            print(f\"Loaded {file} with shape {df.shape}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaa62b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display headers of all dataframes\n",
    "for i, df in enumerate(dataframes):\n",
    "    print(f\"\\nDataFrame {i} headers:\")\n",
    "    print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdbddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_from_names_file = \"data/names/names.json\"\n",
    "\n",
    "gender_dict = load_gender_dict(gender_from_names_file)\n",
    "get_gender_from_name = get_gender_from_name_factory(gender_dict)\n",
    "\n",
    "print(get_gender_from_name(\"Alice\"))  # example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcbb197",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_iso_file = \"data/names/country_iso.json\"\n",
    "country_iso_data = []\n",
    "\n",
    "if os.path.exists(country_iso_file):\n",
    "    with open(country_iso_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        country_iso_data = data.get(\"3166-1\", [])\n",
    "\n",
    "country_iso_dict, alpha3_codes = load_country_iso_dict(country_iso_file)\n",
    "get_country_iso = get_country_iso_factory(country_iso_dict)\n",
    "get_random_country_iso = get_random_country_iso_factory(country_iso_data)\n",
    "\n",
    "print(get_country_iso(\"Finland\"))  # example usage\n",
    "print(get_random_country_iso())  # example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c52dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_variations = {\n",
    "    'first_name': ['first name', 'First Name', 'FirstName'],\n",
    "    'last_name': ['last name', 'Last Name', 'LastName'],\n",
    "    'year': ['Year', 'year', 'fiscal_year', 'fiscal year', 'calendar_year', 'work_year'],\n",
    "    'Gender': ['gender', 'sex'],\n",
    "    'Education': ['education', 'Education Level', 'education_level', 'highest_education'],\n",
    "    'ExperienceYears': ['experience_years', 'years_of_experience', 'years experience', 'years of experience'],\n",
    "    'JobSatisfaction': ['Job Satisfaction', 'job_satisfaction', 'job satisfaction'],\n",
    "    'WorkLifeBalance': ['Work-Life Balance', 'work_life_balance', 'work life balance'],\n",
    "    'Opportunities': ['opportunities', 'job opportunities', 'career_opportunities'],\n",
    "    'Certifications': ['certifications', 'number_of_certifications', 'num_certifications'],\n",
    "    'FreelancingExperience': ['freelancing_experience', 'freelancing experience', 'freelance_experience', 'freelancing experience'],\n",
    "    'CompanySize': ['company_size', 'Company Size', 'size_of_company'],\n",
    "    'Remote': ['remote', 'Remote Work', 'remote_ratio', 'remote work percentage', 'status'],\n",
    "    'JobPosition': ['title', 'job_position', 'Job Position', 'position_title', 'job title', 'Job Title', 'job_title', 'Division','Position Title', 'current occupation', 'occ_title'],\n",
    "    'CompanyLocation': ['company_location', 'Company Location', 'work_location'],\n",
    "    'EmployeeResidence': ['employee_residence', 'Employee Residence', 'residence', 'home_location'],\n",
    "    'EmployementStatus': ['employment_status', 'Employment Status', 'job_status'],\n",
    "    'Currency': ['currency', 'salary_currency'],\n",
    "    'Salary': ['salary', 'base_salary', 'BaseSalary', 'Base Pay'],\n",
    "    'SeniorityLevel': ['seniority_level', 'Seniority Level', 'level_of_seniority'],\n",
    "    'OvertimePay': ['overtime_gross_pay_qtd'],\n",
    "    'LongivityPay': ['longevity_gross_pay_qtd', 'LongevityPay'],\n",
    "    'TechnologyAdoption': ['Technology Adoption'],\n",
    "    'SkillsGap': ['Skills Gap'],\n",
    "} # combine similar columns\n",
    "\n",
    "# We drop these columns just to simplify the dataframes. That way we will be able to focus on fewer columns for the analysis and modeling.\n",
    "# We do lose some potentially useful information, but for now it's ok.\n",
    "cols_to_drop = [\n",
    "    'the_geom', 'cartodb_id', 'job_code', 'department_number',\n",
    "    'compulsory_union_code', 'termination_month', 'public_id', 'termination_year',\n",
    "    'the_geom_webmercator', 'objectid', 'salary_type', 'quarter', 'occ_code',\n",
    "    'group', 'department', 'fiscal quarter', 'fiscal period', 'middle init',\n",
    "    'job code', 'original hire date', 'position id', 'employee identifier',\n",
    "    'office', 'career change interest', 'family influence', 'mentorship available',\n",
    "    'likely to change occupation', 'career change events', 'first_name', 'last_name',\n",
    "    'department_name', 'position class code', 'grade', 'number of employees', 'ownership',\n",
    "    'revenue', 'skills', 'company', 'company name', 'rating', 'source_file', 'post_date',\n",
    "    'salaries reported', 'headquarter', 'location', 'job roles', 'salary_in_usd', 'experience_level',\n",
    "    'employment_type', 'office name', 'bureau', 'industry', 'departmentname', 'employee_category',\n",
    "    'post_separation_gross_pay_qtd', 'miscellaneous_gross_pay_qtd', 'base_gross_pay_qtd', 'occ_title',\n",
    "    'a_pct10', 'h_pct25', 'jobs_1000', 'i_group', 'loc_quotient', 'h_pct75', 'area', 'naics', 'h_pct90',\n",
    "    'h_mean', 'h_median', 'tot_emp', 'emp_prse', 'h_pct10', 'a_median', 'own_code', 'pct_rpt', 'a_pct75',\n",
    "    'naics_title', 'area_title', 'pct_total', 'a_pct90', 'prim_state', 'a_mean', 'mean_prse', 'a_pct25',\n",
    "    'annual', 'area_type', 'hourly'\n",
    "]\n",
    "\n",
    "exp_map = {\n",
    "    'EN': 'Entry-level',\n",
    "    'MI': 'Mid-level',\n",
    "    'SE': 'Senior',\n",
    "    'EX': 'Executive'\n",
    "}\n",
    "\n",
    "employment_type_map = {\n",
    "    'FT': 'Full-time',\n",
    "    'PT': 'Part-time',\n",
    "    'CT': 'Contractor',\n",
    "    'FL': 'Freelance',\n",
    "    'TM': 'Temporary'\n",
    "}\n",
    "\n",
    "currency_map = {\n",
    "    '$': 'USD',\n",
    "    '€': 'EUR',\n",
    "    '£': 'GBP',\n",
    "    '¥': 'JPY',\n",
    "    '₩': 'KRW',\n",
    "    '₹': 'INR',\n",
    "}\n",
    "\n",
    "location_map = {\n",
    "    'Bangalore': 'India',\n",
    "    'Chennai': 'India',\n",
    "    'Hyderabad': 'India',\n",
    "    'New Delhi': 'India',\n",
    "    'Pune': 'India',\n",
    "    'Jaipur': 'India',\n",
    "    'Kerala': 'India',\n",
    "    'Kolkata': 'India',\n",
    "    'Madhya Pradesh': 'India',\n",
    "    'Mumbai': 'India',\n",
    "}\n",
    "\n",
    "fields = ['Medicine', 'Education', 'Arts', 'Computer Science', 'Business', 'Mechanical Engineering', 'Biology', 'Law', 'Economics', 'Psychology']\n",
    "\n",
    "\n",
    "\n",
    "for df in dataframes:\n",
    "    rename_map = {}\n",
    "    cols_lower = df.columns.str.lower()\n",
    "\n",
    "    for canonical, variations in common_variations.items():\n",
    "        variations_lower = [v.lower() for v in variations]\n",
    "        mask = cols_lower.isin(variations_lower)\n",
    "        if mask.any():\n",
    "            matched_col = df.columns[mask][0]\n",
    "            rename_map[matched_col] = canonical\n",
    "    df.rename(columns=rename_map, inplace=True)\n",
    "    if 'first_name' in df.columns:\n",
    "        df['first_name'] = df['first_name'].astype(str).str.strip().str.capitalize()\n",
    "    if 'Gender' not in df.columns and 'first_name' in df.columns: # We assume gender from the name\n",
    "        df['Gender'] = df['first_name'].apply(get_gender_from_name)\n",
    "    if 'Original Hire Date' in df.columns and 'year' in df.columns:\n",
    "        df['ExperienceYears'] = df['year'] - pd.to_datetime(df['Original Hire Date'], errors='coerce').dt.year\n",
    "    if 'JobSatisfaction' not in df.columns: # We fake some data for JobSatisfaction\n",
    "        df['JobSatisfaction'] = np.random.randint(1, 11, size=len(df))\n",
    "    if 'WorkLifeBalance' not in df.columns: # We fake some data for WorkLifeBalance\n",
    "        df['WorkLifeBalance'] = np.random.randint(1, 11, size=len(df))\n",
    "    if 'Opportunities' not in df.columns: # We fake some data for Opportunities\n",
    "        df['Opportunities'] = np.random.randint(1, 101, size=len(df))\n",
    "    if 'Education' not in df.columns: # We fake some data for Education <-- this is flawed but for now it's ok; The problem is that sometimes you could be a doctor without any degree or such\n",
    "        education_levels = ['Unknown', 'High School', 'Associate', 'Bachelor', 'Master', 'PhD']\n",
    "        df['Education'] = np.random.choice(education_levels, size=len(df))\n",
    "        df['Field of Study'] = None\n",
    "        mask = df['Education'].isin(['Bachelor', 'Master', 'PhD'])\n",
    "        df.loc[mask, 'Field of Study'] = np.random.choice(fields, size=mask.sum())\n",
    "    if 'Certifications' not in df.columns: # We fake some data for Certifications\n",
    "        df['Certifications'] = np.random.randint(0, 2, size=len(df))\n",
    "    if 'FreelancingExperience' not in df.columns: # We fake some data for FreelancingExperience\n",
    "        df['FreelancingExperience'] = np.random.randint(0, 2, size=len(df))\n",
    "    if 'CompanySize' not in df.columns: # We fake some data for CompanySize\n",
    "        company_sizes = ['S', 'M', 'L', 'XL']\n",
    "        df['CompanySize'] = np.random.choice(company_sizes, size=len(df))\n",
    "    if 'Remote' not in df.columns: # We fake some data for Remote\n",
    "        remote_options = [0, 50, 100] # on site; hybrid; remote\n",
    "        df['Remote'] = np.random.choice(remote_options, size=len(df))\n",
    "    if 'Location' in df.columns and 'CompanyLocation' not in df.columns:\n",
    "        country = df['Location'].map(location_map).fillna('unknown')\n",
    "        df['CompanyLocation'] = country.apply(get_country_iso)\n",
    "    if 'headquarter' in df.columns:\n",
    "        df['CompanyLocation'] = df['headquarter'].astype(str).str.split(',').str[-1].str.strip()\n",
    "    if 'CompanyLocation' not in df.columns:\n",
    "        df['CompanyLocation'] = np.random.choice(alpha3_codes, size=len(df))\n",
    "    if 'EmployeeResidence' not in df.columns:\n",
    "        df['EmployeeResidence'] = np.random.choice(alpha3_codes, size=len(df))\n",
    "    if 'Average of Base Salary' in df.columns:\n",
    "        # use symbol to find currency and replace commas with nothing and dots with commas\n",
    "        df.rename(columns={'Average of Base Salary': 'Salary'}, inplace=True)\n",
    "        df['Currency'] = df['Salary'].astype(str).str.extract(r'([^\\d.,\\s]+)')[0].str.strip()\n",
    "        df['Currency'] = df['Currency'].replace(currency_map)\n",
    "        df['Salary'] = df['Salary'].astype(str).str.replace(r'[^\\d.,]', '', regex=True)\n",
    "        df['Salary'] = df['Salary'].str.replace(',', '')\n",
    "        df['Salary'] = pd.to_numeric(df['Salary'], errors='coerce')\n",
    "    if 'Currency' not in df.columns:\n",
    "        df['Currency'] = 'USD'  # default to USD if currency not found\n",
    "    if 'year' not in df.columns and 'source_file' in df.columns: # if year is missing, we try to extract it from the source file name\n",
    "        df['year'] = df['source_file'].str.extract(r'(\\d{4})').astype(float)\n",
    "    if 'EmployementStatus' not in df.columns: # We fake some data for EmployementStatus\n",
    "        employment_types = list(employment_type_map.values())\n",
    "        df['EmployementStatus'] = np.random.choice(employment_types, size=len(df))\n",
    "    if 'annual' in df.columns and 'Salary' not in df.columns:\n",
    "        df.rename(columns={'annual': 'Salary'}, inplace=True)\n",
    "        missing_mask = df['Salary'].isna() | (df['Salary'] == 0)\n",
    "\n",
    "        if missing_mask.any():\n",
    "            if 'a_10' in df.columns and 'a_pct90' in df.columns:\n",
    "                # Fill missing Salary values using random values between a_10 and a_pct90\n",
    "                df.loc[missing_mask, 'Salary'] = df.loc[missing_mask].apply(\n",
    "                    lambda row: np.random.uniform(row['a_10'], row['a_pct90'])\n",
    "                    if pd.notna(row['a_10']) and pd.notna(row['a_pct90'])\n",
    "                    else np.random.uniform(10000, 120000),\n",
    "                    axis=1\n",
    "                )\n",
    "            else:\n",
    "                df.loc[missing_mask, 'Salary'] = np.random.uniform(10000, 120000, size=missing_mask.sum())\n",
    "    if 'OvertimePay' not in df.columns and 'Salary' in df.columns: # We fake some data for OvertimePay\n",
    "        if df['Salary'].dtype in [np.float64, np.int64]:\n",
    "            df['OvertimePay'] = round(df['Salary'] * np.random.uniform(0, 0.5, size=len(df)), 2)\n",
    "        else:\n",
    "            df['OvertimePay'] = 0.0\n",
    "    if 'experience_level' in df.columns:\n",
    "        df['SeniorityLevel'] = df['experience_level'].str.upper().map(exp_map).fillna('Unknown')\n",
    "    if 'SeniorityLevel' not in df.columns:\n",
    "        df['SeniorityLevel'] = np.random.choice(list(exp_map.values()), size=len(df))\n",
    "    if 'employment_type' in df.columns:\n",
    "        df['EmployementStatus'] = df['employment_type'].str.upper().map(employment_type_map).fillna('Unknown')\n",
    "    if 'LongivityPay' not in df.columns and 'Salary' in df.columns:\n",
    "        if df['Salary'].dtype in [np.float64, np.int64]:\n",
    "            df['LongivityPay'] = round(df['Salary'] * np.random.uniform(0, 0.3, size=len(df)), 2)\n",
    "        else:\n",
    "            df['LongivityPay'] = 0.0\n",
    "    if 'TechnologyAdoption' not in df.columns: # We fake some data for TechnologyAdoption; How happy are people for new technology adoption at their workplace\n",
    "        df['TechnologyAdoption'] = np.random.randint(1, 11, size=len(df))\n",
    "    if 'SkillsGap' not in df.columns: # We fake some data for SkillsGap; How does the employees skills match the job requirements\n",
    "        df['SkillsGap'] = np.random.randint(1, 11, size=len(df))\n",
    "    if 'Age' not in df.columns: # We fake some data for Age\n",
    "        df['Age'] = np.random.randint(22, 65, size=len(df))\n",
    "    if 'Gender' not in df.columns: # We fake some data\n",
    "        df['Gender'] = np.random.choice(['Male', 'Female', 'Other'], size=len(df))\n",
    "    if 'Geographic Mobility' not in df.columns: # We fake some data for Geographic Mobility; Whether the individual is willing to relocate for a job.\n",
    "        df['Geographic Mobility'] = np.random.randint(0, 2, size=len(df))\n",
    "    if 'Professional Networks' not in df.columns: # We fake some data for Professional Networks; A measure of how strong the individual's professional network is.\n",
    "        df['Professional Networks'] = np.random.randint(1, 11, size=len(df))\n",
    "    if 'Industry Growth Rate' not in df.columns: # We fake some data for Industry Growth Rate; The growth rate of the industry the individual works in.\n",
    "        growth_type = ['High', 'Medium', 'Low']\n",
    "        df['Industry Growth Rate'] = np.random.choice(growth_type, size=len(df))\n",
    "    if 'Job Security' not in df.columns: # We fake some data for Job Security; A measure of how secure the individual feels in their current job.\n",
    "        df['Job Security'] = np.random.randint(1, 11, size=len(df))\n",
    "    if 'ExperienceYears' not in df.columns: # We fake some data for ExperienceYears\n",
    "        if 'Age' not in df.columns:\n",
    "            df['ExperienceYears'] = np.random.randint(0, 40, size=len(df))\n",
    "        else:\n",
    "            max_exp = df['Age'] - 22\n",
    "            if (max_exp <= 0).any():\n",
    "                max_exp = 2\n",
    "            df['ExperienceYears'] = np.random.randint(0, max_exp, size=len(df))\n",
    "    if 'Field of Study' not in df.columns:\n",
    "        df['Field of Study'] = None\n",
    "        mask = df['Education'].isin(['Bachelor', 'Master', 'PhD'])\n",
    "        df.loc[mask, 'Field of Study'] = np.random.choice(fields, size=mask.sum())\n",
    "    if 'occ_title' in df.columns and 'JobPosition' not in df.columns:\n",
    "        mask = df['occ_title'].astype(str).str.strip().str.lower() != 'all occupations'\n",
    "        df.loc[mask, 'JobPosition'] = df.loc[mask, 'occ_title']\n",
    "        df = df.loc[mask]\n",
    "    # probably we could have industry column too, for now we dropping for simplicity ['Retail', 'Manufacturing', 'Technology', 'Finance', 'Education', 'Healthcare', 'Energy', 'Logistics']\n",
    "\n",
    "    drop_matches = [c for c in df.columns if c.lower() in cols_to_drop]\n",
    "    if drop_matches:\n",
    "        df.drop(columns=drop_matches, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Get OvertimePay values where it's above 0\n",
    "# dataframes[2] = dataframes[2][dataframes[2]['OvertimePay'] > 0]\n",
    "dataframes[3].head()\n",
    "# dataframes[4]['Field of Study'].unique()\n",
    "\n",
    "# Experience_level: en - entry; mi - mid; se - senior; ex - executive\n",
    "\n",
    "# display all unique values for salary_type column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9805b8b",
   "metadata": {},
   "source": [
    "We found that there are some dataframes that don't have sex/gender columns, so we centralize the way first_name and last_name is defined and we add new column called full_name, which we might use in order to find the sex/gender out from the name.\n",
    "\n",
    "we drop salary_in_usd --> ideally you wouldn't want to do that...\n",
    "for simplicity of the project even though we have currency column.. we will assume that salary is in usd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732059ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_index = 4\n",
    "reference_columns = set(dataframes[reference_index].columns)\n",
    "\n",
    "print(f\"Using DataFrame {reference_index} as reference.\\n\")\n",
    "\n",
    "for i, df in enumerate(dataframes):\n",
    "    if i == reference_index:\n",
    "        continue\n",
    "\n",
    "    cols = set(df.columns)\n",
    "    missing = reference_columns - cols\n",
    "    extra = cols - reference_columns\n",
    "\n",
    "    if not missing and not extra:\n",
    "        continue\n",
    "\n",
    "    print(f\"--- DataFrame {i} compared to reference {reference_index} ---\")\n",
    "    if missing:\n",
    "        print(\"  Missing columns:\", missing)\n",
    "    if extra:\n",
    "        print(\"  Extra columns:  \", extra)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404f09f0",
   "metadata": {},
   "source": [
    "If above is empty (no missing columns and no extra columns), we can continue and combine all of it together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7f9bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([df.loc[:, ~df.columns.duplicated()].reset_index(drop=True) for df in dataframes],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980b7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6215d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = 'data/PART_4_COMBINED_CSV/combined_data.csv'\n",
    "combined_df.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bd0c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/PART_4_COMBINED_CSV/combined_data.csv\")\n",
    "\n",
    "print(\"Loaded:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e525bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicates\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(\"After duplicates:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fcba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize column names\n",
    "\n",
    "df.columns = (\n",
    "    df.columns\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .str.replace(\" \", \"_\")\n",
    "    .str.replace(\"-\", \"_\")\n",
    "    .str.replace(\"/\", \"_\")\n",
    ")\n",
    "print(\"Column names cleaned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f95f6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix salary fields\n",
    "\n",
    "salary_cols = [c for c in df.columns if \"salary\" in c or \"pay\" in c or \"income\" in c]\n",
    "\n",
    "for col in salary_cols:\n",
    "    df[col] = (\n",
    "        df[col]\n",
    "        .astype(str)\n",
    "        .str.replace(\",\", \"\")\n",
    "        .str.replace(\"$\", \"\")\n",
    "        .str.replace(\"€\", \"\")\n",
    "    )\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "print(\"Salary fields cleaned:\", salary_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d960b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_cols = [\"salary\", \"experienceyears\", \"age\"]\n",
    "\n",
    "for col in fill_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(df[col].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d35dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis=1, how=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a303da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isna().sum().sort_values(ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109cdeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "\n",
    "df.isna().sum().sort_values(ascending=False).head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0243e1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric columns\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[\"float64\",\"int64\"]).columns.tolist()\n",
    "numeric_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f576a938",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    converted = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    print(col, converted.notna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a304f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "print(\"Numeric columns:\", numeric_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11284dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "safe_numeric_cols = []\n",
    "\n",
    "for col in df.columns:\n",
    "\n",
    "    # ensure we always work on a Series\n",
    "    s = df[col]\n",
    "\n",
    "    # skip if column is not a Series of simple values\n",
    "    if not isinstance(s, pd.Series):\n",
    "        continue\n",
    "\n",
    "    # try convert\n",
    "    converted = pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "    # keep only if at least 1 real number exists\n",
    "    if converted.notna().sum() > 0:\n",
    "        median_val = converted.median()\n",
    "        df[col] = converted.fillna(median_val)\n",
    "        safe_numeric_cols.append(col)\n",
    "\n",
    "print(\"Numeric columns filled:\", safe_numeric_cols)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026f0a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[safe_numeric_cols].isna().sum().sort_values(ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418f46d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Drop columns with all missing values\n",
    "df = df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "# 2. Drop columns with only one unique non-null value\n",
    "single_val_cols = []\n",
    "\n",
    "for c in df.columns:\n",
    "    try:\n",
    "        if df[c].dropna().nunique() <= 1:\n",
    "            single_val_cols.append(c)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "df = df.drop(columns=single_val_cols)\n",
    "\n",
    "print(\"Dropped:\", single_val_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bdba2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().sort_values(ascending=False).head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0046f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned dataset\n",
    "\n",
    "df.to_csv(\"data/PART_4_COMBINED_CSV/cleaned_part4.csv\", index=False)\n",
    "print(\"Saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0611452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Drop useless identifier columns\n",
    "id_cols = [\n",
    "    \"first_name\",\n",
    "    \"last_name\",\n",
    "    \"employee_identifier\",\n",
    "    \"original_hire_date\",\n",
    "    \"public_id\",\n",
    "    \"position_id\",\n",
    "    \"job_code\",\n",
    "    \"bureau\"\n",
    "]\n",
    "\n",
    "df = df.drop(columns=[c for c in id_cols if c in df.columns])\n",
    "\n",
    "\n",
    "# 2. Drop columns with more than 90% missing\n",
    "missing_ratio = df.isna().mean()\n",
    "high_missing_cols = missing_ratio[missing_ratio > 0.90].index.tolist()\n",
    "\n",
    "df = df.drop(columns=high_missing_cols)\n",
    "\n",
    "\n",
    "# 3. Merge duplicate categorical fields\n",
    "if \"companylocation\" in df.columns and \"employeeresidence\" in df.columns:\n",
    "    df[\"location\"] = df[\"companylocation\"].fillna(df[\"employeeresidence\"])\n",
    "    df = df.drop(columns=[\"companylocation\", \"employeeresidence\"], errors=\"ignore\")\n",
    "\n",
    "if \"jobroles\" in df.columns and \"jobposition\" in df.columns:\n",
    "    df[\"job_role\"] = df[\"jobroles\"].fillna(df[\"jobposition\"])\n",
    "    df = df.drop(columns=[\"jobroles\", \"jobposition\"], errors=\"ignore\")\n",
    "\n",
    "\n",
    "# 4. Normalize categorical text\n",
    "cat_cols = df.select_dtypes(include=\"object\").columns\n",
    "\n",
    "for col in cat_cols:\n",
    "    df[col] = (\n",
    "        df[col]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(r\"\\s+\", \"_\", regex=True)\n",
    "    )\n",
    "\n",
    "\n",
    "# 5. Fix impossible numeric values\n",
    "if \"age\" in df.columns:\n",
    "    df.loc[df[\"age\"] < 15, \"age\"] = np.nan\n",
    "\n",
    "if \"experienceyears\" in df.columns and \"age\" in df.columns:\n",
    "    df.loc[df[\"experienceyears\"] > df[\"age\"], \"experienceyears\"] = np.nan\n",
    "\n",
    "\n",
    "# 6. Remove extreme salary outliers\n",
    "if \"salary\" in df.columns:\n",
    "    df = df[df[\"salary\"].between(500, 1000000)]\n",
    "\n",
    "\n",
    "# 7. Parse dates and create tenure\n",
    "if \"termination_year\" in df.columns and \"termination_month\" in df.columns:\n",
    "    df[\"termination_date\"] = pd.to_datetime(\n",
    "        df[\"termination_year\"].astype(str) + \"-\" + df[\"termination_month\"].astype(str) + \"-01\",\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "if \"fiscal_year\" in df.columns and \"fiscal_quarter\" in df.columns:\n",
    "    df[\"fiscal_period\"] = df[\"fiscal_year\"].astype(str) + \"Q\" + df[\"fiscal_quarter\"].astype(str)\n",
    "\n",
    "if \"termination_date\" in df.columns and \"year\" in df.columns:\n",
    "    df[\"tenure_years\"] = df[\"year\"] - df[\"termination_date\"].dt.year\n",
    "\n",
    "\n",
    "print(\"Cleaning additions completed.\")\n",
    "print(\"Dropped high missing:\", high_missing_cols)\n",
    "print(\"Shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbd8ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().sort_values(ascending=False).head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6096b260",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# identify categorical columns\n",
    "cat_cols = df.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "# split by cardinality\n",
    "low_card = [c for c in cat_cols if df[c].nunique() <= 20]\n",
    "high_card = [c for c in cat_cols if df[c].nunique() > 20]\n",
    "\n",
    "print(\"Low-cardinality:\", low_card)\n",
    "print(\"High-cardinality:\", high_card)\n",
    "\n",
    "# frequency encode high-cardinality\n",
    "for col in high_card:\n",
    "    freq = df[col].value_counts(normalize=True)\n",
    "    df[col] = df[col].map(freq)\n",
    "\n",
    "# build transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", numeric_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), low_card),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "processed = preprocessor.fit_transform(df)\n",
    "\n",
    "print(\"Processed shape:\", processed.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdc8915",
   "metadata": {},
   "source": [
    "Generate more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3880ca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_existing_rows = len(combined_df)\n",
    "num_desired_rows = 20_000_000\n",
    "num_rows_to_generate = num_desired_rows - num_existing_rows\n",
    "out_path = 'data/PART_5_fill_with_more_data/combined_data_filled.csv'\n",
    "\n",
    "if \"Salary\" in combined_df.columns:\n",
    "    salary_numeric = pd.to_numeric(combined_df[\"Salary\"], errors=\"coerce\")\n",
    "    salary_numeric = salary_numeric.dropna()\n",
    "\n",
    "    if not salary_numeric.empty:\n",
    "        salary_min_safe = salary_numeric.quantile(0.01)\n",
    "        salary_max_safe = salary_numeric.quantile(0.99)\n",
    "    else:\n",
    "        salary_min_safe = 10_000\n",
    "        salary_max_safe = 500_000\n",
    "else:\n",
    "    salary_min_safe = 10_000\n",
    "    salary_max_safe = 500_000\n",
    "\n",
    "print(f\"Using salary clip range: [{salary_min_safe:.2f}, {salary_max_safe:.2f}]\")\n",
    "\n",
    "if num_rows_to_generate > 0:\n",
    "    sampled_df = combined_df.sample(n=num_rows_to_generate, replace=True, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    numeric_cols = ['Salary', 'OvertimePay', 'LongivityPay', 'ExperienceYears', 'Age', 'JobSatisfaction', 'WorkLifeBalance', 'Opportunities', 'Certifications', 'FreelancingExperience', 'TechnologyAdoption', 'SkillsGap', 'Geographic Mobility', 'Professional Networks', 'Job Security']\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        if col not in sampled_df.columns:\n",
    "            continue\n",
    "\n",
    "        sampled_df[col] = pd.to_numeric(sampled_df[col], errors='coerce')\n",
    "\n",
    "        if sampled_df[col].notna().sum() == 0:\n",
    "            continue\n",
    "        std = sampled_df[col].std()\n",
    "        if pd.isna(std) or std == 0:\n",
    "            continue\n",
    "        noise = np.random.normal(0, 0.05 * std, size=len(sampled_df))\n",
    "        sampled_df[col] = sampled_df[col] + noise\n",
    "        if col == \"Salary\":\n",
    "            sampled_df[col] = sampled_df[col].clip(lower=salary_min_safe,upper=salary_max_safe)\n",
    "        elif col == \"Age\":\n",
    "            sampled_df[col] = sampled_df[col].clip(lower=16, upper=80)\n",
    "        elif col == \"ExperienceYears\":\n",
    "            sampled_df[col] = sampled_df[col].clip(lower=0, upper=60)\n",
    "        else:\n",
    "            sampled_df[col] = sampled_df[col].clip(lower=0)\n",
    "\n",
    "    combined_df = pd.concat([combined_df, sampled_df], ignore_index=True)\n",
    "    print(f\"Generated {num_rows_to_generate} additional rows for a total of {len(combined_df)} rows.\")\n",
    "\n",
    "if \"Salary\" in combined_df.columns:\n",
    "    combined_df[\"Salary\"] = pd.to_numeric(combined_df[\"Salary\"], errors=\"coerce\")\n",
    "    combined_df[\"Salary\"] = combined_df[\"Salary\"].clip(lower=salary_min_safe, upper=salary_max_safe)\n",
    "\n",
    "combined_df.to_csv(out_path, index=False)\n",
    "print(f\"Saved combined data to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371c5523",
   "metadata": {},
   "source": [
    "CLEANING STEPS (DONE)\n",
    "\n",
    "• loading and combining data\n",
    "• removing duplicates\n",
    "• standardizing column names\n",
    "• cleaning salary fields\n",
    "• converting numeric fields\n",
    "• filling numeric missing values safely\n",
    "• filling categorical missing values\n",
    "• dropping high-missing columns\n",
    "• dropping all-missing columns\n",
    "• merging duplicate categorical fields\n",
    "• normalizing categorical text\n",
    "• removing impossible ages\n",
    "• removing impossible experience values\n",
    "• removing extreme salary outliers\n",
    "• parsing date fields\n",
    "• removing useless date columns afterward\n",
    "• confirming no remaining missing values in usable columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dcbbbb",
   "metadata": {},
   "source": [
    "## Spark Medallion Architecture: Bronze, Silver, Gold\n",
    "We now re-ingest the large PART 5 file with Spark and process it through a simple medallion pipeline:\n",
    "\n",
    "- **Bronze**: raw ingestion from `data/PART_5_fill_with_more_data/combined_data_filled.csv`.\n",
    "- **Silver**: MapReduce-style aggregations and basic cleaning.\n",
    "- **Gold**: ML, features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3014fd",
   "metadata": {},
   "source": [
    "# Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b43154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "SparkSession.builder.getOrCreate().stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97cc81d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/27 22:53:52 WARN Utils: Your hostname, Red-Macbook-Pro.local, resolves to a loopback address: 127.0.0.1; using 192.168.1.94 instead (on interface en0)\n",
      "25/11/27 22:53:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/27 22:53:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SalaryMedallionPipeline\") \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33f80f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:===============================================>         (41 + 8) / 49]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze count: 20000000\n",
      "Bronze schema:\n",
      "root\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Education: string (nullable = true)\n",
      " |-- JobPosition: string (nullable = true)\n",
      " |-- ExperienceYears: double (nullable = true)\n",
      " |-- Salary: double (nullable = true)\n",
      " |-- JobSatisfaction: double (nullable = true)\n",
      " |-- WorkLifeBalance: double (nullable = true)\n",
      " |-- Opportunities: double (nullable = true)\n",
      " |-- Certifications: double (nullable = true)\n",
      " |-- FreelancingExperience: double (nullable = true)\n",
      " |-- CompanySize: string (nullable = true)\n",
      " |-- Remote: string (nullable = true)\n",
      " |-- CompanyLocation: string (nullable = true)\n",
      " |-- EmployeeResidence: string (nullable = true)\n",
      " |-- Currency: string (nullable = true)\n",
      " |-- year: double (nullable = true)\n",
      " |-- EmployementStatus: string (nullable = true)\n",
      " |-- OvertimePay: double (nullable = true)\n",
      " |-- SeniorityLevel: string (nullable = true)\n",
      " |-- LongivityPay: double (nullable = true)\n",
      " |-- TechnologyAdoption: double (nullable = true)\n",
      " |-- SkillsGap: double (nullable = true)\n",
      " |-- Geographic Mobility: double (nullable = true)\n",
      " |-- Professional Networks: double (nullable = true)\n",
      " |-- Industry Growth Rate: string (nullable = true)\n",
      " |-- Job Security: double (nullable = true)\n",
      " |-- Field of Study: string (nullable = true)\n",
      "\n",
      "+----+------+----------+-----------------+---------------+--------+---------------+---------------+-------------+--------------+---------------------+-----------+------+---------------+-----------------+--------+----+-----------------+-----------+--------------+------------+------------------+---------+-------------------+---------------------+--------------------+------------+----------------------+\n",
      "|Age |Gender|Education |JobPosition      |ExperienceYears|Salary  |JobSatisfaction|WorkLifeBalance|Opportunities|Certifications|FreelancingExperience|CompanySize|Remote|CompanyLocation|EmployeeResidence|Currency|year|EmployementStatus|OvertimePay|SeniorityLevel|LongivityPay|TechnologyAdoption|SkillsGap|Geographic Mobility|Professional Networks|Industry Growth Rate|Job Security|Field of Study        |\n",
      "+----+------+----------+-----------------+---------------+--------+---------------+---------------+-------------+--------------+---------------------+-----------+------+---------------+-----------------+--------+----+-----------------+-----------+--------------+------------+------------------+---------+-------------------+---------------------+--------------------+------------+----------------------+\n",
      "|32.0|Male  |Bachelor's|Software Engineer|5.0            |90000.0 |5.0            |6.0            |65.0         |1.0           |1.0                  |L          |50    |LKA            |NIC              |USD     |NULL|Contractor       |32659.99   |Executive     |815.67      |10.0              |7.0      |1.0                |6.0                  |Low                 |1.0         |NULL                  |\n",
      "|28.0|Female|Master's  |Data Analyst     |3.0            |65000.0 |10.0           |10.0           |21.0         |1.0           |1.0                  |L          |50    |TON            |TCA              |USD     |NULL|Freelance        |29286.98   |Entry-level   |702.03      |1.0               |6.0      |1.0                |2.0                  |Medium              |10.0        |NULL                  |\n",
      "|45.0|Male  |PhD       |Senior Manager   |15.0           |150000.0|5.0            |6.0            |62.0         |0.0           |1.0                  |S          |50    |DZA            |DEU              |USD     |NULL|Temporary        |16904.89   |Entry-level   |25719.32    |2.0               |6.0      |0.0                |4.0                  |High                |5.0         |Mechanical Engineering|\n",
      "|36.0|Female|Bachelor's|Sales Associate  |7.0            |60000.0 |4.0            |1.0            |58.0         |0.0           |1.0                  |L          |50    |AFG            |SWE              |USD     |NULL|Part-time        |16671.28   |Entry-level   |14517.4     |7.0               |6.0      |0.0                |5.0                  |Medium              |5.0         |NULL                  |\n",
      "|52.0|Male  |Master's  |Director         |20.0           |200000.0|6.0            |4.0            |61.0         |0.0           |1.0                  |S          |100   |SXM            |HND              |USD     |NULL|Freelance        |68073.53   |Senior        |37574.04    |6.0               |10.0     |0.0                |2.0                  |Medium              |4.0         |NULL                  |\n",
      "+----+------+----------+-----------------+---------------+--------+---------------+---------------+-------------+--------------+---------------------+-----------+------+---------------+-----------------+--------+----+-----------------+-----------+--------------+------------+------------------+---------+-------------------+---------------------+--------------------+------------+----------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/27 22:54:15 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "bronze_path = \"data/PART_5_fill_with_more_data/combined_data_filled.csv\"\n",
    "\n",
    "bronze_df = (\n",
    "    spark.read\n",
    "         .option(\"header\", \"true\")\n",
    "         .option(\"inferSchema\", \"true\")\n",
    "         .csv(bronze_path)\n",
    ")\n",
    "\n",
    "print(\"Bronze count:\", bronze_df.count())\n",
    "print(\"Bronze schema:\")\n",
    "bronze_df.printSchema()\n",
    "bronze_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0f4e02",
   "metadata": {},
   "source": [
    "# Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf8ce25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: double (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- jobposition: string (nullable = true)\n",
      " |-- experienceyears: double (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- jobsatisfaction: double (nullable = true)\n",
      " |-- worklifebalance: double (nullable = true)\n",
      " |-- opportunities: double (nullable = true)\n",
      " |-- certifications: double (nullable = true)\n",
      " |-- freelancingexperience: double (nullable = true)\n",
      " |-- companysize: string (nullable = true)\n",
      " |-- remote: string (nullable = true)\n",
      " |-- companylocation: string (nullable = true)\n",
      " |-- employeeresidence: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- year: double (nullable = true)\n",
      " |-- employementstatus: string (nullable = true)\n",
      " |-- overtimepay: double (nullable = true)\n",
      " |-- senioritylevel: string (nullable = true)\n",
      " |-- longivitypay: double (nullable = true)\n",
      " |-- technologyadoption: double (nullable = true)\n",
      " |-- skillsgap: double (nullable = true)\n",
      " |-- geographic_mobility: double (nullable = true)\n",
      " |-- professional_networks: double (nullable = true)\n",
      " |-- industry_growth_rate: string (nullable = true)\n",
      " |-- job_security: double (nullable = true)\n",
      " |-- field_of_study: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                        (0 + 10) / 49]\r"
     ]
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import col, regexp_replace, split, trim, size, lower, when\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# normalize column names\n",
    "silver_df = bronze_df\n",
    "for c in silver_df.columns:\n",
    "    silver_df = silver_df.withColumnRenamed(c,c.strip().lower().replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"/\", \"_\"))\n",
    "silver_df.printSchema()\n",
    "\n",
    "# salary cleaning as we sometimes have salaries like \"€44,521 - €59,359\"\n",
    "if \"salary\" in silver_df.columns:\n",
    "    salary_str = col(\"salary\").cast(\"string\")\n",
    "    cleaned_salary = regexp_replace(salary_str, \"[^0-9Ee.,\\\\-]\", \"\") # keep only digits and dash\n",
    "    parts = split(cleaned_salary, \"-\") # split on dash to get low/high\n",
    "\n",
    "    low_raw = trim(parts.getItem(0))\n",
    "    high_raw = trim(F.when(size(parts) > 1, parts.getItem(1)).otherwise(F.lit(None)))\n",
    "\n",
    "    low_clean  = regexp_replace(low_raw,  \",\", \"\")\n",
    "    high_clean = regexp_replace(high_raw, \",\", \"\")\n",
    "\n",
    "    # cast to double when non-empty\n",
    "    low_num = F.when((low_raw.isNotNull()) & (low_raw != \"\"), low_raw.cast(DoubleType()))\n",
    "    high_num = F.when((high_raw.isNotNull()) & (high_raw != \"\"), high_raw.cast(DoubleType()))\n",
    "\n",
    "    silver_df = silver_df.withColumn(\"salary_clean\", F.when(high_num.isNotNull(), (low_num + high_num) / 2.0).otherwise(low_num)) # if high exists, use average of low/high; else just low\n",
    "\n",
    "\n",
    "silver_df = silver_df.filter(col(\"salary_clean\").isNotNull()) # filter for only valid numeric salary\n",
    "salary = F.col(\"salary_clean\")\n",
    "silver_df = silver_df.filter((F.col(\"salary_clean\") > 1000) & (F.col(\"salary_clean\") < 500_000)) # filter out extreme outliers\n",
    "\n",
    "if \"jobposition\" in silver_df.columns and \"job_role\" not in silver_df.columns: silver_df = silver_df.withColumnRenamed(\"jobposition\", \"job_role\")\n",
    "if \"senioritylevel\" in silver_df.columns and \"seniority_level\" not in silver_df.columns: silver_df = silver_df.withColumnRenamed(\"senioritylevel\", \"seniority_level\")\n",
    "\n",
    "silver_df = silver_df.fillna({\"seniority_level\": \"EN\"})\n",
    "\n",
    "job_lower = lower(F.col(\"job_role\"))\n",
    "\n",
    "silver_df = silver_df.withColumn(\n",
    "    \"seniority_from_title\",\n",
    "    when(job_lower.rlike(r\"\\b(executive|vp|chief|cto|cfo|ceo|head)\\b\"), F.lit(\"EX\"))\n",
    "    .when(job_lower.rlike(r\"\\b(lead|principal|staff|senior|sr\\.?)\\b\"), F.lit(\"SE\"))\n",
    "    .when(job_lower.rlike(r\"\\b(mid|middle|intermediate)\\b\"), F.lit(\"MI\"))\n",
    "    .when(job_lower.rlike(r\"\\b(junior|jr\\.?)\\b\"), F.lit(\"EN\"))\n",
    "    .otherwise(F.lit(None))\n",
    ")\n",
    "\n",
    "silver_df = silver_df.withColumn(\"seniority_level\", when(F.col(\"seniority_from_title\").isNotNull(), F.col(\"seniority_from_title\")).otherwise(F.col(\"seniority_level\")))\n",
    "seniority_lower = lower(F.col(\"seniority_level\"))\n",
    "silver_df = silver_df.withColumn(\n",
    "    \"seniority_level\",\n",
    "    when(seniority_lower.rlike(r\"entry|junior\"), \"EN\")\n",
    "    .when(seniority_lower.rlike(r\"mid|middle|intermediate\"), \"MI\")\n",
    "    .when(seniority_lower.rlike(r\"senior|sr\"), \"SE\")\n",
    "    .when(seniority_lower.rlike(r\"executive|vp|chief|head\"), \"EX\")\n",
    "    .otherwise(F.col(\"seniority_level\"))\n",
    ") # We standardize seniority levels\n",
    "silver_df = silver_df.withColumn(\"job_role\", trim(regexp_replace(job_lower,r\"\\b(executive|vp|chief|cto|cfo|ceo|head|lead|principal|staff|senior|sr\\.?|mid|middle|intermediate|junior|jr\\.?)\\b\",\"\"))) # remove seniority words from job role\n",
    "silver_df = silver_df.withColumn(\"job_role\", regexp_replace(col(\"job_role\"), r\"[0-9]\", \"\")) # remove any numbers in case they have it\n",
    "silver_df = silver_df.withColumn( \"job_role\", regexp_replace(col(\"job_role\"), r\"\\b(?i)(i|ii|iii|iv|v|vi|vii|viii|ix|x)\\b\", \"\") ) # remove roman numerals in case they have it\n",
    "silver_df = silver_df.withColumn(\"job_role\", regexp_replace(col(\"job_role\"), r\"\\([^)]*\\)\", \"\")) # remove anything in parentheses\n",
    "silver_df = silver_df.withColumn(\"job_role\", trim(regexp_replace(col(\"job_role\"), r\"\\s+\", \" \")))\n",
    "silver_df = silver_df.filter(~col(\"job_role\").like(\"%--%\"))\n",
    "\n",
    "silver_df = silver_df.withColumn( \"job_role\", F.initcap(F.col(\"job_role\")))\n",
    "\n",
    "silver_df = silver_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "print(\"silver count:\", silver_df.count())\n",
    "silver_df.select(\"salary\", \"salary_clean\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a61be40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate in Silver --> average salary, count per job_role and location\n",
    "group_cols = []\n",
    "\n",
    "if \"job_role\" in silver_df.columns:\n",
    "    group_cols.append(\"job_role\")\n",
    "elif \"jobposition\" in silver_df.columns:\n",
    "    group_cols.append(\"jobposition\")\n",
    "elif \"occ_title\" in silver_df.columns:\n",
    "    group_cols.append(\"occ_title\")\n",
    "\n",
    "if \"location\" in silver_df.columns:\n",
    "    group_cols.append(\"location\")\n",
    "elif \"companylocation\" in silver_df.columns:\n",
    "    group_cols.append(\"companylocation\")\n",
    "elif \"employeeresidence\" in silver_df.columns:\n",
    "    group_cols.append(\"employeeresidence\")\n",
    "\n",
    "if \"salary_clean\" in silver_df.columns and group_cols:\n",
    "    silver_agg_df = silver_df.groupBy(*group_cols).agg(\n",
    "        F.avg(col(\"salary_clean\")).alias(\"avg_salary\"),\n",
    "        F.min(col(\"salary_clean\")).alias(\"min_salary\"),\n",
    "        F.max(col(\"salary_clean\")).alias(\"max_salary\"),\n",
    "        F.count(col(\"salary_clean\")).alias(\"salary_records\")\n",
    "    )\n",
    "else:\n",
    "    silver_agg_df = silver_df\n",
    "\n",
    "print(\"silver agg count:\", silver_agg_df.count())\n",
    "silver_agg_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54a3b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_detail_path = \"data/PART_5_fill_with_more_data/silver_detail.parquet\"\n",
    "silver_agg_path = \"data/PART_5_fill_with_more_data/silver_agg.parquet\"\n",
    "silver_agg_csv_path = \"data/PART_5_fill_with_more_data/silver_agg_csv\"\n",
    "\n",
    "silver_df.write.mode(\"overwrite\").parquet(silver_detail_path)\n",
    "silver_agg_df.write.mode(\"overwrite\").parquet(silver_agg_path)\n",
    "silver_agg_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(silver_agg_csv_path)\n",
    "\n",
    "print(\"silver saved to:\")\n",
    "print(\"detail saved to:\", silver_detail_path)\n",
    "print(\"agg saved to:\", silver_agg_path)\n",
    "print(\"agg saved to (csv):\", silver_agg_csv_path)\n",
    "\n",
    "silver_df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4556b3e",
   "metadata": {},
   "source": [
    "# Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2f4ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "silver_detail_loaded = spark.read.parquet(silver_detail_path)\n",
    "silver_agg_loaded = spark.read.parquet(silver_agg_path)\n",
    "\n",
    "print(\"silver columns:\")\n",
    "print(\"detail cols:\", silver_detail_loaded.columns)\n",
    "print(\"agg cols:\", silver_agg_loaded.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206bd66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we choose role and location columns from aggregated Silver\n",
    "candidate_role_cols = [\"job_role\", \"jobposition\", \"occ_title\"]\n",
    "candidate_location_cols = [\"location\", \"companylocation\", \"employeeresidence\"]\n",
    "\n",
    "role_col = next((c for c in candidate_role_cols if c in silver_agg_loaded.columns), None)\n",
    "location_col = next((c for c in candidate_location_cols if c in silver_agg_loaded.columns), None)\n",
    "\n",
    "selected_cols = []\n",
    "if role_col: selected_cols.append(role_col)\n",
    "if location_col: selected_cols.append(location_col)\n",
    "\n",
    "for metric_col in [\"avg_salary\", \"min_salary\", \"max_salary\", \"salary_records\"]:\n",
    "    if metric_col in silver_agg_loaded.columns:\n",
    "        selected_cols.append(metric_col)\n",
    "\n",
    "if not selected_cols:\n",
    "    print(\"no columns in silver agg which is why we take all\")\n",
    "    gold_features_df = silver_agg_loaded\n",
    "else:\n",
    "    gold_features_df = silver_agg_loaded.select(*selected_cols)\n",
    "\n",
    "gold_features_path_parquet = \"data/PART_5_fill_with_more_data/gold_features.parquet\"\n",
    "gold_features_path_csv = \"data/PART_5_fill_with_more_data/gold_features_csv\"\n",
    "\n",
    "gold_features_df.write.mode(\"overwrite\").parquet(gold_features_path_parquet)\n",
    "gold_features_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(gold_features_path_csv)\n",
    "\n",
    "print(\"gold agg features saved to:\")\n",
    "print(\"parquet:\", gold_features_path_parquet)\n",
    "print(\"csv:\", gold_features_path_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39520c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_base_df = silver_detail_loaded\n",
    "detail_cols = []\n",
    "\n",
    "# even though we shouldn't have all these columns as we dropped them or combined them before, we still going to check for them just in case something happened.\n",
    "for c in [\"job_role\", \"jobposition\", \"occ_title\"]:\n",
    "    if not c in ml_base_df.columns: continue\n",
    "    detail_cols.append(c)\n",
    "    break\n",
    "\n",
    "for c in [\"location\", \"companylocation\", \"employeeresidence\"]:\n",
    "    if not c in ml_base_df.columns: continue\n",
    "    detail_cols.append(c)\n",
    "    break\n",
    "\n",
    "for c in [\"year\", \"fiscal_year\"]:\n",
    "    if not c in ml_base_df.columns: continue\n",
    "    detail_cols.append(c)\n",
    "    break\n",
    "\n",
    "for c in [\"seniority_level\", \"senioritylevel\"]:\n",
    "    if not c in ml_base_df.columns: continue\n",
    "    detail_cols.append(c)\n",
    "    break\n",
    "\n",
    "for c in [\"experience_years\", \"experienceyears\"]:\n",
    "    if not c in ml_base_df.columns: continue\n",
    "    detail_cols.append(c)\n",
    "    break\n",
    "\n",
    "detail_cols.append(\"salary_clean\")\n",
    "gold_detail_df = ml_base_df.select(*detail_cols)\n",
    "\n",
    "print(\"shape of the gold:\", gold_detail_df.count(), len(gold_detail_df.columns))\n",
    "gold_detail_df.show(5, truncate=False)\n",
    "\n",
    "gold_detail_path = \"data/PART_5_fill_with_more_data/gold_detail.parquet\"\n",
    "gold_detail_df.write.mode(\"overwrite\").parquet(gold_detail_path)\n",
    "print(\"gold saved to:\", gold_detail_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8050a719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import FeatureHasher, VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "ml_df = gold_detail_df\n",
    "\n",
    "# same like above, even though we shouldn't have all these columns as we dropped them or combined them before, we still going to check for them just in case something happened.\n",
    "if \"occ_title\" in ml_df.columns and \"job_role\" not in ml_df.columns: ml_df = ml_df.withColumnRenamed(\"occ_title\", \"job_role\")\n",
    "if \"jobposition\" in ml_df.columns and \"job_role\" not in ml_df.columns: ml_df = ml_df.withColumnRenamed(\"jobposition\", \"job_role\")\n",
    "if \"companylocation\" in ml_df.columns and \"location\" not in ml_df.columns: ml_df = ml_df.withColumnRenamed(\"companylocation\", \"location\")\n",
    "if \"employeeresidence\" in ml_df.columns and \"location\" not in ml_df.columns: ml_df = ml_df.withColumnRenamed(\"employeeresidence\", \"location\")\n",
    "if \"fiscal_year\" in ml_df.columns and \"year\" not in ml_df.columns: ml_df = ml_df.withColumnRenamed(\"fiscal_year\", \"year\")\n",
    "if \"senioritylevel\" in ml_df.columns and \"seniority_level\" not in ml_df.columns: ml_df = ml_df.withColumnRenamed(\"senioritylevel\", \"seniority_level\")\n",
    "if \"experienceyears\" in ml_df.columns and \"experience_years\" not in ml_df.columns: ml_df = ml_df.withColumnRenamed(\"experienceyears\", \"experience_years\")\n",
    "\n",
    "agg_for_join = silver_agg_loaded\n",
    "if \"occ_title\" in agg_for_join.columns and \"job_role\" not in agg_for_join.columns: agg_for_join = agg_for_join.withColumnRenamed(\"occ_title\", \"job_role\")\n",
    "if \"jobposition\" in agg_for_join.columns and \"job_role\" not in agg_for_join.columns: agg_for_join = agg_for_join.withColumnRenamed(\"jobposition\", \"job_role\")\n",
    "if \"companylocation\" in agg_for_join.columns and \"location\" not in agg_for_join.columns: agg_for_join = agg_for_join.withColumnRenamed(\"companylocation\", \"location\")\n",
    "if \"employeeresidence\" in agg_for_join.columns and \"location\" not in agg_for_join.columns: agg_for_join = agg_for_join.withColumnRenamed(\"employeeresidence\", \"location\")\n",
    "\n",
    "join_cols = []\n",
    "if \"job_role\" in ml_df.columns and \"job_role\" in agg_for_join.columns: join_cols.append(\"job_role\")\n",
    "if \"location\" in ml_df.columns and \"location\" in agg_for_join.columns: join_cols.append(\"location\")\n",
    "\n",
    "if join_cols: ml_df = ml_df.join(agg_for_join.select(*join_cols, \"avg_salary\", \"min_salary\", \"max_salary\", \"salary_records\"),on=join_cols,how=\"left\")\n",
    "\n",
    "ml_df.printSchema()\n",
    "\n",
    "# we fill missing values in case we have any\n",
    "if \"seniority_level\" in ml_df.columns: ml_df = ml_df.fillna({\"seniority_level\": \"Unknown\"})\n",
    "if \"experience_years\" in ml_df.columns: ml_df = ml_df.withColumn(\"experience_years\", F.col(\"experience_years\").cast(\"double\"))\n",
    "ml_df = ml_df.fillna({\"avg_salary\":  ml_df.agg(F.avg(\"salary_clean\")).first()[0], \"min_salary\":  ml_df.agg(F.min(\"salary_clean\")).first()[0], \"max_salary\":  ml_df.agg(F.max(\"salary_clean\")).first()[0]})\n",
    "\n",
    "# ml_df = ml_df.withColumn(\"log_salary\", F.log(F.col(\"salary_clean\"))) # log transform of salary\n",
    "# ml_df = ml_df.dropna(subset=[\"job_role\", \"location\", \"year\", \"salary_clean\", \"log_salary\"])\n",
    "\n",
    "ml_df = ml_df.dropna(subset=[\"job_role\", \"location\", \"year\", \"salary_clean\"])\n",
    "\n",
    "ml_df = ml_df.cache()\n",
    "print(\"ML dataframe after dropna:\", ml_df.count())\n",
    "\n",
    "ml_df.groupBy(\"job_role\").agg(F.count(\"*\").alias(\"cnt\"), F.avg(\"salary_clean\").alias(\"avg_salary\")).orderBy(\"avg_salary\").show(50, truncate=False)\n",
    "\n",
    "ml_df.select(F.mean(\"salary_clean\"), F.stddev(\"salary_clean\"), F.min(\"salary_clean\"), F.max(\"salary_clean\")).show()\n",
    "\n",
    "\n",
    "hasher_input_cols = []\n",
    "if \"job_role\" in ml_df.columns: hasher_input_cols.append(\"job_role\")\n",
    "if \"location\" in ml_df.columns: hasher_input_cols.append(\"location\")\n",
    "if \"seniority_level\" in ml_df.columns: hasher_input_cols.append(\"seniority_level\")\n",
    "\n",
    "hasher = FeatureHasher(inputCols=hasher_input_cols, outputCol=\"cat_features\", numFeatures=1024) # 512\n",
    "\n",
    "assembler_input_cols = [\"cat_features\"]\n",
    "for col_name in [\"year\", \"experience_years\", \"avg_salary\", \"min_salary\", \"max_salary\", \"salary_records\"]:\n",
    "    if col_name in ml_df.columns:\n",
    "        assembler_input_cols.append(col_name)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=assembler_input_cols, outputCol=\"features\")\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"salary_clean\", predictionCol=\"prediction_salary\", numTrees=50, maxDepth=10)\n",
    "pipeline = Pipeline(stages=[hasher, assembler, rf])\n",
    "\n",
    "train_df, test_df = ml_df.randomSplit([0.8, 0.2], seed=42)\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "model_path = \"data/PART_5_fill_with_more_data/salary_rf_model\"\n",
    "model.write().overwrite().save(model_path) # we overwrite any existing model in order to prevent errors about already existing model, I guess another possible way is to either comment it out (not save it) or use try and except or maybe have a variable to control overwriting\n",
    "print(\"model saved to:\", model_path)\n",
    "\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"salary_clean\", predictionCol=\"prediction_salary\", metricName=\"rmse\")\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"salary_clean\", predictionCol=\"prediction_salary\", metricName=\"r2\")\n",
    "\n",
    "trained_off_by = evaluator_rmse.evaluate(predictions)\n",
    "trained_percentage = evaluator_r2.evaluate(predictions)\n",
    "\n",
    "print(f\"Test [How much trained data is off by]: {trained_off_by:.2f}\")\n",
    "print(f\"Test [R^2 (accuracy) of the model]: {trained_percentage:.3f}\")\n",
    "\n",
    "ml_df.unpersist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ab7ad6",
   "metadata": {},
   "source": [
    "We use RMSE and R^2 to tell how well the model is doing\n",
    "\n",
    "RMSE (Root mean squared error) -> trained_off_by --> Lower is better. RMSE basically tells us how much it can be off by.\n",
    "\n",
    "R^2 (Coefficient of determination) -> trained_percentage  --> closer to 1 the better it is. It explains variation in our data by used features.\n",
    "\n",
    "How ML works:\n",
    "- Standardize column names\n",
    "- Feature encoding (featurehasher) --> job_role and location are high-cardinality (basically with lots of unique values) strings --> turns these to numeric vector of length of 512 (cat_features / category_features)\n",
    "- VectorAssembler combines cat_features and year into features\n",
    "- We use RandomForestRegressor in order to learn mapping so that would be from features to salary_clean --> it tries to learn patterns could be that that role + location + year --> x salary.\n",
    "- We train 80% of data and use 20% for testing and then we evaluate using RMSE and R^2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d8e04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "\n",
    "loaded_model = PipelineModel.load(\"data/PART_5_fill_with_more_data/salary_rf_model\")\n",
    "silver_agg_loaded = spark.read.parquet(\"data/PART_5_fill_with_more_data/silver_agg.parquet\")\n",
    "\n",
    "agg_for_join = silver_agg_loaded\n",
    "if \"occ_title\" in agg_for_join.columns and \"job_role\" not in agg_for_join.columns:\n",
    "    agg_for_join = agg_for_join.withColumnRenamed(\"occ_title\", \"job_role\")\n",
    "if \"jobposition\" in agg_for_join.columns and \"job_role\" not in agg_for_join.columns:\n",
    "    agg_for_join = agg_for_join.withColumnRenamed(\"jobposition\", \"job_role\")\n",
    "if \"companylocation\" in agg_for_join.columns and \"location\" not in agg_for_join.columns:\n",
    "    agg_for_join = agg_for_join.withColumnRenamed(\"companylocation\", \"location\")\n",
    "if \"employeeresidence\" in agg_for_join.columns and \"location\" not in agg_for_join.columns:\n",
    "    agg_for_join = agg_for_join.withColumnRenamed(\"employeeresidence\", \"location\")\n",
    "\n",
    "job_roles = [\n",
    "    \"Data Scientist\",\n",
    "    \"Librarian\",\n",
    "    \"Research Scientist\",\n",
    "    \"Engineer\",\n",
    "]\n",
    "\n",
    "data = [\n",
    "    (role, \"US\", 2026, 0, \"EN\")\n",
    "    for role in job_roles\n",
    "]\n",
    "\n",
    "example_df = spark.createDataFrame(data, [\"job_role\", \"location\", \"year\", \"experience_years\", \"seniority_level\"])\n",
    "example_with_stats = example_df.join(agg_for_join.select(\"job_role\", \"location\", \"avg_salary\", \"min_salary\", \"max_salary\", \"salary_records\"), on=[\"job_role\", \"location\"], how=\"left\")\n",
    "example_with_stats = example_with_stats.fillna({\"avg_salary\": 0.0, \"min_salary\": 0.0, \"max_salary\": 0.0, \"salary_records\": 0.0})\n",
    "example_pred = loaded_model.transform(example_with_stats)\n",
    "example_pred = example_pred.withColumn(\"prediction_salary_rounded\",F.round(F.col(\"prediction_salary\"),2))\n",
    "example_pred.select(\"job_role\", \"location\", \"year\", \"prediction_salary_rounded\").show(truncate=False)\n",
    "display(example_pred.select(\"job_role\", \"prediction_salary_rounded\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e59d840",
   "metadata": {},
   "source": [
    "# Predicting top 10 paid jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e05406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loaded_model = PipelineModel.load(\"data/PART_5_fill_with_more_data/salary_rf_model\")\n",
    "silver_agg_loaded = spark.read.parquet(\"data/PART_5_fill_with_more_data/silver_agg.parquet\")\n",
    "gold_detail_loaded = spark.read.parquet(\"data/PART_5_fill_with_more_data/gold_detail.parquet\")\n",
    "\n",
    "agg_for_join = silver_agg_loaded\n",
    "if \"occ_title\" in agg_for_join.columns and \"job_role\" not in agg_for_join.columns:\n",
    "    agg_for_join = agg_for_join.withColumnRenamed(\"occ_title\", \"job_role\")\n",
    "if \"jobposition\" in agg_for_join.columns and \"job_role\" not in agg_for_join.columns:\n",
    "    agg_for_join = agg_for_join.withColumnRenamed(\"jobposition\", \"job_role\")\n",
    "if \"companylocation\" in agg_for_join.columns and \"location\" not in agg_for_join.columns:\n",
    "    agg_for_join = agg_for_join.withColumnRenamed(\"companylocation\", \"location\")\n",
    "if \"employeeresidence\" in agg_for_join.columns and \"location\" not in agg_for_join.columns:\n",
    "    agg_for_join = agg_for_join.withColumnRenamed(\"employeeresidence\", \"location\")\n",
    "\n",
    "ml_df = gold_detail_loaded\n",
    "\n",
    "if \"occ_title\" in ml_df.columns and \"job_role\" not in ml_df.columns: ml_df = ml_df.withColumnRenamed(\"occ_title\", \"job_role\")\n",
    "if \"jobposition\" in ml_df.columns and \"job_role\" not in ml_df.columns: ml_df = ml_df.withColumnRenamed(\"jobposition\", \"job_role\")\n",
    "if \"companylocation\" in ml_df.columns and \"location\" not in ml_df.columns: ml_df = ml_df.withColumnRenamed(\"companylocation\", \"location\")\n",
    "if \"employeeresidence\" in ml_df.columns and \"location\" not in ml_df.columns: ml_df = ml_df.withColumnRenamed(\"employeeresidence\", \"location\")\n",
    "if \"fiscal_year\" in ml_df.columns and \"year\" not in ml_df.columns: ml_df = ml_df.withColumnRenamed(\"fiscal_year\", \"year\")\n",
    "if \"senioritylevel\" in ml_df.columns and \"seniority_level\" not in ml_df.columns: ml_df = ml_df.withColumnRenamed(\"senioritylevel\", \"seniority_level\")\n",
    "if \"experienceyears\" in ml_df.columns and \"experience_years\" not in ml_df.columns: ml_df = ml_df.withColumnRenamed(\"experienceyears\", \"experience_years\")\n",
    "\n",
    "join_cols = []\n",
    "if \"job_role\" in ml_df.columns and \"job_role\" in agg_for_join.columns: join_cols.append(\"job_role\")\n",
    "if \"location\" in ml_df.columns and \"location\" in agg_for_join.columns: join_cols.append(\"location\")\n",
    "\n",
    "if join_cols:\n",
    "    ml_df = ml_df.join(\n",
    "        agg_for_join.select(\n",
    "            *join_cols,\n",
    "            \"avg_salary\",\n",
    "            \"min_salary\",\n",
    "            \"max_salary\",\n",
    "            \"salary_records\"\n",
    "        ),\n",
    "        on=join_cols,\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "if \"experience_years\" in ml_df.columns:\n",
    "    ml_df = ml_df.withColumn(\"experience_years\", F.col(\"experience_years\").cast(\"double\"))\n",
    "\n",
    "ml_df = ml_df.dropna(subset=[\"job_role\", \"location\", \"year\", \"salary_clean\"])\n",
    "\n",
    "target_year = 2025\n",
    "\n",
    "pred_input = ml_df.filter(F.col(\"year\") == target_year)\n",
    "predictions_year = loaded_model.transform(pred_input)\n",
    "\n",
    "top_jobs_df = (\n",
    "    predictions_year\n",
    "    .groupBy(\"job_role\")\n",
    "    .agg(F.avg(\"prediction_salary\").alias(\"avg_pred_salary\"))\n",
    "    .orderBy(F.desc(\"avg_pred_salary\"))\n",
    "    .limit(10)\n",
    ")\n",
    "\n",
    "top_jobs_pd = top_jobs_df.toPandas()\n",
    "\n",
    "print(\"rows for plotting:\", len(top_jobs_pd))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(top_jobs_pd[\"job_role\"], top_jobs_pd[\"avg_pred_salary\"])\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Predicted average salary\")\n",
    "plt.xlabel(\"Job role\")\n",
    "plt.title(f\"Top 10 predicted-paying job roles in {target_year}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
