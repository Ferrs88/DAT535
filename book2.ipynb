{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a8c8933",
   "metadata": {},
   "source": [
    "# Predicting Salary Trends Using Multi-Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44d03754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e950ad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions that we use futher in the code\n",
    "\n",
    "def safe_make_dir(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def year_from_path(path, base_dir):\n",
    "    rel = os.path.relpath(path, start=base_dir)\n",
    "    parts = rel.split(os.sep)\n",
    "    for part in parts:\n",
    "        if len(part) == 4 and part.isdigit():\n",
    "            return int(part)\n",
    "    for part in parts:\n",
    "        if len(part) >= 4 and part[:4].isdigit():\n",
    "            return int(part[:4])\n",
    "    return None\n",
    "\n",
    "\n",
    "def load_gender_dict(gender_from_names_file):\n",
    "    if not os.path.exists(gender_from_names_file):\n",
    "        return {}\n",
    "    with open(gender_from_names_file, \"r\") as f:\n",
    "        gender_data = json.load(f)\n",
    "    return {item[\"name\"].strip().lower(): item[\"gender\"] for item in gender_data}\n",
    "\n",
    "\n",
    "def get_gender_from_name_factory(gender_dict):\n",
    "    def _inner(name):\n",
    "        if not isinstance(name, str) or not name:\n",
    "            return \"unknown\"\n",
    "        return gender_dict.get(name.strip().lower(), \"unknown\").capitalize()\n",
    "    return _inner\n",
    "\n",
    "\n",
    "def load_country_iso_dict(country_iso_file):\n",
    "    if not os.path.exists(country_iso_file):\n",
    "        return {}, []\n",
    "    with open(country_iso_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        country_iso_data = data.get(\"3166-1\", [])\n",
    "    country_iso_dict = {\n",
    "        item[\"name\"].strip().lower(): item[\"alpha_2\"]\n",
    "        for item in country_iso_data\n",
    "        if \"name\" in item and \"alpha_2\" in item\n",
    "    }\n",
    "    alpha3_codes = [item[\"alpha_3\"] for item in country_iso_data if \"alpha_3\" in item]\n",
    "    return country_iso_dict, alpha3_codes\n",
    "\n",
    "\n",
    "def get_country_iso_factory(country_iso_dict):\n",
    "    def _inner(name):\n",
    "        if not isinstance(name, str) or not name:\n",
    "            return \"unknown\"\n",
    "        return country_iso_dict.get(name.strip().lower(), \"unknown\")\n",
    "    return _inner\n",
    "\n",
    "\n",
    "def get_random_country_iso_factory(country_iso_data):\n",
    "    def _inner():\n",
    "        if not country_iso_data:\n",
    "            return \"unknown\"\n",
    "        return random.choice(country_iso_data).get(\"alpha_2\", \"unknown\")\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e53293",
   "metadata": {},
   "source": [
    "Convert XLS to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "677adec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion completed.\n"
     ]
    }
   ],
   "source": [
    "input_directory = \"data/PART_0_xls/\"\n",
    "output_directory = \"data/PART_1_xls_to_csv/\"\n",
    "scan_depth = 300  # how many rows to scan to find the header\n",
    "\n",
    "safe_make_dir(output_directory)\n",
    "\n",
    "for root, _, files in os.walk(input_directory):\n",
    "    for file in files:\n",
    "        if not (file.lower().endswith(\".xls\") or file.lower().endswith(\".xlsx\")):\n",
    "            continue\n",
    "        if file.lower() in [\"field_descriptions.xls\", \"field_descriptions.xlsx\"]:\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(root, file)\n",
    "        save_dir = os.path.join(output_directory, os.path.relpath(root, input_directory))\n",
    "        safe_make_dir(save_dir)\n",
    "\n",
    "        engine = \"openpyxl\" if file.lower().endswith(\".xlsx\") else \"xlrd\"\n",
    "        xls = pd.ExcelFile(file_path, engine=engine)\n",
    "\n",
    "        for sheet in xls.sheet_names:\n",
    "            if sheet.lower() in [\"field descriptions\", \"updatetime\"]:\n",
    "                continue\n",
    "\n",
    "            df_raw = pd.read_excel(file_path, sheet_name=sheet, header=None, engine=engine)\n",
    "\n",
    "            if df_raw.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            header_row = 0\n",
    "            for i in range(min(scan_depth, df_raw.shape[0])):\n",
    "                row_values = [str(x).strip().lower() for x in df_raw.iloc[i]]\n",
    "                if \"occ_code\" in row_values:\n",
    "                    header_row = i\n",
    "                    break\n",
    "\n",
    "            header = df_raw.iloc[header_row].astype(str).str.strip()\n",
    "            df = df_raw.iloc[header_row + 1 :].copy()\n",
    "            df.columns = [str(x).strip() for x in header]\n",
    "\n",
    "            suffix = f\"_{sheet}\" if len(xls.sheet_names) > 1 else \"\"\n",
    "            out_path = os.path.join(save_dir, f\"{os.path.splitext(file)[0]}{suffix}.csv\")\n",
    "\n",
    "            df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"Conversion completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaf684fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differences compared to the first header:\n",
      "\n",
      "Baseline header: ('occ_code', 'occ_title', 'occ_group', 'tot_emp', 'emp_prse', 'h_mean', 'a_mean', 'mean_prse', 'h_pct10', 'h_pct25', 'h_median', 'h_pct75', 'h_pct90', 'a_pct10', 'a_pct25', 'a_median', 'a_pct75', 'a_pct90', 'annual', 'hourly') \n",
      "\n",
      "Header format #2: ('area', 'area_title', 'area_type', 'prim_state', 'naics', 'naics_title', 'i_group', 'own_code', 'occ_code', 'occ_title', 'o_group', 'tot_emp', 'emp_prse', 'jobs_1000', 'loc_quotient', 'pct_total', 'pct_rpt', 'h_mean', 'a_mean', 'mean_prse', 'h_pct10', 'h_pct25', 'h_median', 'h_pct75', 'h_pct90', 'a_pct10', 'a_pct25', 'a_median', 'a_pct75', 'a_pct90', 'annual', 'hourly')\n",
      "  Missing columns: ['occ_group']\n",
      "  Extra columns:   ['area', 'area_title', 'area_type', 'i_group', 'jobs_1000', 'loc_quotient', 'naics', 'naics_title', 'o_group', 'own_code', 'pct_rpt', 'pct_total', 'prim_state']\n",
      "Files:\n",
      " - data/PART_1_xls_to_csv/2022/national_M2022_dl_national_M2022_dl.csv\n",
      " - data/PART_1_xls_to_csv/2024/national_M2024_dl_national_M2024_dl.csv\n",
      " - data/PART_1_xls_to_csv/2023/national_M2023_dl_national_M2023_dl.csv\n",
      " - data/PART_1_xls_to_csv/2021/national_M2021_dl_national_M2021_dl.csv\n",
      "\n",
      "Header format #3: ('occ_code', 'occ_title', 'group', 'tot_emp', 'emp_prse', 'h_mean', 'a_mean', 'mean_prse', 'h_pct10', 'h_pct25', 'h_median', 'h_pct75', 'h_pct90', 'a_pct10', 'a_pct25', 'a_median', 'a_pct75', 'a_pct90', 'annual', 'hourly')\n",
      "  Missing columns: ['occ_group']\n",
      "  Extra columns:   ['group']\n",
      "Files:\n",
      " - data/PART_1_xls_to_csv/2008/national__M2008_dl.csv\n",
      " - data/PART_1_xls_to_csv/2006/national_may2006_dl.csv\n",
      " - data/PART_1_xls_to_csv/2007/national_May2007_dl.csv\n",
      " - data/PART_1_xls_to_csv/2009/national_dl.csv\n",
      " - data/PART_1_xls_to_csv/2010/national_M2010_dl.csv\n",
      " - data/PART_1_xls_to_csv/2011/national_M2011_dl.csv\n",
      " - data/PART_1_xls_to_csv/2005/national_may2005_dl.csv\n",
      " - data/PART_1_xls_to_csv/2004/national_november2004_dl.csv\n",
      " - data/PART_1_xls_to_csv/2004/national_may2004_dl.csv\n",
      "\n",
      "Header format #4: ('occ_code', 'occ_title', 'group', 'tot_emp', 'emp_prse', 'h_mean', 'a_mean', 'mean_prse', 'h_wpct10', 'h_wpct25', 'h_median', 'h_wpct75', 'h_wpct90', 'a_wpct10', 'a_wpct25', 'a_median', 'a_wpct75', 'a_wpct90', 'annual', 'year')\n",
      "  Missing columns: ['a_pct10', 'a_pct25', 'a_pct75', 'a_pct90', 'h_pct10', 'h_pct25', 'h_pct75', 'h_pct90', 'hourly', 'occ_group']\n",
      "  Extra columns:   ['a_wpct10', 'a_wpct25', 'a_wpct75', 'a_wpct90', 'group', 'h_wpct10', 'h_wpct25', 'h_wpct75', 'h_wpct90', 'year']\n",
      "Files:\n",
      " - data/PART_1_xls_to_csv/2001/national_2001_dl.csv\n",
      "\n",
      "Header format #5: ('occ_code', 'occ_titl', 'group', 'tot_emp', 'emp_prse', 'h_mean', 'a_mean', 'mean_prse', 'h_wpct10', 'h_wpct25', 'h_median', 'h_wpct75', 'h_wpct90', 'a_wpct10', 'a_wpct25', 'a_median', 'a_wpct75', 'a_wpct90', 'annual', 'year')\n",
      "  Missing columns: ['a_pct10', 'a_pct25', 'a_pct75', 'a_pct90', 'h_pct10', 'h_pct25', 'h_pct75', 'h_pct90', 'hourly', 'occ_group', 'occ_title']\n",
      "  Extra columns:   ['a_wpct10', 'a_wpct25', 'a_wpct75', 'a_wpct90', 'group', 'h_wpct10', 'h_wpct25', 'h_wpct75', 'h_wpct90', 'occ_titl', 'year']\n",
      "Files:\n",
      " - data/PART_1_xls_to_csv/2000/national_2000_dl_Sheet1.csv\n",
      "\n",
      "Header format #6: ('area', 'area_title', 'area_type', 'naics', 'naics_title', 'i_group', 'own_code', 'occ_code', 'occ_title', 'o_group', 'tot_emp', 'emp_prse', 'jobs_1000', 'loc_quotient', 'pct_total', 'h_mean', 'a_mean', 'mean_prse', 'h_pct10', 'h_pct25', 'h_median', 'h_pct75', 'h_pct90', 'a_pct10', 'a_pct25', 'a_median', 'a_pct75', 'a_pct90', 'annual', 'hourly')\n",
      "  Missing columns: ['occ_group']\n",
      "  Extra columns:   ['area', 'area_title', 'area_type', 'i_group', 'jobs_1000', 'loc_quotient', 'naics', 'naics_title', 'o_group', 'own_code', 'pct_total']\n",
      "Files:\n",
      " - data/PART_1_xls_to_csv/2019/national_M2019_dl_national_M2019_dl.csv\n",
      "\n",
      "Header format #7: ('area', 'area_title', 'area_type', 'prim_state', 'naics', 'naics_title', 'i_group', 'own_code', 'occ_code', 'occ_title', 'o_group', 'tot_emp', 'emp_prse', 'jobs_1000', 'loc_quotient', 'pct_total', 'h_mean', 'a_mean', 'mean_prse', 'h_pct10', 'h_pct25', 'h_median', 'h_pct75', 'h_pct90', 'a_pct10', 'a_pct25', 'a_median', 'a_pct75', 'a_pct90', 'annual', 'hourly')\n",
      "  Missing columns: ['occ_group']\n",
      "  Extra columns:   ['area', 'area_title', 'area_type', 'i_group', 'jobs_1000', 'loc_quotient', 'naics', 'naics_title', 'o_group', 'own_code', 'pct_total', 'prim_state']\n",
      "Files:\n",
      " - data/PART_1_xls_to_csv/2020/national_M2020_dl_national_M2020_dl.csv\n",
      "\n",
      "Header format #8: ('occ_code', 'occ_titl', 'group', 'tot_emp', 'emp_prse', 'h_mean', 'a_mean', 'mean_prse', 'h_wpct10', 'h_wpct25', 'h_median', 'h_wpct75', 'h_wpct90', 'a_wpct10', 'a_wpct25', 'a_median', 'a_wpct75', 'a_wpct90', 'annual')\n",
      "  Missing columns: ['a_pct10', 'a_pct25', 'a_pct75', 'a_pct90', 'h_pct10', 'h_pct25', 'h_pct75', 'h_pct90', 'hourly', 'occ_group', 'occ_title']\n",
      "  Extra columns:   ['a_wpct10', 'a_wpct25', 'a_wpct75', 'a_wpct90', 'group', 'h_wpct10', 'h_wpct25', 'h_wpct75', 'h_wpct90', 'occ_titl']\n",
      "Files:\n",
      " - data/PART_1_xls_to_csv/2002/national_2002_dl.csv\n",
      "\n",
      "Header format #9: ('occ_code', 'occ_title', 'group', 'tot_emp', 'emp_prse', 'h_mean', 'a_mean', 'mean_prse', 'h_pct10', 'h_pct25', 'h_median', 'h_pct75', 'h_pct90', 'a_pct10', 'a_pct25', 'a_median', 'a_pct75', 'a_pct90', 'annual')\n",
      "  Missing columns: ['hourly', 'occ_group']\n",
      "  Extra columns:   ['group']\n",
      "Files:\n",
      " - data/PART_1_xls_to_csv/2003/national_may2003_dl.csv\n",
      " - data/PART_1_xls_to_csv/2003/national_november2003_dl.csv\n",
      "\n",
      "Wrote 9731 rows from 7 files -> data/PART_2_xls_to_csv_combined_header/group_1.csv\n",
      "Wrote 5611 rows from 4 files -> data/PART_2_xls_to_csv_combined_header/group_2.csv\n",
      "Wrote 7396 rows from 9 files -> data/PART_2_xls_to_csv_combined_header/group_3.csv\n",
      "Wrote 733 rows from 1 files -> data/PART_2_xls_to_csv_combined_header/group_4.csv\n",
      "Wrote 1467 rows from 2 files -> data/PART_2_xls_to_csv_combined_header/group_5.csv\n",
      "Wrote 1329 rows from 1 files -> data/PART_2_xls_to_csv_combined_header/group_6.csv\n",
      "Wrote 1329 rows from 1 files -> data/PART_2_xls_to_csv_combined_header/group_7.csv\n",
      "Wrote 1468 rows from 2 files -> data/PART_2_xls_to_csv_combined_header/group_8.csv\n",
      "Wrote 5611 rows from 4 files -> data/PART_2_xls_to_csv_combined_header/group_2.csv\n",
      "Wrote 7396 rows from 9 files -> data/PART_2_xls_to_csv_combined_header/group_3.csv\n",
      "Wrote 733 rows from 1 files -> data/PART_2_xls_to_csv_combined_header/group_4.csv\n",
      "Wrote 1467 rows from 2 files -> data/PART_2_xls_to_csv_combined_header/group_5.csv\n",
      "Wrote 1329 rows from 1 files -> data/PART_2_xls_to_csv_combined_header/group_6.csv\n",
      "Wrote 1329 rows from 1 files -> data/PART_2_xls_to_csv_combined_header/group_7.csv\n",
      "Wrote 1468 rows from 2 files -> data/PART_2_xls_to_csv_combined_header/group_8.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'data/PART_2_xls_to_csv_combined_header/group_1.csv': ['data/PART_1_xls_to_csv/2013/national_M2013_dl.csv',\n",
       "  'data/PART_1_xls_to_csv/2014/national_M2014_dl.csv',\n",
       "  'data/PART_1_xls_to_csv/2015/national_M2015_dl.csv',\n",
       "  'data/PART_1_xls_to_csv/2012/national_M2012_dl.csv',\n",
       "  'data/PART_1_xls_to_csv/2017/national_M2017_dl.csv',\n",
       "  'data/PART_1_xls_to_csv/2018/national_M2018_dl.csv',\n",
       "  'data/PART_1_xls_to_csv/2016/national_M2016_dl.csv'],\n",
       " 'data/PART_2_xls_to_csv_combined_header/group_2.csv': ['data/PART_1_xls_to_csv/2022/national_M2022_dl_national_M2022_dl.csv',\n",
       "  'data/PART_1_xls_to_csv/2024/national_M2024_dl_national_M2024_dl.csv',\n",
       "  'data/PART_1_xls_to_csv/2023/national_M2023_dl_national_M2023_dl.csv',\n",
       "  'data/PART_1_xls_to_csv/2021/national_M2021_dl_national_M2021_dl.csv'],\n",
       " 'data/PART_2_xls_to_csv_combined_header/group_3.csv': ['data/PART_1_xls_to_csv/2008/national__M2008_dl.csv',\n",
       "  'data/PART_1_xls_to_csv/2006/national_may2006_dl.csv',\n",
       "  'data/PART_1_xls_to_csv/2007/national_May2007_dl.csv',\n",
       "  'data/PART_1_xls_to_csv/2009/national_dl.csv',\n",
       "  'data/PART_1_xls_to_csv/2010/national_M2010_dl.csv',\n",
       "  'data/PART_1_xls_to_csv/2011/national_M2011_dl.csv',\n",
       "  'data/PART_1_xls_to_csv/2005/national_may2005_dl.csv',\n",
       "  'data/PART_1_xls_to_csv/2004/national_november2004_dl.csv',\n",
       "  'data/PART_1_xls_to_csv/2004/national_may2004_dl.csv'],\n",
       " 'data/PART_2_xls_to_csv_combined_header/group_4.csv': ['data/PART_1_xls_to_csv/2001/national_2001_dl.csv'],\n",
       " 'data/PART_2_xls_to_csv_combined_header/group_5.csv': ['data/PART_1_xls_to_csv/2000/national_2000_dl_Sheet1.csv',\n",
       "  'data/PART_1_xls_to_csv/2002/national_2002_dl.csv'],\n",
       " 'data/PART_2_xls_to_csv_combined_header/group_6.csv': ['data/PART_1_xls_to_csv/2019/national_M2019_dl_national_M2019_dl.csv'],\n",
       " 'data/PART_2_xls_to_csv_combined_header/group_7.csv': ['data/PART_1_xls_to_csv/2020/national_M2020_dl_national_M2020_dl.csv'],\n",
       " 'data/PART_2_xls_to_csv_combined_header/group_8.csv': ['data/PART_1_xls_to_csv/2003/national_may2003_dl.csv',\n",
       "  'data/PART_1_xls_to_csv/2003/national_november2003_dl.csv']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \"data/PART_1_xls_to_csv/\"\n",
    "out_dir = \"data/PART_2_xls_to_csv_combined_header/\"\n",
    "\n",
    "safe_make_dir(out_dir)\n",
    "\n",
    "def collect_csv_files(base_dir, ext=\".csv\", **kwargs):\n",
    "    file_infos = []\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if not file.lower().endswith(ext.lower()):\n",
    "                continue\n",
    "            path = os.path.join(root, file)\n",
    "            try:\n",
    "                df = pd.read_csv(path, **kwargs)\n",
    "                df.columns = [col.strip().lower() for col in df.columns]\n",
    "                header = tuple(df.columns)\n",
    "                year = year_from_path(path, base_dir)\n",
    "                file_infos.append({\"path\": path, \"df\": df, \"header\": header, \"year\": year})\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {path}: {e}\")\n",
    "    return file_infos\n",
    "\n",
    "\n",
    "\n",
    "def check_header_differences(file_infos):\n",
    "    if not file_infos:\n",
    "        print(\"No CSV files found.\")\n",
    "        return\n",
    "    header_map = {}\n",
    "    for info in file_infos:\n",
    "        header_map.setdefault(info[\"header\"], []).append(info[\"path\"])\n",
    "    headers = list(header_map.keys())\n",
    "    base = headers[0]\n",
    "    if len(headers) == 1:\n",
    "        print(\"All CSV files have the same (lowercased) header format\")\n",
    "        return\n",
    "    print(\"Differences compared to the first header:\\n\")\n",
    "    print(\"Baseline header:\", base, \"\\n\")\n",
    "    base_set = set(base)\n",
    "    for i, hdr in enumerate(headers[1:], start=2):\n",
    "        missing = sorted(base_set - set(hdr))\n",
    "        extra = sorted(set(hdr) - base_set)\n",
    "        print(f\"Header format #{i}: {hdr}\")\n",
    "        if missing:\n",
    "            print(f\"  Missing columns: {missing}\")\n",
    "        if extra:\n",
    "            print(f\"  Extra columns:   {extra}\")\n",
    "        print(\"Files:\")\n",
    "        for p in header_map[hdr]:\n",
    "            print(f\" - {p}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "\n",
    "def combine_same_format_files(file_infos, out_dir, base_dir, add_source=True):\n",
    "    safe_make_dir(out_dir)\n",
    "    groups = {}\n",
    "    for info in file_infos:\n",
    "        df = info[\"df\"]\n",
    "        header = list(info[\"header\"])\n",
    "        cols_with_year = list(header)\n",
    "        if \"year\" not in cols_with_year:\n",
    "            cols_with_year.append(\"year\")\n",
    "        key = frozenset(cols_with_year)\n",
    "        if key not in groups:\n",
    "            order = list(header)\n",
    "            if \"year\" not in order:\n",
    "                order.append(\"year\")\n",
    "            groups[key] = {\"order\": order, \"files\": []}\n",
    "        groups[key][\"files\"].append(info)\n",
    "    if not groups:\n",
    "        print(\"No CSV files found.\")\n",
    "        return {}\n",
    "    outputs = {}\n",
    "    for idx, (key, info) in enumerate(groups.items(), start=1):\n",
    "        order = info[\"order\"]\n",
    "        out_path = os.path.join(out_dir, f\"group_{idx}.csv\")\n",
    "        frames = []\n",
    "        for file_info in info[\"files\"]:\n",
    "            df = file_info[\"df\"].copy()\n",
    "            if \"year\" not in df.columns:\n",
    "                yr = file_info[\"year\"]\n",
    "                df[\"year\"] = yr if yr is not None else pd.NA\n",
    "            if add_source:\n",
    "                df[\"__source_file\"] = os.path.relpath(file_info[\"path\"], start=base_dir)\n",
    "            cols = order + ([\"__source_file\"] if add_source else [])\n",
    "            df = df.reindex(columns=cols)\n",
    "            frames.append(df)\n",
    "        if not frames:\n",
    "            print(f\"(skip) No readable files for group #{idx}\")\n",
    "            continue\n",
    "        combined = pd.concat(frames, ignore_index=True)\n",
    "        combined.to_csv(out_path, index=False)\n",
    "        outputs[out_path] = [fi[\"path\"] for fi in info[\"files\"]]\n",
    "        print(f\"Wrote {len(combined)} rows from {len(info['files'])} files -> {out_path}\")\n",
    "    return outputs\n",
    "\n",
    "file_infos = collect_csv_files(data)\n",
    "check_header_differences(file_infos)\n",
    "combine_same_format_files(file_infos, out_dir, base_dir=data, add_source=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98f00db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of combined data:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60711 entries, 0 to 60710\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Department      60711 non-null  object \n",
      " 1   DepartmentName  60711 non-null  object \n",
      " 2   Division        60711 non-null  object \n",
      " 3   Gender          60711 non-null  object \n",
      " 4   BaseSalary      60711 non-null  float64\n",
      " 5   OvertimePay     52133 non-null  float64\n",
      " 6   LongevityPay    45564 non-null  float64\n",
      " 7   Grade           60617 non-null  object \n",
      " 8   Year            60711 non-null  int64  \n",
      "dtypes: float64(3), int64(1), object(5)\n",
      "memory usage: 4.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dataframes = []\n",
    "input_directory = 'data/PART_2_1_CSV_EMPLOYEE_SALARIES'\n",
    "output_file = 'data/PART_3_csv/combined_employee_salaries.csv'\n",
    "\n",
    "for filename in os.listdir(input_directory):\n",
    "    if not filename.endswith('.csv'):\n",
    "        continue\n",
    "    file_path = os.path.join(input_directory, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # 2020 Overtime Pay; 2021 Overtime Pay; 2019 Overtime Pay; 2024 Overtime Pay; Overtime Pay; Overtime_Pay\n",
    "    # 2024 Longevity Pay; 2019 Longevity Pay; 2021 Longevity Pay; 2020 Longevity Pay; Longevity_Pay\n",
    "\n",
    "    for col in df.columns:\n",
    "        column = col.lower()\n",
    "        if column in ['2020 overtime pay', '2021 overtime pay', '2019 overtime pay', '2024 overtime pay', 'overtime pay', 'overtime_pay']: # rename to unified column\n",
    "            df.rename(columns={col: 'OvertimePay'}, inplace=True)\n",
    "        if column in ['2024 longevity pay', '2019 longevity pay', '2021 longevity pay', '2020 longevity pay', 'longevity pay', 'longevity_pay']:\n",
    "            df.rename(columns={col: 'LongevityPay'}, inplace=True)\n",
    "        if column in ['department name', 'department_name']:\n",
    "            df.rename(columns={col: 'DepartmentName'}, inplace=True)\n",
    "        if column in ['base salary', 'base_salary']:\n",
    "            df.rename(columns={col: 'BaseSalary'}, inplace=True)\n",
    "        if 'year' not in df.columns: # TODO: perhaps remake it so that it's not hardcoded\n",
    "            if '2019' in filename:\n",
    "                df['Year'] = 2019\n",
    "            elif '2020' in filename:\n",
    "                df['Year'] = 2020\n",
    "            elif '2021' in filename:\n",
    "                df['Year'] = 2021\n",
    "            elif '2022' in filename:\n",
    "                df['Year'] = 2022\n",
    "            elif '2023' in filename:\n",
    "                df['Year'] = 2023\n",
    "            elif '2024' in filename:\n",
    "                df['Year'] = 2024\n",
    "            else:\n",
    "                df['Year'] = None\n",
    "    dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "print (\"Summary of combined data:\")\n",
    "print (combined_df.info())\n",
    "combined_df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68c58779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of combined data:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 217396 entries, 0 to 217395\n",
      "Data columns (total 11 columns):\n",
      " #   Column              Non-Null Count   Dtype \n",
      "---  ------              --------------   ----- \n",
      " 0   work_year           217396 non-null  int64 \n",
      " 1   experience_level    217396 non-null  object\n",
      " 2   employment_type     217396 non-null  object\n",
      " 3   job_title           217396 non-null  object\n",
      " 4   salary              217396 non-null  int64 \n",
      " 5   salary_currency     217396 non-null  object\n",
      " 6   salary_in_usd       217396 non-null  int64 \n",
      " 7   employee_residence  217396 non-null  object\n",
      " 8   remote_ratio        217396 non-null  int64 \n",
      " 9   company_location    217396 non-null  object\n",
      " 10  company_size        217396 non-null  object\n",
      "dtypes: int64(4), object(7)\n",
      "memory usage: 18.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dataframes = []\n",
    "input_directory = 'data/PART_2_2_SALARIES'\n",
    "output_file = 'data/PART_3_csv/combined_salaries.csv'\n",
    "\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(input_directory, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "print (\"Summary of combined data:\")\n",
    "print (combined_df.info())\n",
    "combined_df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e31c369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Directory: data/PART_2_xls_to_csv_combined_header/ ===\n",
      "Before:\n",
      "--> Differences compared to the first header:\n",
      "\n",
      "Baseline header: ('occ_code', 'occ_title', 'group', 'tot_emp', 'emp_prse', 'h_mean', 'a_mean', 'mean_prse', 'h_pct10', 'h_pct25', 'h_median', 'h_pct75', 'h_pct90', 'a_pct10', 'a_pct25', 'a_median', 'a_pct75', 'a_pct90', 'annual', 'year') \n",
      "\n",
      "Header format #2: ('occ_code', 'occ_titl', 'group', 'tot_emp', 'emp_prse', 'h_mean', 'a_mean', 'mean_prse', 'h_wpct10', 'h_wpct25', 'h_median', 'h_wpct75', 'h_wpct90', 'a_wpct10', 'a_wpct25', 'a_median', 'a_wpct75', 'a_wpct90', 'annual', 'year')\n",
      "  Missing columns: ['a_pct10', 'a_pct25', 'a_pct75', 'a_pct90', 'h_pct10', 'h_pct25', 'h_pct75', 'h_pct90', 'occ_title']\n",
      "  Extra columns:   ['a_wpct10', 'a_wpct25', 'a_wpct75', 'a_wpct90', 'h_wpct10', 'h_wpct25', 'h_wpct75', 'h_wpct90', 'occ_titl']\n",
      "Files:\n",
      " - data/PART_2_xls_to_csv_combined_header/group_9.csv\n",
      " - data/PART_2_xls_to_csv_combined_header/group_5.csv\n",
      "\n",
      "Header format #3: ('area', 'area_title', 'area_type', 'prim_state', 'naics', 'naics_title', 'i_group', 'own_code', 'occ_code', 'occ_title', 'o_group', 'tot_emp', 'emp_prse', 'jobs_1000', 'loc_quotient', 'pct_total', 'pct_rpt', 'h_mean', 'a_mean', 'mean_prse', 'h_pct10', 'h_pct25', 'h_median', 'h_pct75', 'h_pct90', 'a_pct10', 'a_pct25', 'a_median', 'a_pct75', 'a_pct90', 'annual', 'hourly', 'year')\n",
      "  Missing columns: ['group']\n",
      "  Extra columns:   ['area', 'area_title', 'area_type', 'hourly', 'i_group', 'jobs_1000', 'loc_quotient', 'naics', 'naics_title', 'o_group', 'own_code', 'pct_rpt', 'pct_total', 'prim_state']\n",
      "Files:\n",
      " - data/PART_2_xls_to_csv_combined_header/group_2.csv\n",
      "\n",
      "Header format #4: ('occ_code', 'occ_title', 'group', 'tot_emp', 'emp_prse', 'h_mean', 'a_mean', 'mean_prse', 'h_pct10', 'h_pct25', 'h_median', 'h_pct75', 'h_pct90', 'a_pct10', 'a_pct25', 'a_median', 'a_pct75', 'a_pct90', 'annual', 'hourly', 'year')\n",
      "  Extra columns:   ['hourly']\n",
      "Files:\n",
      " - data/PART_2_xls_to_csv_combined_header/group_3.csv\n",
      "\n",
      "Header format #5: ('occ_code', 'occ_title', 'occ_group', 'tot_emp', 'emp_prse', 'h_mean', 'a_mean', 'mean_prse', 'h_pct10', 'h_pct25', 'h_median', 'h_pct75', 'h_pct90', 'a_pct10', 'a_pct25', 'a_median', 'a_pct75', 'a_pct90', 'annual', 'hourly', 'year')\n",
      "  Missing columns: ['group']\n",
      "  Extra columns:   ['hourly', 'occ_group']\n",
      "Files:\n",
      " - data/PART_2_xls_to_csv_combined_header/group_1.csv\n",
      "\n",
      "Header format #6: ('occ_code', 'occ_title', 'group', 'tot_emp', 'emp_prse', 'h_mean', 'a_mean', 'mean_prse', 'h_wpct10', 'h_wpct25', 'h_median', 'h_wpct75', 'h_wpct90', 'a_wpct10', 'a_wpct25', 'a_median', 'a_wpct75', 'a_wpct90', 'annual', 'year')\n",
      "  Missing columns: ['a_pct10', 'a_pct25', 'a_pct75', 'a_pct90', 'h_pct10', 'h_pct25', 'h_pct75', 'h_pct90']\n",
      "  Extra columns:   ['a_wpct10', 'a_wpct25', 'a_wpct75', 'a_wpct90', 'h_wpct10', 'h_wpct25', 'h_wpct75', 'h_wpct90']\n",
      "Files:\n",
      " - data/PART_2_xls_to_csv_combined_header/group_4.csv\n",
      "\n",
      "Header format #7: ('area', 'area_title', 'area_type', 'prim_state', 'naics', 'naics_title', 'i_group', 'own_code', 'occ_code', 'occ_title', 'o_group', 'tot_emp', 'emp_prse', 'jobs_1000', 'loc_quotient', 'pct_total', 'h_mean', 'a_mean', 'mean_prse', 'h_pct10', 'h_pct25', 'h_median', 'h_pct75', 'h_pct90', 'a_pct10', 'a_pct25', 'a_median', 'a_pct75', 'a_pct90', 'annual', 'hourly', 'year')\n",
      "  Missing columns: ['group']\n",
      "  Extra columns:   ['area', 'area_title', 'area_type', 'hourly', 'i_group', 'jobs_1000', 'loc_quotient', 'naics', 'naics_title', 'o_group', 'own_code', 'pct_total', 'prim_state']\n",
      "Files:\n",
      " - data/PART_2_xls_to_csv_combined_header/group_7.csv\n",
      "\n",
      "Header format #8: ('area', 'area_title', 'area_type', 'naics', 'naics_title', 'i_group', 'own_code', 'occ_code', 'occ_title', 'o_group', 'tot_emp', 'emp_prse', 'jobs_1000', 'loc_quotient', 'pct_total', 'h_mean', 'a_mean', 'mean_prse', 'h_pct10', 'h_pct25', 'h_median', 'h_pct75', 'h_pct90', 'a_pct10', 'a_pct25', 'a_median', 'a_pct75', 'a_pct90', 'annual', 'hourly', 'year')\n",
      "  Missing columns: ['group']\n",
      "  Extra columns:   ['area', 'area_title', 'area_type', 'hourly', 'i_group', 'jobs_1000', 'loc_quotient', 'naics', 'naics_title', 'o_group', 'own_code', 'pct_total']\n",
      "Files:\n",
      " - data/PART_2_xls_to_csv_combined_header/group_6.csv\n",
      "\n",
      "→ data/PART_2_xls_to_csv_combined_header/group_9.csv\n",
      "   - occ_titl → occ_title (renamed)\n",
      "   - a_wpct10 → a_pct10 (renamed)\n",
      "   - a_wpct25 → a_pct25 (renamed)\n",
      "   - a_wpct75 → a_pct75 (renamed)\n",
      "   - a_wpct90 → a_pct90 (renamed)\n",
      "   - h_wpct10 → h_pct10 (renamed)\n",
      "   - h_wpct25 → h_pct25 (renamed)\n",
      "   - h_wpct75 → h_pct75 (renamed)\n",
      "   - h_wpct90 → h_pct90 (renamed)\n",
      "→ data/PART_2_xls_to_csv_combined_header/group_2.csv\n",
      "   - o_group → group (renamed)\n",
      "→ data/PART_2_xls_to_csv_combined_header/group_1.csv\n",
      "   - occ_group → group (renamed)\n",
      "→ data/PART_2_xls_to_csv_combined_header/group_4.csv\n",
      "   - a_wpct10 → a_pct10 (renamed)\n",
      "   - a_wpct25 → a_pct25 (renamed)\n",
      "   - a_wpct75 → a_pct75 (renamed)\n",
      "   - a_wpct90 → a_pct90 (renamed)\n",
      "   - h_wpct10 → h_pct10 (renamed)\n",
      "   - h_wpct25 → h_pct25 (renamed)\n",
      "   - h_wpct75 → h_pct75 (renamed)\n",
      "   - h_wpct90 → h_pct90 (renamed)\n",
      "→ data/PART_2_xls_to_csv_combined_header/group_5.csv\n",
      "   - occ_titl → occ_title (renamed)\n",
      "   - a_wpct10 → a_pct10 (renamed)\n",
      "   - a_wpct25 → a_pct25 (renamed)\n",
      "   - a_wpct75 → a_pct75 (renamed)\n",
      "   - a_wpct90 → a_pct90 (renamed)\n",
      "   - h_wpct10 → h_pct10 (renamed)\n",
      "   - h_wpct25 → h_pct25 (renamed)\n",
      "   - h_wpct75 → h_pct75 (renamed)\n",
      "   - h_wpct90 → h_pct90 (renamed)\n",
      "→ data/PART_2_xls_to_csv_combined_header/group_7.csv\n",
      "   - o_group → group (renamed)\n",
      "→ data/PART_2_xls_to_csv_combined_header/group_6.csv\n",
      "   - o_group → group (renamed)\n",
      "\n",
      "After:\n",
      "--> All CSV files have the same (lowercased) header format.\n",
      "Combined 10 files, 31266 rows -> data/PART_3_csv/all_combined_data.csv\n",
      "Combined 10 files, 31266 rows -> data/PART_3_csv/all_combined_data.csv\n"
     ]
    }
   ],
   "source": [
    "data_directory = \"data/PART_2_xls_to_csv_combined_header/\"\n",
    "combined_output_path = \"data/PART_3_csv/all_combined_data.csv\"\n",
    "\n",
    "def _coalesce_into(df, target, src, changes): # helper to coalesced and rename columns\n",
    "    if src not in df.columns:\n",
    "        return\n",
    "    if target in df.columns:\n",
    "        df[target] = df[target].where(df[target].notna(), df[src])\n",
    "        df.drop(columns=[src], inplace=True)\n",
    "        changes.append(f\"{src} → {target} (coalesced)\")\n",
    "    else:\n",
    "        df.rename(columns={src: target}, inplace=True)\n",
    "        changes.append(f\"{src} → {target} (renamed)\")\n",
    "\n",
    "\n",
    "def unify_schema_in_df(df):\n",
    "    changes = []\n",
    "    df = df.copy()\n",
    "\n",
    "    _coalesce_into(df, \"occ_title\", \"occ_titl\", changes) # combine similar columns\n",
    "    _coalesce_into(df, \"group\", \"o_group\", changes)\n",
    "    _coalesce_into(df, \"group\", \"occ_group\", changes)\n",
    "\n",
    "    cols = list(df.columns)\n",
    "    for c in cols:\n",
    "        if c.startswith(\"a_wpct\"):\n",
    "            target = \"a_pct\" + c[len(\"a_wpct\"):]\n",
    "            _coalesce_into(df, target, c, changes)\n",
    "    cols = list(df.columns)\n",
    "    for c in cols:\n",
    "        if c.startswith(\"h_wpct\"):\n",
    "            target = \"h_pct\" + c[len(\"h_wpct\"):]\n",
    "            _coalesce_into(df, target, c, changes)\n",
    "\n",
    "    return df, changes\n",
    "\n",
    "\n",
    "def collect_csv_files(base_dir, **read_csv_kwargs):\n",
    "    file_infos = []\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if not file.lower().endswith(\".csv\"):\n",
    "                continue\n",
    "            path = os.path.join(root, file)\n",
    "            try:\n",
    "                df = pd.read_csv(path, **read_csv_kwargs)\n",
    "                df.columns = [col.strip().lower() for col in df.columns]\n",
    "                header = tuple(df.columns)\n",
    "                file_infos.append({\"path\": path, \"df\": df, \"header\": header})\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {path}: {e}\")\n",
    "    return file_infos\n",
    "\n",
    "\n",
    "def build_header_map(file_infos):\n",
    "    header_map = {}\n",
    "    for info in file_infos:\n",
    "        header_map.setdefault(info[\"header\"], []).append(info[\"path\"]) # group files by header\n",
    "    return header_map\n",
    "\n",
    "\n",
    "def check_header_differences(header_map):\n",
    "    if not header_map:\n",
    "        print(\"No CSV files found.\")\n",
    "        return\n",
    "\n",
    "    headers = list(header_map.keys())\n",
    "    base = headers[0]\n",
    "    if len(headers) == 1:\n",
    "        print(\"--> All CSV files have the same (lowercased) header format.\")\n",
    "        return\n",
    "\n",
    "    print(\"--> Differences compared to the first header:\\n\")\n",
    "    print(\"Baseline header:\", base, \"\\n\")\n",
    "    base_set = set(base)\n",
    "\n",
    "    for i, hdr in enumerate(headers[1:], start=2):\n",
    "        missing = sorted(base_set - set(hdr)) # find missing columns compared to baseline\n",
    "        extra = sorted(set(hdr) - base_set) # find extra columns compared to baseline\n",
    "        print(f\"Header format #{i}: {hdr}\")\n",
    "        if missing:\n",
    "            print(f\"  Missing columns: {missing}\")\n",
    "        if extra:\n",
    "            print(f\"  Extra columns:   {extra}\")\n",
    "        print(\"Files:\")\n",
    "        for p in header_map[hdr]:\n",
    "            print(f\" - {p}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "def expand_to_include_extra_columns(file_infos):\n",
    "    if not file_infos:\n",
    "        return\n",
    "\n",
    "    headers = [info[\"header\"] for info in file_infos]\n",
    "    base = list(headers[0]) # use the first header as the base\n",
    "    base_set = set(base) # convert base header to set\n",
    "\n",
    "    any_missing_from_base = False\n",
    "    extra_cols_in_order = []\n",
    "    extra_seen = set()\n",
    "\n",
    "    for hdr in headers[1:]:\n",
    "        hdr_list = list(hdr)\n",
    "        hdr_set = set(hdr_list) # convert current header to set\n",
    "\n",
    "        missing = base_set - hdr_set # find missing columns compared to baseline\n",
    "        if missing:\n",
    "            any_missing_from_base = True\n",
    "\n",
    "        for c in hdr_list:\n",
    "            if c not in base_set and c not in extra_seen:\n",
    "                extra_cols_in_order.append(c)\n",
    "                extra_seen.add(c)\n",
    "\n",
    "    if any_missing_from_base or not extra_cols_in_order:\n",
    "        return\n",
    "\n",
    "    full_header = base + extra_cols_in_order\n",
    "\n",
    "    for info in file_infos:\n",
    "        df = info[\"df\"].copy()\n",
    "        for col in full_header:\n",
    "            if col not in df.columns:\n",
    "                df[col] = pd.NA\n",
    "        df = df[full_header]\n",
    "        info[\"df\"] = df\n",
    "        info[\"header\"] = tuple(full_header)\n",
    "\n",
    "\n",
    "def unify_dirs(dirs, **read_csv_kwargs):\n",
    "    if isinstance(dirs, str):\n",
    "        dirs = [dirs]\n",
    "\n",
    "    all_infos = []\n",
    "\n",
    "    for base_dir in dirs:\n",
    "        print(f\"\\n=== Directory: {base_dir} ===\")\n",
    "        file_infos = collect_csv_files(base_dir, **read_csv_kwargs)\n",
    "\n",
    "        print(\"Before:\")\n",
    "        pre_map = build_header_map(file_infos)\n",
    "        check_header_differences(pre_map)\n",
    "\n",
    "        for info in file_infos:\n",
    "            path = info[\"path\"]\n",
    "            df = info[\"df\"]\n",
    "            try:\n",
    "                new_df, changes = unify_schema_in_df(df)\n",
    "                if not changes:\n",
    "                    continue\n",
    "                print(f\"→ {os.path.relpath(path)}\")\n",
    "                for c in changes:\n",
    "                    print(f\"   - {c}\")\n",
    "                info[\"df\"] = new_df\n",
    "                info[\"header\"] = tuple(new_df.columns)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {path}: {e}\")\n",
    "\n",
    "        expand_to_include_extra_columns(file_infos)\n",
    "\n",
    "        print(\"\\nAfter:\")\n",
    "        post_map = build_header_map(file_infos)\n",
    "        check_header_differences(post_map)\n",
    "\n",
    "        all_infos.extend(file_infos)\n",
    "    return all_infos\n",
    "\n",
    "\n",
    "def combine_unified_files(file_infos, output_path):\n",
    "    if not file_infos:\n",
    "        print(\"No files to combine.\")\n",
    "        return\n",
    "\n",
    "    frames = [info[\"df\"] for info in file_infos]\n",
    "    combined = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    for col in combined.columns:\n",
    "        if col == \"occ_title\":\n",
    "            continue\n",
    "        s = combined[col]\n",
    "        mask = s.notna()\n",
    "        combined[col] = s.where(~mask, s[mask].astype(str).str.replace(r\"\\s+\", \"\", regex=True)) # clean whitespace in string columns\n",
    "\n",
    "    out_dir = os.path.dirname(output_path)\n",
    "    if out_dir:\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "    combined.to_csv(output_path, index=False)\n",
    "    print(f\"Combined {len(file_infos)} files, {len(combined)} rows -> {output_path}\")\n",
    "\n",
    "infos = unify_dirs(data_directory)\n",
    "combine_unified_files(infos, combined_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71b44413",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data/PART_3_csv/'\n",
    "all_files = os.listdir(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59fb1ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 files in the directory.\n",
      "Loaded Salary_Data.csv with shape (6704, 7).\n",
      "Loaded employee_earnings.csv with shape (808653, 25).\n",
      "Loaded combined_employee_salaries.csv with shape (60711, 10).\n",
      "Loaded all_combined_data.csv with shape (31266, 34).\n",
      "Loaded career_change_prediction_dataset_2023.csv with shape (38444, 24).\n",
      "Loaded Salary_Dataset_with_Extra_Features_2022.csv with shape (22770, 9).\n",
      "Loaded data_science_job_posts_2025.csv with shape (944, 14).\n",
      "Loaded employee_earnings.csv with shape (808653, 25).\n",
      "Loaded combined_employee_salaries.csv with shape (60711, 10).\n",
      "Loaded all_combined_data.csv with shape (31266, 34).\n",
      "Loaded career_change_prediction_dataset_2023.csv with shape (38444, 24).\n",
      "Loaded Salary_Dataset_with_Extra_Features_2022.csv with shape (22770, 9).\n",
      "Loaded data_science_job_posts_2025.csv with shape (944, 14).\n",
      "Loaded combined_salaries.csv with shape (217396, 12).\n",
      "Loaded combined_salaries.csv with shape (217396, 12).\n",
      "Loaded Employee_Payroll.csv with shape (234299, 16).\n",
      "Loaded Average_Salary_by_Job_Classification_2020.csv with shape (321, 6).\n",
      "Loaded Employee_Payroll.csv with shape (234299, 16).\n",
      "Loaded Average_Salary_by_Job_Classification_2020.csv with shape (321, 6).\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {len(all_files)} files in the directory.\")\n",
    "\n",
    "dataframes = []\n",
    "for file in all_files:\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(data_folder, file)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['source_file'] = file # add source file so that we can use it later\n",
    "            dataframes.append(df)\n",
    "            print(f\"Loaded {file} with shape {df.shape}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aaa62b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame 0 headers:\n",
      "['Age', 'Gender', 'Education Level', 'Job Title', 'Years of Experience', 'Salary', 'source_file']\n",
      "\n",
      "DataFrame 1 headers:\n",
      "['the_geom', 'cartodb_id', 'the_geom_webmercator', 'objectid', 'calendar_year', 'quarter', 'last_name', 'first_name', 'title', 'job_code', 'department_name', 'department_number', 'base_salary', 'salary_type', 'overtime_gross_pay_qtd', 'base_gross_pay_qtd', 'longevity_gross_pay_qtd', 'post_separation_gross_pay_qtd', 'miscellaneous_gross_pay_qtd', 'employee_category', 'compulsory_union_code', 'termination_month', 'termination_year', 'public_id', 'source_file']\n",
      "\n",
      "DataFrame 2 headers:\n",
      "['Department', 'DepartmentName', 'Division', 'Gender', 'BaseSalary', 'OvertimePay', 'LongevityPay', 'Grade', 'Year', 'source_file']\n",
      "\n",
      "DataFrame 3 headers:\n",
      "['occ_code', 'occ_title', 'group', 'tot_emp', 'emp_prse', 'h_mean', 'a_mean', 'mean_prse', 'h_pct10', 'h_pct25', 'h_median', 'h_pct75', 'h_pct90', 'a_pct10', 'a_pct25', 'a_median', 'a_pct75', 'a_pct90', 'annual', 'year', 'area', 'area_title', 'area_type', 'prim_state', 'naics', 'naics_title', 'i_group', 'own_code', 'jobs_1000', 'loc_quotient', 'pct_total', 'pct_rpt', 'hourly', 'source_file']\n",
      "\n",
      "DataFrame 4 headers:\n",
      "['Field of Study', 'Current Occupation', 'Age', 'Gender', 'Years of Experience', 'Education Level', 'Industry Growth Rate', 'Job Satisfaction', 'Work-Life Balance', 'Job Opportunities', 'Salary', 'Job Security', 'Career Change Interest', 'Skills Gap', 'Family Influence', 'Mentorship Available', 'Certifications', 'Freelancing Experience', 'Geographic Mobility', 'Professional Networks', 'Career Change Events', 'Technology Adoption', 'Likely to Change Occupation', 'source_file']\n",
      "\n",
      "DataFrame 5 headers:\n",
      "['Rating', 'Company Name', 'Job Title', 'Salary', 'Salaries Reported', 'Location', 'Employment Status', 'Job Roles', 'source_file']\n",
      "\n",
      "DataFrame 6 headers:\n",
      "['job_title', 'seniority_level', 'status', 'company', 'location', 'post_date', 'headquarter', 'industry', 'ownership', 'company_size', 'revenue', 'salary', 'skills', 'source_file']\n",
      "\n",
      "DataFrame 7 headers:\n",
      "['work_year', 'experience_level', 'employment_type', 'job_title', 'salary', 'salary_currency', 'salary_in_usd', 'employee_residence', 'remote_ratio', 'company_location', 'company_size', 'source_file']\n",
      "\n",
      "DataFrame 8 headers:\n",
      "['Fiscal Year', 'Fiscal Quarter', 'Fiscal Period', 'First Name', 'Last Name', 'Middle Init', 'Bureau', 'Office', 'Office Name', 'Job Code', 'Job Title', 'Base Pay', 'Position ID', 'Employee Identifier', 'Original Hire Date', 'source_file']\n",
      "\n",
      "DataFrame 9 headers:\n",
      "['Position Title', 'Position Class Code', 'Grade', 'Average of Base Salary', 'Number of Employees', 'source_file']\n"
     ]
    }
   ],
   "source": [
    "# display headers of all dataframes\n",
    "for i, df in enumerate(dataframes):\n",
    "    print(f\"\\nDataFrame {i} headers:\")\n",
    "    print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfdbddc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female\n"
     ]
    }
   ],
   "source": [
    "gender_from_names_file = \"data/names/names.json\"\n",
    "\n",
    "gender_dict = load_gender_dict(gender_from_names_file)\n",
    "get_gender_from_name = get_gender_from_name_factory(gender_dict)\n",
    "\n",
    "print(get_gender_from_name(\"Alice\"))  # example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dcbb197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FI\n",
      "UM\n"
     ]
    }
   ],
   "source": [
    "country_iso_file = \"data/names/country_iso.json\"\n",
    "country_iso_data = []\n",
    "\n",
    "if os.path.exists(country_iso_file):\n",
    "    with open(country_iso_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        country_iso_data = data.get(\"3166-1\", [])\n",
    "\n",
    "country_iso_dict, alpha3_codes = load_country_iso_dict(country_iso_file)\n",
    "get_country_iso = get_country_iso_factory(country_iso_dict)\n",
    "get_random_country_iso = get_random_country_iso_factory(country_iso_data)\n",
    "\n",
    "print(get_country_iso(\"Finland\"))  # example usage\n",
    "print(get_random_country_iso())  # example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0c52dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "JobPosition",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Salary",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "year",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "JobSatisfaction",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "WorkLifeBalance",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Opportunities",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Education",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Field of Study",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Certifications",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "FreelancingExperience",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "CompanySize",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Remote",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "CompanyLocation",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "EmployeeResidence",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Currency",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "EmployementStatus",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "OvertimePay",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SeniorityLevel",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "LongivityPay",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "TechnologyAdoption",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "SkillsGap",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Age",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Gender",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Geographic Mobility",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Professional Networks",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Industry Growth Rate",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Job Security",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ExperienceYears",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "a68b09c9-a962-452b-83c8-44c89d0b571b",
       "rows": [
        [
         "0",
         "All Occupations",
         "94200.53495172324",
         "2003",
         "4",
         "8",
         "17",
         "PhD",
         "Mechanical Engineering",
         "0",
         "1",
         "XL",
         "100",
         "MCO",
         "KWT",
         "USD",
         "Temporary",
         "0.0",
         "Mid-level",
         "0.0",
         "6",
         "2",
         "22",
         "Other",
         "0",
         "9",
         "Medium",
         "2",
         "1"
        ],
        [
         "1",
         "Management occupations",
         "85938.52546941205",
         "2003",
         "8",
         "1",
         "37",
         "Associate",
         null,
         "1",
         "1",
         "S",
         "100",
         "CUB",
         "AND",
         "USD",
         "Part-time",
         "0.0",
         "Senior",
         "0.0",
         "8",
         "6",
         "38",
         "Other",
         "1",
         "9",
         "High",
         "2",
         "1"
        ],
        [
         "2",
         "Chief executives",
         "23832.110248833334",
         "2003",
         "10",
         "9",
         "32",
         "Associate",
         null,
         "0",
         "0",
         "XL",
         "50",
         "MTQ",
         "IND",
         "USD",
         "Part-time",
         "0.0",
         "Senior",
         "0.0",
         "4",
         "2",
         "40",
         "Male",
         "1",
         "8",
         "High",
         "10",
         "1"
        ],
        [
         "3",
         "General and operations managers",
         "58128.194968342774",
         "2003",
         "1",
         "2",
         "51",
         "Master",
         "Psychology",
         "1",
         "0",
         "M",
         "50",
         "TON",
         "ERI",
         "USD",
         "Contractor",
         "0.0",
         "Mid-level",
         "0.0",
         "4",
         "3",
         "37",
         "Female",
         "0",
         "1",
         "Low",
         "1",
         "1"
        ],
        [
         "4",
         "Legislators",
         "84648.84133770393",
         "2003",
         "1",
         "5",
         "37",
         "PhD",
         "Arts",
         "0",
         "0",
         "S",
         "0",
         "CHL",
         "REU",
         "USD",
         "Part-time",
         "0.0",
         "Senior",
         "0.0",
         "6",
         "4",
         "57",
         "Female",
         "0",
         "1",
         "Low",
         "10",
         "1"
        ]
       ],
       "shape": {
        "columns": 28,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JobPosition</th>\n",
       "      <th>Salary</th>\n",
       "      <th>year</th>\n",
       "      <th>JobSatisfaction</th>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <th>Opportunities</th>\n",
       "      <th>Education</th>\n",
       "      <th>Field of Study</th>\n",
       "      <th>Certifications</th>\n",
       "      <th>FreelancingExperience</th>\n",
       "      <th>...</th>\n",
       "      <th>LongivityPay</th>\n",
       "      <th>TechnologyAdoption</th>\n",
       "      <th>SkillsGap</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Geographic Mobility</th>\n",
       "      <th>Professional Networks</th>\n",
       "      <th>Industry Growth Rate</th>\n",
       "      <th>Job Security</th>\n",
       "      <th>ExperienceYears</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All Occupations</td>\n",
       "      <td>94200.534952</td>\n",
       "      <td>2003</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Mechanical Engineering</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>Other</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Management occupations</td>\n",
       "      <td>85938.525469</td>\n",
       "      <td>2003</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>Associate</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>38</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>High</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chief executives</td>\n",
       "      <td>23832.110249</td>\n",
       "      <td>2003</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>32</td>\n",
       "      <td>Associate</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>High</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>General and operations managers</td>\n",
       "      <td>58128.194968</td>\n",
       "      <td>2003</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>Master</td>\n",
       "      <td>Psychology</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Legislators</td>\n",
       "      <td>84648.841338</td>\n",
       "      <td>2003</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>37</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Arts</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>57</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Low</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       JobPosition        Salary  year  JobSatisfaction  \\\n",
       "0                  All Occupations  94200.534952  2003                4   \n",
       "1           Management occupations  85938.525469  2003                8   \n",
       "2                 Chief executives  23832.110249  2003               10   \n",
       "3  General and operations managers  58128.194968  2003                1   \n",
       "4                      Legislators  84648.841338  2003                1   \n",
       "\n",
       "   WorkLifeBalance  Opportunities  Education          Field of Study  \\\n",
       "0                8             17        PhD  Mechanical Engineering   \n",
       "1                1             37  Associate                    None   \n",
       "2                9             32  Associate                    None   \n",
       "3                2             51     Master              Psychology   \n",
       "4                5             37        PhD                    Arts   \n",
       "\n",
       "   Certifications  FreelancingExperience  ... LongivityPay  \\\n",
       "0               0                      1  ...          0.0   \n",
       "1               1                      1  ...          0.0   \n",
       "2               0                      0  ...          0.0   \n",
       "3               1                      0  ...          0.0   \n",
       "4               0                      0  ...          0.0   \n",
       "\n",
       "   TechnologyAdoption SkillsGap Age  Gender Geographic Mobility  \\\n",
       "0                   6         2  22   Other                   0   \n",
       "1                   8         6  38   Other                   1   \n",
       "2                   4         2  40    Male                   1   \n",
       "3                   4         3  37  Female                   0   \n",
       "4                   6         4  57  Female                   0   \n",
       "\n",
       "   Professional Networks Industry Growth Rate  Job Security  ExperienceYears  \n",
       "0                      9               Medium             2                1  \n",
       "1                      9                 High             2                1  \n",
       "2                      8                 High            10                1  \n",
       "3                      1                  Low             1                1  \n",
       "4                      1                  Low            10                1  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_variations = {\n",
    "    'first_name': ['first name', 'First Name', 'FirstName'],\n",
    "    'last_name': ['last name', 'Last Name', 'LastName'],\n",
    "    'year': ['Year', 'year', 'fiscal_year', 'fiscal year', 'calendar_year', 'work_year'],\n",
    "    'Gender': ['gender', 'sex'],\n",
    "    'Education': ['education', 'Education Level', 'education_level', 'highest_education'],\n",
    "    'ExperienceYears': ['experience_years', 'years_of_experience', 'years experience', 'years of experience'],\n",
    "    'JobSatisfaction': ['Job Satisfaction', 'job_satisfaction', 'job satisfaction'],\n",
    "    'WorkLifeBalance': ['Work-Life Balance', 'work_life_balance', 'work life balance'],\n",
    "    'Opportunities': ['opportunities', 'job opportunities', 'career_opportunities'],\n",
    "    'Certifications': ['certifications', 'number_of_certifications', 'num_certifications'],\n",
    "    'FreelancingExperience': ['freelancing_experience', 'freelancing experience', 'freelance_experience', 'freelancing experience'],\n",
    "    'CompanySize': ['company_size', 'Company Size', 'size_of_company'],\n",
    "    'Remote': ['remote', 'Remote Work', 'remote_ratio', 'remote work percentage', 'status'],\n",
    "    'JobPosition': ['title', 'job_position', 'Job Position', 'position_title', 'job title', 'Job Title', 'job_title', 'Division','Position Title', 'current occupation', 'occ_title'],\n",
    "    'CompanyLocation': ['company_location', 'Company Location', 'work_location'],\n",
    "    'EmployeeResidence': ['employee_residence', 'Employee Residence', 'residence', 'home_location'],\n",
    "    'EmployementStatus': ['employment_status', 'Employment Status', 'job_status'],\n",
    "    'Currency': ['currency', 'salary_currency'],\n",
    "    'Salary': ['salary', 'base_salary', 'BaseSalary', 'Base Pay'],\n",
    "    'SeniorityLevel': ['seniority_level', 'Seniority Level', 'level_of_seniority'],\n",
    "    'OvertimePay': ['overtime_gross_pay_qtd'],\n",
    "    'LongivityPay': ['longevity_gross_pay_qtd', 'LongevityPay'],\n",
    "    'TechnologyAdoption': ['Technology Adoption'],\n",
    "    'SkillsGap': ['Skills Gap'],\n",
    "} # combine similar columns\n",
    "\n",
    "# We drop these columns just to simplify the dataframes. That way we will be able to focus on fewer columns for the analysis and modeling.\n",
    "# We do lose some potentially useful information, but for now it's ok.\n",
    "cols_to_drop = [\n",
    "    'the_geom', 'cartodb_id', 'job_code', 'department_number',\n",
    "    'compulsory_union_code', 'termination_month', 'public_id', 'termination_year',\n",
    "    'the_geom_webmercator', 'objectid', 'salary_type', 'quarter', 'occ_code',\n",
    "    'group', 'department', 'fiscal quarter', 'fiscal period', 'middle init',\n",
    "    'job code', 'original hire date', 'position id', 'employee identifier',\n",
    "    'office', 'career change interest', 'family influence', 'mentorship available',\n",
    "    'likely to change occupation', 'career change events', 'first_name', 'last_name',\n",
    "    'department_name', 'position class code', 'grade', 'number of employees', 'ownership',\n",
    "    'revenue', 'skills', 'company', 'company name', 'rating', 'source_file', 'post_date',\n",
    "    'salaries reported', 'headquarter', 'location', 'job roles', 'salary_in_usd', 'experience_level',\n",
    "    'employment_type', 'office name', 'bureau', 'industry', 'departmentname', 'employee_category',\n",
    "    'post_separation_gross_pay_qtd', 'miscellaneous_gross_pay_qtd', 'base_gross_pay_qtd', 'occ_title',\n",
    "    'a_pct10', 'h_pct25', 'jobs_1000', 'i_group', 'loc_quotient', 'h_pct75', 'area', 'naics', 'h_pct90',\n",
    "    'h_mean', 'h_median', 'tot_emp', 'emp_prse', 'h_pct10', 'a_median', 'own_code', 'pct_rpt', 'a_pct75',\n",
    "    'naics_title', 'area_title', 'pct_total', 'a_pct90', 'prim_state', 'a_mean', 'mean_prse', 'a_pct25',\n",
    "    'annual', 'area_type', 'hourly'\n",
    "]\n",
    "\n",
    "exp_map = {\n",
    "    'EN': 'Entry-level',\n",
    "    'MI': 'Mid-level',\n",
    "    'SE': 'Senior',\n",
    "    'EX': 'Executive'\n",
    "}\n",
    "\n",
    "employment_type_map = {\n",
    "    'FT': 'Full-time',\n",
    "    'PT': 'Part-time',\n",
    "    'CT': 'Contractor',\n",
    "    'FL': 'Freelance',\n",
    "    'TM': 'Temporary'\n",
    "}\n",
    "\n",
    "currency_map = {\n",
    "    '$': 'USD',\n",
    "    '€': 'EUR',\n",
    "    '£': 'GBP',\n",
    "    '¥': 'JPY',\n",
    "    '₩': 'KRW',\n",
    "    '₹': 'INR',\n",
    "}\n",
    "\n",
    "location_map = {\n",
    "    'Bangalore': 'India',\n",
    "    'Chennai': 'India',\n",
    "    'Hyderabad': 'India',\n",
    "    'New Delhi': 'India',\n",
    "    'Pune': 'India',\n",
    "    'Jaipur': 'India',\n",
    "    'Kerala': 'India',\n",
    "    'Kolkata': 'India',\n",
    "    'Madhya Pradesh': 'India',\n",
    "    'Mumbai': 'India',\n",
    "}\n",
    "\n",
    "fields = ['Medicine', 'Education', 'Arts', 'Computer Science', 'Business', 'Mechanical Engineering', 'Biology', 'Law', 'Economics', 'Psychology']\n",
    "\n",
    "\n",
    "\n",
    "for df in dataframes:\n",
    "    rename_map = {}\n",
    "    cols_lower = df.columns.str.lower()\n",
    "\n",
    "    for canonical, variations in common_variations.items():\n",
    "        variations_lower = [v.lower() for v in variations]\n",
    "        mask = cols_lower.isin(variations_lower)\n",
    "        if mask.any():\n",
    "            matched_col = df.columns[mask][0]\n",
    "            rename_map[matched_col] = canonical\n",
    "    df.rename(columns=rename_map, inplace=True)\n",
    "    if 'first_name' in df.columns:\n",
    "        df['first_name'] = df['first_name'].astype(str).str.strip().str.capitalize()\n",
    "    if 'Gender' not in df.columns and 'first_name' in df.columns: # We assume gender from the name\n",
    "        df['Gender'] = df['first_name'].apply(get_gender_from_name)\n",
    "    if 'Original Hire Date' in df.columns and 'year' in df.columns:\n",
    "        df['ExperienceYears'] = df['year'] - pd.to_datetime(df['Original Hire Date'], errors='coerce').dt.year\n",
    "    if 'JobSatisfaction' not in df.columns: # We fake some data for JobSatisfaction\n",
    "        df['JobSatisfaction'] = np.random.randint(1, 11, size=len(df))\n",
    "    if 'WorkLifeBalance' not in df.columns: # We fake some data for WorkLifeBalance\n",
    "        df['WorkLifeBalance'] = np.random.randint(1, 11, size=len(df))\n",
    "    if 'Opportunities' not in df.columns: # We fake some data for Opportunities\n",
    "        df['Opportunities'] = np.random.randint(1, 101, size=len(df))\n",
    "    if 'Education' not in df.columns: # We fake some data for Education <-- this is flawed but for now it's ok; The problem is that sometimes you could be a doctor without any degree or such\n",
    "        education_levels = ['Unknown', 'High School', 'Associate', 'Bachelor', 'Master', 'PhD']\n",
    "        df['Education'] = np.random.choice(education_levels, size=len(df))\n",
    "        df['Field of Study'] = None\n",
    "        mask = df['Education'].isin(['Bachelor', 'Master', 'PhD'])\n",
    "        df.loc[mask, 'Field of Study'] = np.random.choice(fields, size=mask.sum())\n",
    "    if 'Certifications' not in df.columns: # We fake some data for Certifications\n",
    "        df['Certifications'] = np.random.randint(0, 2, size=len(df))\n",
    "    if 'FreelancingExperience' not in df.columns: # We fake some data for FreelancingExperience\n",
    "        df['FreelancingExperience'] = np.random.randint(0, 2, size=len(df))\n",
    "    if 'CompanySize' not in df.columns: # We fake some data for CompanySize\n",
    "        company_sizes = ['S', 'M', 'L', 'XL']\n",
    "        df['CompanySize'] = np.random.choice(company_sizes, size=len(df))\n",
    "    if 'Remote' not in df.columns: # We fake some data for Remote\n",
    "        remote_options = [0, 50, 100] # on site; hybrid; remote\n",
    "        df['Remote'] = np.random.choice(remote_options, size=len(df))\n",
    "    if 'Location' in df.columns and 'CompanyLocation' not in df.columns:\n",
    "        country = df['Location'].map(location_map).fillna('unknown')\n",
    "        df['CompanyLocation'] = country.apply(get_country_iso)\n",
    "    if 'headquarter' in df.columns:\n",
    "        df['CompanyLocation'] = df['headquarter'].astype(str).str.split(',').str[-1].str.strip()\n",
    "    if 'CompanyLocation' not in df.columns:\n",
    "        df['CompanyLocation'] = np.random.choice(alpha3_codes, size=len(df))\n",
    "    if 'EmployeeResidence' not in df.columns:\n",
    "        df['EmployeeResidence'] = np.random.choice(alpha3_codes, size=len(df))\n",
    "    if 'Average of Base Salary' in df.columns:\n",
    "        # use symbol to find currency and replace commas with nothing and dots with commas\n",
    "        df.rename(columns={'Average of Base Salary': 'Salary'}, inplace=True)\n",
    "        df['Currency'] = df['Salary'].astype(str).str.extract(r'([^\\d.,\\s]+)')[0].str.strip()\n",
    "        df['Currency'] = df['Currency'].replace(currency_map)\n",
    "        df['Salary'] = df['Salary'].astype(str).str.replace(r'[^\\d.,]', '', regex=True)\n",
    "        df['Salary'] = df['Salary'].str.replace(',', '')\n",
    "        df['Salary'] = pd.to_numeric(df['Salary'], errors='coerce')\n",
    "    if 'Currency' not in df.columns:\n",
    "        df['Currency'] = 'USD'  # default to USD if currency not found\n",
    "    if 'year' not in df.columns and 'source_file' in df.columns: # if year is missing, we try to extract it from the source file name\n",
    "        df['year'] = df['source_file'].str.extract(r'(\\d{4})').astype(float)\n",
    "    if 'EmployementStatus' not in df.columns: # We fake some data for EmployementStatus\n",
    "        employment_types = list(employment_type_map.values())\n",
    "        df['EmployementStatus'] = np.random.choice(employment_types, size=len(df))\n",
    "    if 'annual' in df.columns and 'Salary' not in df.columns:\n",
    "        df.rename(columns={'annual': 'Salary'}, inplace=True)\n",
    "        missing_mask = df['Salary'].isna() | (df['Salary'] == 0)\n",
    "\n",
    "        if missing_mask.any():\n",
    "            if 'a_10' in df.columns and 'a_pct90' in df.columns:\n",
    "                # Fill missing Salary values using random values between a_10 and a_pct90\n",
    "                df.loc[missing_mask, 'Salary'] = df.loc[missing_mask].apply(\n",
    "                    lambda row: np.random.uniform(row['a_10'], row['a_pct90'])\n",
    "                    if pd.notna(row['a_10']) and pd.notna(row['a_pct90'])\n",
    "                    else np.random.uniform(10000, 120000),\n",
    "                    axis=1\n",
    "                )\n",
    "            else:\n",
    "                df.loc[missing_mask, 'Salary'] = np.random.uniform(10000, 120000, size=missing_mask.sum())\n",
    "    if 'OvertimePay' not in df.columns and 'Salary' in df.columns: # We fake some data for OvertimePay\n",
    "        if df['Salary'].dtype in [np.float64, np.int64]:\n",
    "            df['OvertimePay'] = round(df['Salary'] * np.random.uniform(0, 0.5, size=len(df)), 2)\n",
    "        else:\n",
    "            df['OvertimePay'] = 0.0\n",
    "    if 'experience_level' in df.columns:\n",
    "        df['SeniorityLevel'] = df['experience_level'].str.upper().map(exp_map).fillna('Unknown')\n",
    "    if 'SeniorityLevel' not in df.columns:\n",
    "        df['SeniorityLevel'] = np.random.choice(list(exp_map.values()), size=len(df))\n",
    "    if 'employment_type' in df.columns:\n",
    "        df['EmployementStatus'] = df['employment_type'].str.upper().map(employment_type_map).fillna('Unknown')\n",
    "    if 'LongivityPay' not in df.columns and 'Salary' in df.columns:\n",
    "        if df['Salary'].dtype in [np.float64, np.int64]:\n",
    "            df['LongivityPay'] = round(df['Salary'] * np.random.uniform(0, 0.3, size=len(df)), 2)\n",
    "        else:\n",
    "            df['LongivityPay'] = 0.0\n",
    "    if 'TechnologyAdoption' not in df.columns: # We fake some data for TechnologyAdoption; How happy are people for new technology adoption at their workplace\n",
    "        df['TechnologyAdoption'] = np.random.randint(1, 11, size=len(df))\n",
    "    if 'SkillsGap' not in df.columns: # We fake some data for SkillsGap; How does the employees skills match the job requirements\n",
    "        df['SkillsGap'] = np.random.randint(1, 11, size=len(df))\n",
    "    if 'Age' not in df.columns: # We fake some data for Age\n",
    "        df['Age'] = np.random.randint(22, 65, size=len(df))\n",
    "    if 'Gender' not in df.columns: # We fake some data\n",
    "        df['Gender'] = np.random.choice(['Male', 'Female', 'Other'], size=len(df))\n",
    "    if 'Geographic Mobility' not in df.columns: # We fake some data for Geographic Mobility; Whether the individual is willing to relocate for a job.\n",
    "        df['Geographic Mobility'] = np.random.randint(0, 2, size=len(df))\n",
    "    if 'Professional Networks' not in df.columns: # We fake some data for Professional Networks; A measure of how strong the individual's professional network is.\n",
    "        df['Professional Networks'] = np.random.randint(1, 11, size=len(df))\n",
    "    if 'Industry Growth Rate' not in df.columns: # We fake some data for Industry Growth Rate; The growth rate of the industry the individual works in.\n",
    "        growth_type = ['High', 'Medium', 'Low']\n",
    "        df['Industry Growth Rate'] = np.random.choice(growth_type, size=len(df))\n",
    "    if 'Job Security' not in df.columns: # We fake some data for Job Security; A measure of how secure the individual feels in their current job.\n",
    "        df['Job Security'] = np.random.randint(1, 11, size=len(df))\n",
    "    if 'ExperienceYears' not in df.columns: # We fake some data for ExperienceYears\n",
    "        if 'Age' not in df.columns:\n",
    "            df['ExperienceYears'] = np.random.randint(0, 40, size=len(df))\n",
    "        else:\n",
    "            max_exp = df['Age'] - 22\n",
    "            if (max_exp <= 0).any():\n",
    "                max_exp = 2\n",
    "            df['ExperienceYears'] = np.random.randint(0, max_exp, size=len(df))\n",
    "    if 'Field of Study' not in df.columns:\n",
    "        df['Field of Study'] = None\n",
    "        mask = df['Education'].isin(['Bachelor', 'Master', 'PhD'])\n",
    "        df.loc[mask, 'Field of Study'] = np.random.choice(fields, size=mask.sum())\n",
    "    if 'occ_title' in df.columns and 'JobPosition' not in df.columns:\n",
    "        mask = df['occ_title'].astype(str).str.strip().str.lower() != 'all occupations'\n",
    "        df.loc[mask, 'JobPosition'] = df.loc[mask, 'occ_title']\n",
    "        df = df.loc[mask]\n",
    "    # probably we could have industry column too, for now we dropping for simplicity ['Retail', 'Manufacturing', 'Technology', 'Finance', 'Education', 'Healthcare', 'Energy', 'Logistics']\n",
    "\n",
    "    drop_matches = [c for c in df.columns if c.lower() in cols_to_drop]\n",
    "    if drop_matches:\n",
    "        df.drop(columns=drop_matches, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Get OvertimePay values where it's above 0\n",
    "# dataframes[2] = dataframes[2][dataframes[2]['OvertimePay'] > 0]\n",
    "dataframes[3].head()\n",
    "# dataframes[4]['Field of Study'].unique()\n",
    "\n",
    "# Experience_level: en - entry; mi - mid; se - senior; ex - executive\n",
    "\n",
    "# display all unique values for salary_type column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9805b8b",
   "metadata": {},
   "source": [
    "We found that there are some dataframes that don't have sex/gender columns, so we centralize the way first_name and last_name is defined and we add new column called full_name, which we might use in order to find the sex/gender out from the name.\n",
    "\n",
    "we drop salary_in_usd --> ideally you wouldn't want to do that...\n",
    "for simplicity of the project even though we have currency column.. we will assume that salary is in usd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "732059ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DataFrame 4 as reference.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reference_index = 4\n",
    "reference_columns = set(dataframes[reference_index].columns)\n",
    "\n",
    "print(f\"Using DataFrame {reference_index} as reference.\\n\")\n",
    "\n",
    "for i, df in enumerate(dataframes):\n",
    "    if i == reference_index:\n",
    "        continue\n",
    "\n",
    "    cols = set(df.columns)\n",
    "    missing = reference_columns - cols\n",
    "    extra = cols - reference_columns\n",
    "\n",
    "    if not missing and not extra:\n",
    "        continue\n",
    "\n",
    "    print(f\"--- DataFrame {i} compared to reference {reference_index} ---\")\n",
    "    if missing:\n",
    "        print(\"  Missing columns:\", missing)\n",
    "    if extra:\n",
    "        print(\"  Extra columns:  \", extra)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404f09f0",
   "metadata": {},
   "source": [
    "If above is empty (no missing columns and no extra columns), we can continue and combine all of it together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d7f9bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([df.loc[:, ~df.columns.duplicated()].reset_index(drop=True) for df in dataframes],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "980b7a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Age",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Gender",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Education",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "JobPosition",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ExperienceYears",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Salary",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "JobSatisfaction",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "WorkLifeBalance",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Opportunities",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Certifications",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "FreelancingExperience",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "CompanySize",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Remote",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "CompanyLocation",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "EmployeeResidence",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Currency",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "year",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "EmployementStatus",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "OvertimePay",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SeniorityLevel",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "LongivityPay",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "TechnologyAdoption",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "SkillsGap",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Geographic Mobility",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Professional Networks",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Industry Growth Rate",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Job Security",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Field of Study",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "edeb1b34-f137-4d3a-834c-36ec4a115684",
       "rows": [
        [
         "0",
         "32.0",
         "Male",
         "Bachelor's",
         "Software Engineer",
         "5.0",
         "90000.0",
         "9",
         "1",
         "89",
         "1",
         "0",
         "S",
         "0",
         "SLB",
         "GRC",
         "USD",
         null,
         "Part-time",
         "24867.66",
         "Mid-level",
         "18072.36",
         "3",
         "6",
         "1",
         "8",
         "Medium",
         "6",
         null
        ],
        [
         "1",
         "28.0",
         "Female",
         "Master's",
         "Data Analyst",
         "3.0",
         "65000.0",
         "2",
         "7",
         "12",
         "1",
         "0",
         "M",
         "0",
         "SPM",
         "GUY",
         "USD",
         null,
         "Part-time",
         "23010.22",
         "Mid-level",
         "11239.44",
         "5",
         "7",
         "1",
         "9",
         "Medium",
         "5",
         null
        ],
        [
         "2",
         "45.0",
         "Male",
         "PhD",
         "Senior Manager",
         "15.0",
         "150000.0",
         "1",
         "5",
         "52",
         "1",
         "1",
         "M",
         "50",
         "BVT",
         "NFK",
         "USD",
         null,
         "Contractor",
         "38462.8",
         "Senior",
         "18917.45",
         "5",
         "9",
         "1",
         "1",
         "High",
         "4",
         "Biology"
        ],
        [
         "3",
         "36.0",
         "Female",
         "Bachelor's",
         "Sales Associate",
         "7.0",
         "60000.0",
         "1",
         "10",
         "94",
         "0",
         "0",
         "XL",
         "100",
         "BFA",
         "PHL",
         "USD",
         null,
         "Freelance",
         "842.72",
         "Executive",
         "16087.02",
         "3",
         "10",
         "1",
         "1",
         "High",
         "2",
         null
        ],
        [
         "4",
         "52.0",
         "Male",
         "Master's",
         "Director",
         "20.0",
         "200000.0",
         "1",
         "8",
         "45",
         "0",
         "1",
         "S",
         "0",
         "ITA",
         "DEU",
         "USD",
         null,
         "Full-time",
         "28522.79",
         "Entry-level",
         "45838.82",
         "2",
         "3",
         "0",
         "3",
         "High",
         "9",
         null
        ]
       ],
       "shape": {
        "columns": 28,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Education</th>\n",
       "      <th>JobPosition</th>\n",
       "      <th>ExperienceYears</th>\n",
       "      <th>Salary</th>\n",
       "      <th>JobSatisfaction</th>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <th>Opportunities</th>\n",
       "      <th>Certifications</th>\n",
       "      <th>...</th>\n",
       "      <th>OvertimePay</th>\n",
       "      <th>SeniorityLevel</th>\n",
       "      <th>LongivityPay</th>\n",
       "      <th>TechnologyAdoption</th>\n",
       "      <th>SkillsGap</th>\n",
       "      <th>Geographic Mobility</th>\n",
       "      <th>Professional Networks</th>\n",
       "      <th>Industry Growth Rate</th>\n",
       "      <th>Job Security</th>\n",
       "      <th>Field of Study</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Software Engineer</td>\n",
       "      <td>5.0</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>24867.66</td>\n",
       "      <td>Mid-level</td>\n",
       "      <td>18072.36</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>Medium</td>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Master's</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>3.0</td>\n",
       "      <td>65000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>23010.22</td>\n",
       "      <td>Mid-level</td>\n",
       "      <td>11239.44</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>Medium</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Senior Manager</td>\n",
       "      <td>15.0</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>38462.80</td>\n",
       "      <td>Senior</td>\n",
       "      <td>18917.45</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>High</td>\n",
       "      <td>4</td>\n",
       "      <td>Biology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>Sales Associate</td>\n",
       "      <td>7.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>842.72</td>\n",
       "      <td>Executive</td>\n",
       "      <td>16087.02</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>High</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Master's</td>\n",
       "      <td>Director</td>\n",
       "      <td>20.0</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28522.79</td>\n",
       "      <td>Entry-level</td>\n",
       "      <td>45838.82</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>High</td>\n",
       "      <td>9</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  Gender   Education        JobPosition  ExperienceYears    Salary  \\\n",
       "0  32.0    Male  Bachelor's  Software Engineer              5.0   90000.0   \n",
       "1  28.0  Female    Master's       Data Analyst              3.0   65000.0   \n",
       "2  45.0    Male         PhD     Senior Manager             15.0  150000.0   \n",
       "3  36.0  Female  Bachelor's    Sales Associate              7.0   60000.0   \n",
       "4  52.0    Male    Master's           Director             20.0  200000.0   \n",
       "\n",
       "   JobSatisfaction  WorkLifeBalance  Opportunities  Certifications  ...  \\\n",
       "0                9                1             89               1  ...   \n",
       "1                2                7             12               1  ...   \n",
       "2                1                5             52               1  ...   \n",
       "3                1               10             94               0  ...   \n",
       "4                1                8             45               0  ...   \n",
       "\n",
       "   OvertimePay SeniorityLevel LongivityPay TechnologyAdoption SkillsGap  \\\n",
       "0     24867.66      Mid-level     18072.36                  3         6   \n",
       "1     23010.22      Mid-level     11239.44                  5         7   \n",
       "2     38462.80         Senior     18917.45                  5         9   \n",
       "3       842.72      Executive     16087.02                  3        10   \n",
       "4     28522.79    Entry-level     45838.82                  2         3   \n",
       "\n",
       "  Geographic Mobility  Professional Networks Industry Growth Rate  \\\n",
       "0                   1                      8               Medium   \n",
       "1                   1                      9               Medium   \n",
       "2                   1                      1                 High   \n",
       "3                   1                      1                 High   \n",
       "4                   0                      3                 High   \n",
       "\n",
       "   Job Security Field of Study  \n",
       "0             6           None  \n",
       "1             5           None  \n",
       "2             4        Biology  \n",
       "3             2           None  \n",
       "4             9           None  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6215d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = 'data/PART_4_COMBINED_CSV/combined_data.csv'\n",
    "combined_df.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02bd0c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n8/q1w9g179281f8wm6bhv_60bh0000gn/T/ipykernel_72956/847846481.py:5: DtypeWarning: Columns (5,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"data/PART_4_COMBINED_CSV/combined_data.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (1421508, 28)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/PART_4_COMBINED_CSV/combined_data.csv\")\n",
    "\n",
    "print(\"Loaded:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e525bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After duplicates: (1421508, 28)\n"
     ]
    }
   ],
   "source": [
    "#Remove duplicates\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(\"After duplicates:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9fcba5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names cleaned.\n"
     ]
    }
   ],
   "source": [
    "# Standardize column names\n",
    "\n",
    "df.columns = (\n",
    "    df.columns\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .str.replace(\" \", \"_\")\n",
    "    .str.replace(\"-\", \"_\")\n",
    "    .str.replace(\"/\", \"_\")\n",
    ")\n",
    "print(\"Column names cleaned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f95f6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salary fields cleaned: ['salary', 'overtimepay', 'longivitypay']\n"
     ]
    }
   ],
   "source": [
    "# Fix salary fields\n",
    "\n",
    "salary_cols = [c for c in df.columns if \"salary\" in c or \"pay\" in c or \"income\" in c]\n",
    "\n",
    "for col in salary_cols:\n",
    "    df[col] = (\n",
    "        df[col]\n",
    "        .astype(str)\n",
    "        .str.replace(\",\", \"\")\n",
    "        .str.replace(\"$\", \"\")\n",
    "        .str.replace(\"€\", \"\")\n",
    "    )\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "print(\"Salary fields cleaned:\", salary_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9d960b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_cols = [\"salary\", \"experienceyears\", \"age\"]\n",
    "\n",
    "for col in fill_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(df[col].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "48d35dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis=1, how=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40a303da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "field_of_study    693182\n",
      "overtimepay       403377\n",
      "longivitypay      303430\n",
      "year                6704\n",
      "remote               256\n",
      "senioritylevel        60\n",
      "jobposition            5\n",
      "education              3\n",
      "gender                 2\n",
      "job_security           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum().sort_values(ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "109cdeda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "c619f180-13a9-4f58-9cb1-d05c6a08eb84",
       "rows": [
        [
         "field_of_study",
         "693182"
        ],
        [
         "overtimepay",
         "403377"
        ],
        [
         "longivitypay",
         "303430"
        ],
        [
         "year",
         "6704"
        ],
        [
         "remote",
         "256"
        ],
        [
         "senioritylevel",
         "60"
        ],
        [
         "jobposition",
         "5"
        ],
        [
         "education",
         "3"
        ],
        [
         "gender",
         "2"
        ],
        [
         "job_security",
         "0"
        ],
        [
         "industry_growth_rate",
         "0"
        ],
        [
         "professional_networks",
         "0"
        ],
        [
         "geographic_mobility",
         "0"
        ],
        [
         "skillsgap",
         "0"
        ],
        [
         "technologyadoption",
         "0"
        ],
        [
         "employementstatus",
         "0"
        ],
        [
         "age",
         "0"
        ],
        [
         "currency",
         "0"
        ],
        [
         "companylocation",
         "0"
        ],
        [
         "companysize",
         "0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 20
       }
      },
      "text/plain": [
       "field_of_study           693182\n",
       "overtimepay              403377\n",
       "longivitypay             303430\n",
       "year                       6704\n",
       "remote                      256\n",
       "senioritylevel               60\n",
       "jobposition                   5\n",
       "education                     3\n",
       "gender                        2\n",
       "job_security                  0\n",
       "industry_growth_rate          0\n",
       "professional_networks         0\n",
       "geographic_mobility           0\n",
       "skillsgap                     0\n",
       "technologyadoption            0\n",
       "employementstatus             0\n",
       "age                           0\n",
       "currency                      0\n",
       "companylocation               0\n",
       "companysize                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing values\n",
    "\n",
    "df.isna().sum().sort_values(ascending=False).head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0243e1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'experienceyears',\n",
       " 'salary',\n",
       " 'jobsatisfaction',\n",
       " 'worklifebalance',\n",
       " 'opportunities',\n",
       " 'certifications',\n",
       " 'freelancingexperience',\n",
       " 'year',\n",
       " 'overtimepay',\n",
       " 'longivitypay',\n",
       " 'technologyadoption',\n",
       " 'skillsgap',\n",
       " 'geographic_mobility',\n",
       " 'professional_networks',\n",
       " 'job_security']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify numeric columns\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[\"float64\",\"int64\"]).columns.tolist()\n",
    "numeric_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f576a938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age 1421508\n",
      "gender 0\n",
      "gender 0\n",
      "education 0\n",
      "education 0\n",
      "jobposition 0\n",
      "experienceyears 1421508\n",
      "salary 1421508\n",
      "jobsatisfaction 1421508\n",
      "worklifebalance 1421508\n",
      "opportunities 1421508\n",
      "certifications 1421508\n",
      "freelancingexperience 1421508\n",
      "jobposition 0\n",
      "experienceyears 1421508\n",
      "salary 1421508\n",
      "jobsatisfaction 1421508\n",
      "worklifebalance 1421508\n",
      "opportunities 1421508\n",
      "certifications 1421508\n",
      "freelancingexperience 1421508\n",
      "companysize 208\n",
      "remote 1420564\n",
      "companysize 208\n",
      "remote 1420564\n",
      "companylocation 0\n",
      "companylocation 0\n",
      "employeeresidence 0\n",
      "employeeresidence 0\n",
      "currency 0\n",
      "year 1414804\n",
      "currency 0\n",
      "year 1414804\n",
      "employementstatus 0\n",
      "overtimepay 1018131\n",
      "employementstatus 0\n",
      "overtimepay 1018131\n",
      "senioritylevel 0\n",
      "longivitypay 1118078\n",
      "technologyadoption 1421508\n",
      "skillsgap 1421508\n",
      "geographic_mobility 1421508\n",
      "professional_networks 1421508\n",
      "senioritylevel 0\n",
      "longivitypay 1118078\n",
      "technologyadoption 1421508\n",
      "skillsgap 1421508\n",
      "geographic_mobility 1421508\n",
      "professional_networks 1421508\n",
      "industry_growth_rate 0\n",
      "job_security 1421508\n",
      "field_of_study 0\n",
      "industry_growth_rate 0\n",
      "job_security 1421508\n",
      "field_of_study 0\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns:\n",
    "    converted = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    print(col, converted.notna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a304f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns: ['age', 'experienceyears', 'salary', 'jobsatisfaction', 'worklifebalance', 'opportunities', 'certifications', 'freelancingexperience', 'year', 'overtimepay', 'longivitypay', 'technologyadoption', 'skillsgap', 'geographic_mobility', 'professional_networks', 'job_security']\n"
     ]
    }
   ],
   "source": [
    "numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "print(\"Numeric columns:\", numeric_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11284dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns filled: ['age', 'experienceyears', 'salary', 'jobsatisfaction', 'worklifebalance', 'opportunities', 'certifications', 'freelancingexperience', 'companysize', 'remote', 'year', 'overtimepay', 'longivitypay', 'technologyadoption', 'skillsgap', 'geographic_mobility', 'professional_networks', 'job_security']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "safe_numeric_cols = []\n",
    "\n",
    "for col in df.columns:\n",
    "\n",
    "    # ensure we always work on a Series\n",
    "    s = df[col]\n",
    "\n",
    "    # skip if column is not a Series of simple values\n",
    "    if not isinstance(s, pd.Series):\n",
    "        continue\n",
    "\n",
    "    # try convert\n",
    "    converted = pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "    # keep only if at least 1 real number exists\n",
    "    if converted.notna().sum() > 0:\n",
    "        median_val = converted.median()\n",
    "        df[col] = converted.fillna(median_val)\n",
    "        safe_numeric_cols.append(col)\n",
    "\n",
    "print(\"Numeric columns filled:\", safe_numeric_cols)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "026f0a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "68a571f2-6dec-4e39-b9d6-e0bcadcd7a5b",
       "rows": [
        [
         "age",
         "0"
        ],
        [
         "experienceyears",
         "0"
        ],
        [
         "professional_networks",
         "0"
        ],
        [
         "geographic_mobility",
         "0"
        ],
        [
         "skillsgap",
         "0"
        ],
        [
         "technologyadoption",
         "0"
        ],
        [
         "longivitypay",
         "0"
        ],
        [
         "overtimepay",
         "0"
        ],
        [
         "year",
         "0"
        ],
        [
         "remote",
         "0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 10
       }
      },
      "text/plain": [
       "age                      0\n",
       "experienceyears          0\n",
       "professional_networks    0\n",
       "geographic_mobility      0\n",
       "skillsgap                0\n",
       "technologyadoption       0\n",
       "longivitypay             0\n",
       "overtimepay              0\n",
       "year                     0\n",
       "remote                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[safe_numeric_cols].isna().sum().sort_values(ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "418f46d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Drop columns with all missing values\n",
    "df = df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "# 2. Drop columns with only one unique non-null value\n",
    "single_val_cols = []\n",
    "\n",
    "for c in df.columns:\n",
    "    try:\n",
    "        if df[c].dropna().nunique() <= 1:\n",
    "            single_val_cols.append(c)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "df = df.drop(columns=single_val_cols)\n",
    "\n",
    "print(\"Dropped:\", single_val_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "42bdba2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "c4dee2ec-9867-4749-811e-c0808a077ca9",
       "rows": [
        [
         "field_of_study",
         "693182"
        ],
        [
         "senioritylevel",
         "60"
        ],
        [
         "jobposition",
         "5"
        ],
        [
         "education",
         "3"
        ],
        [
         "gender",
         "2"
        ],
        [
         "year",
         "0"
        ],
        [
         "job_security",
         "0"
        ],
        [
         "industry_growth_rate",
         "0"
        ],
        [
         "professional_networks",
         "0"
        ],
        [
         "geographic_mobility",
         "0"
        ],
        [
         "skillsgap",
         "0"
        ],
        [
         "technologyadoption",
         "0"
        ],
        [
         "longivitypay",
         "0"
        ],
        [
         "overtimepay",
         "0"
        ],
        [
         "employementstatus",
         "0"
        ],
        [
         "age",
         "0"
        ],
        [
         "currency",
         "0"
        ],
        [
         "companylocation",
         "0"
        ],
        [
         "remote",
         "0"
        ],
        [
         "companysize",
         "0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 20
       }
      },
      "text/plain": [
       "field_of_study           693182\n",
       "senioritylevel               60\n",
       "jobposition                   5\n",
       "education                     3\n",
       "gender                        2\n",
       "year                          0\n",
       "job_security                  0\n",
       "industry_growth_rate          0\n",
       "professional_networks         0\n",
       "geographic_mobility           0\n",
       "skillsgap                     0\n",
       "technologyadoption            0\n",
       "longivitypay                  0\n",
       "overtimepay                   0\n",
       "employementstatus             0\n",
       "age                           0\n",
       "currency                      0\n",
       "companylocation               0\n",
       "remote                        0\n",
       "companysize                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum().sort_values(ascending=False).head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0046f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved.\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned dataset\n",
    "\n",
    "df.to_csv(\"data/PART_4_COMBINED_CSV/cleaned_part4.csv\", index=False)\n",
    "print(\"Saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0611452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning additions completed.\n",
      "Dropped high missing: []\n",
      "Shape: (1410130, 27)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Drop useless identifier columns\n",
    "id_cols = [\n",
    "    \"first_name\",\n",
    "    \"last_name\",\n",
    "    \"employee_identifier\",\n",
    "    \"original_hire_date\",\n",
    "    \"public_id\",\n",
    "    \"position_id\",\n",
    "    \"job_code\",\n",
    "    \"bureau\"\n",
    "]\n",
    "\n",
    "df = df.drop(columns=[c for c in id_cols if c in df.columns])\n",
    "\n",
    "\n",
    "# 2. Drop columns with more than 90% missing\n",
    "missing_ratio = df.isna().mean()\n",
    "high_missing_cols = missing_ratio[missing_ratio > 0.90].index.tolist()\n",
    "\n",
    "df = df.drop(columns=high_missing_cols)\n",
    "\n",
    "\n",
    "# 3. Merge duplicate categorical fields\n",
    "if \"companylocation\" in df.columns and \"employeeresidence\" in df.columns:\n",
    "    df[\"location\"] = df[\"companylocation\"].fillna(df[\"employeeresidence\"])\n",
    "    df = df.drop(columns=[\"companylocation\", \"employeeresidence\"], errors=\"ignore\")\n",
    "\n",
    "if \"jobroles\" in df.columns and \"jobposition\" in df.columns:\n",
    "    df[\"job_role\"] = df[\"jobroles\"].fillna(df[\"jobposition\"])\n",
    "    df = df.drop(columns=[\"jobroles\", \"jobposition\"], errors=\"ignore\")\n",
    "\n",
    "\n",
    "# 4. Normalize categorical text\n",
    "cat_cols = df.select_dtypes(include=\"object\").columns\n",
    "\n",
    "for col in cat_cols:\n",
    "    df[col] = (\n",
    "        df[col]\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(r\"\\s+\", \"_\", regex=True)\n",
    "    )\n",
    "\n",
    "\n",
    "# 5. Fix impossible numeric values\n",
    "if \"age\" in df.columns:\n",
    "    df.loc[df[\"age\"] < 15, \"age\"] = np.nan\n",
    "\n",
    "if \"experienceyears\" in df.columns and \"age\" in df.columns:\n",
    "    df.loc[df[\"experienceyears\"] > df[\"age\"], \"experienceyears\"] = np.nan\n",
    "\n",
    "\n",
    "# 6. Remove extreme salary outliers\n",
    "if \"salary\" in df.columns:\n",
    "    df = df[df[\"salary\"].between(500, 1000000)]\n",
    "\n",
    "\n",
    "# 7. Parse dates and create tenure\n",
    "if \"termination_year\" in df.columns and \"termination_month\" in df.columns:\n",
    "    df[\"termination_date\"] = pd.to_datetime(\n",
    "        df[\"termination_year\"].astype(str) + \"-\" + df[\"termination_month\"].astype(str) + \"-01\",\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "if \"fiscal_year\" in df.columns and \"fiscal_quarter\" in df.columns:\n",
    "    df[\"fiscal_period\"] = df[\"fiscal_year\"].astype(str) + \"Q\" + df[\"fiscal_quarter\"].astype(str)\n",
    "\n",
    "if \"termination_date\" in df.columns and \"year\" in df.columns:\n",
    "    df[\"tenure_years\"] = df[\"year\"] - df[\"termination_date\"].dt.year\n",
    "\n",
    "\n",
    "print(\"Cleaning additions completed.\")\n",
    "print(\"Dropped high missing:\", high_missing_cols)\n",
    "print(\"Shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0bbd8ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "8c761f84-56ea-4d37-8cd6-2935efbbd826",
       "rows": [
        [
         "experienceyears",
         "11558"
        ],
        [
         "age",
         "0"
        ],
        [
         "year",
         "0"
        ],
        [
         "field_of_study",
         "0"
        ],
        [
         "job_security",
         "0"
        ],
        [
         "industry_growth_rate",
         "0"
        ],
        [
         "professional_networks",
         "0"
        ],
        [
         "geographic_mobility",
         "0"
        ],
        [
         "skillsgap",
         "0"
        ],
        [
         "technologyadoption",
         "0"
        ],
        [
         "longivitypay",
         "0"
        ],
        [
         "senioritylevel",
         "0"
        ],
        [
         "overtimepay",
         "0"
        ],
        [
         "employementstatus",
         "0"
        ],
        [
         "currency",
         "0"
        ],
        [
         "gender",
         "0"
        ],
        [
         "remote",
         "0"
        ],
        [
         "companysize",
         "0"
        ],
        [
         "freelancingexperience",
         "0"
        ],
        [
         "certifications",
         "0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 20
       }
      },
      "text/plain": [
       "experienceyears          11558\n",
       "age                          0\n",
       "year                         0\n",
       "field_of_study               0\n",
       "job_security                 0\n",
       "industry_growth_rate         0\n",
       "professional_networks        0\n",
       "geographic_mobility          0\n",
       "skillsgap                    0\n",
       "technologyadoption           0\n",
       "longivitypay                 0\n",
       "senioritylevel               0\n",
       "overtimepay                  0\n",
       "employementstatus            0\n",
       "currency                     0\n",
       "gender                       0\n",
       "remote                       0\n",
       "companysize                  0\n",
       "freelancingexperience        0\n",
       "certifications               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum().sort_values(ascending=False).head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6096b260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-cardinality: ['gender', 'education', 'employementstatus', 'senioritylevel', 'industry_growth_rate', 'field_of_study']\n",
      "High-cardinality: ['jobposition', 'currency', 'location']\n",
      "Processed shape: (1410130, 65)\n",
      "Processed shape: (1410130, 65)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# identify categorical columns\n",
    "cat_cols = df.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "# split by cardinality\n",
    "low_card = [c for c in cat_cols if df[c].nunique() <= 20]\n",
    "high_card = [c for c in cat_cols if df[c].nunique() > 20]\n",
    "\n",
    "print(\"Low-cardinality:\", low_card)\n",
    "print(\"High-cardinality:\", high_card)\n",
    "\n",
    "# frequency encode high-cardinality\n",
    "for col in high_card:\n",
    "    freq = df[col].value_counts(normalize=True)\n",
    "    df[col] = df[col].map(freq)\n",
    "\n",
    "# build transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", numeric_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), low_card),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "processed = preprocessor.fit_transform(df)\n",
    "\n",
    "print(\"Processed shape:\", processed.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3880ca5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting column Salary to numeric: could not convert string to float: '€306,623'\n",
      "Generated 18589870 additional rows for a total of 20011378 rows.\n",
      "Generated 18589870 additional rows for a total of 20011378 rows.\n"
     ]
    }
   ],
   "source": [
    "# fake more data (around 2 million rows) based on the existing data for better modeling later\n",
    "num_existing_rows = len(df)\n",
    "num_desired_rows = 20000000\n",
    "num_rows_to_generate = num_desired_rows - num_existing_rows\n",
    "out_path = 'data/PART_5_fill_with_more_data/combined_data_filled.csv'\n",
    "\n",
    "if num_rows_to_generate > 0:\n",
    "    sampled_df = combined_df.sample(n=num_rows_to_generate, replace=True, random_state=42).reset_index(drop=True)\n",
    "    numeric_cols = ['Salary', 'OvertimePay', 'LongivityPay', 'ExperienceYears', 'Age', 'JobSatisfaction', 'WorkLifeBalance', 'Opportunities', 'Certifications', 'FreelancingExperience', 'TechnologyAdoption', 'SkillsGap', 'Geographic Mobility', 'Professional Networks', 'Job Security']\n",
    "    for col in numeric_cols:\n",
    "        if col in sampled_df.columns:\n",
    "            try:\n",
    "                noise = np.random.normal(0, 0.05 * sampled_df[col].std(), size=len(sampled_df))\n",
    "                sampled_df[col] = pd.to_numeric(sampled_df[col] + noise, errors='coerce')\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting column {col} to numeric: {e}\")\n",
    "                continue\n",
    "            # sampled_df[col] = (sampled_df[col].astype(float) + noise).clip(lower=0)\n",
    "\n",
    "    combined_df = pd.concat([combined_df, sampled_df], ignore_index=True)\n",
    "    print(f\"Generated {num_rows_to_generate} additional rows for a total of {len(combined_df)} rows.\")\n",
    "    \n",
    "combined_df.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371c5523",
   "metadata": {},
   "source": [
    "CLEANING STEPS (DONE)\n",
    "\n",
    "• loading and combining data\n",
    "• removing duplicates\n",
    "• standardizing column names\n",
    "• cleaning salary fields\n",
    "• converting numeric fields\n",
    "• filling numeric missing values safely\n",
    "• filling categorical missing values\n",
    "• dropping high-missing columns\n",
    "• dropping all-missing columns\n",
    "• merging duplicate categorical fields\n",
    "• normalizing categorical text\n",
    "• removing impossible ages\n",
    "• removing impossible experience values\n",
    "• removing extreme salary outliers\n",
    "• parsing date fields\n",
    "• removing useless date columns afterward\n",
    "• confirming no remaining missing values in usable columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dcbbbb",
   "metadata": {},
   "source": [
    "## Spark Medallion Architecture: Bronze, Silver, Gold\n",
    "We now re-ingest the large PART 5 file with Spark and process it through a simple medallion pipeline:\n",
    "\n",
    "- **Bronze**: raw ingestion from `data/PART_5_fill_with_more_data/combined_data_filled.csv`.\n",
    "- **Silver**: MapReduce-style aggregations and basic cleaning.\n",
    "- **Gold**: ML, features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3014fd",
   "metadata": {},
   "source": [
    "# Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33f80f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/26 00:51:00 WARN Utils: Your hostname, Red-Macbook-Pro.local, resolves to a loopback address: 127.0.0.1; using 192.168.1.94 instead (on interface en0)\n",
      "25/11/26 00:51:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/26 00:51:00 WARN Utils: Your hostname, Red-Macbook-Pro.local, resolves to a loopback address: 127.0.0.1; using 192.168.1.94 instead (on interface en0)\n",
      "25/11/26 00:51:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/26 00:51:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/26 00:51:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[Stage 2:===========================================>            (40 + 10) / 51]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze count: 20011378\n",
      "Bronze schema:\n",
      "root\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Education: string (nullable = true)\n",
      " |-- JobPosition: string (nullable = true)\n",
      " |-- ExperienceYears: double (nullable = true)\n",
      " |-- Salary: string (nullable = true)\n",
      " |-- JobSatisfaction: double (nullable = true)\n",
      " |-- WorkLifeBalance: double (nullable = true)\n",
      " |-- Opportunities: double (nullable = true)\n",
      " |-- Certifications: double (nullable = true)\n",
      " |-- FreelancingExperience: double (nullable = true)\n",
      " |-- CompanySize: string (nullable = true)\n",
      " |-- Remote: string (nullable = true)\n",
      " |-- CompanyLocation: string (nullable = true)\n",
      " |-- EmployeeResidence: string (nullable = true)\n",
      " |-- Currency: string (nullable = true)\n",
      " |-- year: double (nullable = true)\n",
      " |-- EmployementStatus: string (nullable = true)\n",
      " |-- OvertimePay: double (nullable = true)\n",
      " |-- SeniorityLevel: string (nullable = true)\n",
      " |-- LongivityPay: double (nullable = true)\n",
      " |-- TechnologyAdoption: double (nullable = true)\n",
      " |-- SkillsGap: double (nullable = true)\n",
      " |-- Geographic Mobility: double (nullable = true)\n",
      " |-- Professional Networks: double (nullable = true)\n",
      " |-- Industry Growth Rate: string (nullable = true)\n",
      " |-- Job Security: double (nullable = true)\n",
      " |-- Field of Study: string (nullable = true)\n",
      "\n",
      "+----+------+----------+-----------------+---------------+--------+---------------+---------------+-------------+--------------+---------------------+-----------+------+---------------+-----------------+--------+----+-----------------+-----------+--------------+------------+------------------+---------+-------------------+---------------------+--------------------+------------+--------------+\n",
      "|Age |Gender|Education |JobPosition      |ExperienceYears|Salary  |JobSatisfaction|WorkLifeBalance|Opportunities|Certifications|FreelancingExperience|CompanySize|Remote|CompanyLocation|EmployeeResidence|Currency|year|EmployementStatus|OvertimePay|SeniorityLevel|LongivityPay|TechnologyAdoption|SkillsGap|Geographic Mobility|Professional Networks|Industry Growth Rate|Job Security|Field of Study|\n",
      "+----+------+----------+-----------------+---------------+--------+---------------+---------------+-------------+--------------+---------------------+-----------+------+---------------+-----------------+--------+----+-----------------+-----------+--------------+------------+------------------+---------+-------------------+---------------------+--------------------+------------+--------------+\n",
      "|32.0|Male  |Bachelor's|Software Engineer|5.0            |90000.0 |9.0            |1.0            |89.0         |1.0           |0.0                  |S          |0     |SLB            |GRC              |USD     |NULL|Part-time        |24867.66   |Mid-level     |18072.36    |3.0               |6.0      |1.0                |8.0                  |Medium              |6.0         |NULL          |\n",
      "|28.0|Female|Master's  |Data Analyst     |3.0            |65000.0 |2.0            |7.0            |12.0         |1.0           |0.0                  |M          |0     |SPM            |GUY              |USD     |NULL|Part-time        |23010.22   |Mid-level     |11239.44    |5.0               |7.0      |1.0                |9.0                  |Medium              |5.0         |NULL          |\n",
      "|45.0|Male  |PhD       |Senior Manager   |15.0           |150000.0|1.0            |5.0            |52.0         |1.0           |1.0                  |M          |50    |BVT            |NFK              |USD     |NULL|Contractor       |38462.8    |Senior        |18917.45    |5.0               |9.0      |1.0                |1.0                  |High                |4.0         |Biology       |\n",
      "|36.0|Female|Bachelor's|Sales Associate  |7.0            |60000.0 |1.0            |10.0           |94.0         |0.0           |0.0                  |XL         |100   |BFA            |PHL              |USD     |NULL|Freelance        |842.72     |Executive     |16087.02    |3.0               |10.0     |1.0                |1.0                  |High                |2.0         |NULL          |\n",
      "|52.0|Male  |Master's  |Director         |20.0           |200000.0|1.0            |8.0            |45.0         |0.0           |1.0                  |S          |0     |ITA            |DEU              |USD     |NULL|Full-time        |28522.79   |Entry-level   |45838.82    |2.0               |3.0      |0.0                |3.0                  |High                |9.0         |NULL          |\n",
      "+----+------+----------+-----------------+---------------+--------+---------------+---------------+-------------+--------------+---------------------+-----------+------+---------------+-----------------+--------+----+-----------------+-----------+--------------+------------+------------------+---------+-------------------+---------------------+--------------------+------------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 00:51:22 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SalaryMedallionPipeline\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "bronze_path = \"data/PART_5_fill_with_more_data/combined_data_filled.csv\"\n",
    "\n",
    "bronze_df = (\n",
    "    spark.read\n",
    "         .option(\"header\", \"true\")\n",
    "         .option(\"inferSchema\", \"true\")\n",
    "         .csv(bronze_path)\n",
    ")\n",
    "\n",
    "print(\"Bronze count:\", bronze_df.count())\n",
    "print(\"Bronze schema:\")\n",
    "bronze_df.printSchema()\n",
    "bronze_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0f4e02",
   "metadata": {},
   "source": [
    "# Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4bf8ce25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: double (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- jobposition: string (nullable = true)\n",
      " |-- experienceyears: double (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- jobsatisfaction: double (nullable = true)\n",
      " |-- worklifebalance: double (nullable = true)\n",
      " |-- opportunities: double (nullable = true)\n",
      " |-- certifications: double (nullable = true)\n",
      " |-- freelancingexperience: double (nullable = true)\n",
      " |-- companysize: string (nullable = true)\n",
      " |-- remote: string (nullable = true)\n",
      " |-- companylocation: string (nullable = true)\n",
      " |-- employeeresidence: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- year: double (nullable = true)\n",
      " |-- employementstatus: string (nullable = true)\n",
      " |-- overtimepay: double (nullable = true)\n",
      " |-- senioritylevel: string (nullable = true)\n",
      " |-- longivitypay: double (nullable = true)\n",
      " |-- technologyadoption: double (nullable = true)\n",
      " |-- skillsgap: double (nullable = true)\n",
      " |-- geographic_mobility: double (nullable = true)\n",
      " |-- professional_networks: double (nullable = true)\n",
      " |-- industry_growth_rate: string (nullable = true)\n",
      " |-- job_security: double (nullable = true)\n",
      " |-- field_of_study: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=======================================================> (50 + 1) / 51]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "silver count: 19406438\n",
      "+--------+------------+\n",
      "|salary  |salary_clean|\n",
      "+--------+------------+\n",
      "|90000.0 |900000.0    |\n",
      "|65000.0 |650000.0    |\n",
      "|150000.0|1500000.0   |\n",
      "|60000.0 |600000.0    |\n",
      "|200000.0|2000000.0   |\n",
      "|55000.0 |550000.0    |\n",
      "|120000.0|1200000.0   |\n",
      "|80000.0 |800000.0    |\n",
      "|45000.0 |450000.0    |\n",
      "|110000.0|1100000.0   |\n",
      "+--------+------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, regexp_replace, split, trim, size\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# normalize column names\n",
    "silver_df = bronze_df\n",
    "for c in silver_df.columns:\n",
    "    silver_df = silver_df.withColumnRenamed(\n",
    "        c,\n",
    "        c.strip().lower().replace(\" \", \"_\").replace(\"-\", \"_\").replace(\"/\", \"_\")\n",
    "    )\n",
    "\n",
    "silver_df.printSchema()\n",
    "\n",
    "# salary cleaning as we sometimes have salaries like \"€44,521 - €59,359\"\n",
    "if \"salary\" in silver_df.columns:\n",
    "    cleaned_salary = regexp_replace(col(\"salary\"), \"[^0-9\\\\-]\", \"\") # keep only digits and dash\n",
    "    parts = split(cleaned_salary, \"-\") # split on dash to get low/high\n",
    "\n",
    "    low_raw = trim(parts.getItem(0))\n",
    "    high_raw = trim(F.when(size(parts) > 1, parts.getItem(1)).otherwise(F.lit(None)))\n",
    "\n",
    "    # cast to double when non-empty\n",
    "    low_num = F.when((low_raw.isNotNull()) & (low_raw != \"\"), low_raw.cast(DoubleType()))\n",
    "    high_num = F.when((high_raw.isNotNull()) & (high_raw != \"\"), high_raw.cast(DoubleType()))\n",
    "\n",
    "    silver_df = silver_df.withColumn(\"salary_clean\", F.when(high_num.isNotNull(), (low_num + high_num) / 2.0).otherwise(low_num)) # if high exists, use average of low/high; else just low\n",
    "\n",
    "\n",
    "silver_df = silver_df.filter(col(\"salary_clean\").isNotNull()) # filter for only valid numeric salary\n",
    "\n",
    "print(\"silver count:\", silver_df.count())\n",
    "silver_df.select(\"salary\", \"salary_clean\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a61be40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "silver agg count: 313370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:======================================================> (50 + 1) / 51]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+---------------+------------------+--------------+\n",
      "|jobposition                |companylocation|avg_salary        |salary_records|\n",
      "+---------------------------+---------------+------------------+--------------+\n",
      "|Director of Human Resources|THA            |1900000.0         |11            |\n",
      "|Director of Marketing      |CMR            |1800000.0         |9             |\n",
      "|Junior Financial Analyst   |UGA            |600000.0          |17            |\n",
      "|Senior Business Analyst    |LAO            |1500000.0         |15            |\n",
      "|Product Manager            |PRY            |1372413.7931034483|29            |\n",
      "|Data Scientist             |SOM            |1598055.5555555555|36            |\n",
      "|Software Engineer          |ECU            |785104.1666666666 |48            |\n",
      "|Senior Software Engineer   |SYC            |1669762.5         |32            |\n",
      "|Full Stack Engineer        |AIA            |1810210.0         |20            |\n",
      "|Software Engineer Manager  |DMA            |1905960.0         |9             |\n",
      "+---------------------------+---------------+------------------+--------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# aggregate in Silver --> average salary, count per job_role and location\n",
    "group_cols = []\n",
    "\n",
    "if \"job_role\" in silver_df.columns:\n",
    "    group_cols.append(\"job_role\")\n",
    "elif \"jobposition\" in silver_df.columns:\n",
    "    group_cols.append(\"jobposition\")\n",
    "elif \"occ_title\" in silver_df.columns:\n",
    "    group_cols.append(\"occ_title\")\n",
    "\n",
    "if \"location\" in silver_df.columns:\n",
    "    group_cols.append(\"location\")\n",
    "elif \"companylocation\" in silver_df.columns:\n",
    "    group_cols.append(\"companylocation\")\n",
    "elif \"employeeresidence\" in silver_df.columns:\n",
    "    group_cols.append(\"employeeresidence\")\n",
    "\n",
    "if \"salary_clean\" in silver_df.columns and group_cols:\n",
    "    silver_agg_df = silver_df.groupBy(*group_cols).agg(\n",
    "        F.avg(col(\"salary_clean\")).alias(\"avg_salary\"),\n",
    "        F.count(col(\"salary_clean\")).alias(\"salary_records\"),\n",
    "    )\n",
    "else:\n",
    "    silver_agg_df = silver_df\n",
    "\n",
    "print(\"silver agg count:\", silver_agg_df.count())\n",
    "silver_agg_df.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e54a3b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 00:52:47 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:52:47 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:52:47 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:52:59 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:52:59 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:52:59 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:52:59 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:52:59 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:52:59 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:52:59 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:52:59 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:52:59 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:52:59 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:52:59 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:52:59 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:52:59 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:52:59 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:00 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:00 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:53:00 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:53:00 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:00 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:00 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:00 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:53:00 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:53:00 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:00 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:04 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:04 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:04 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:04 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:53:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:53:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:53:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:53:10 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:53:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:53:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:11 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:15 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:16 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:16 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:16 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:16 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:21 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:21 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:53:21 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:21 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:21 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:21 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:53:21 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:21 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:21 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:21 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:53:21 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:21 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:21 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:21 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:53:21 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:21 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:53:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:53:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:53:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:53:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:26 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:26 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:26 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:26 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:32 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:32 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:53:32 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:32 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:32 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:32 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:53:32 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:32 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:32 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:32 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:53:32 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:32 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:53:32 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:32 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:32 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:32 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:53:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:53:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:53:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:53:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:33 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:37 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:37 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:37 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:37 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:43 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:43 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:43 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:43 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:43 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 00:53:43 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 00:53:43 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 00:53:43 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "[Stage 20:======================================================> (50 + 1) / 51]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "silver saved to:\n",
      "detail saved to: data/PART_5_fill_with_more_data/silver_detail.parquet\n",
      "agg saved to: data/PART_5_fill_with_more_data/silver_agg.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "silver_detail_path = \"data/PART_5_fill_with_more_data/silver_detail.parquet\"\n",
    "silver_agg_path    = \"data/PART_5_fill_with_more_data/silver_agg.parquet\"\n",
    "\n",
    "silver_df.write.mode(\"overwrite\").parquet(silver_detail_path)\n",
    "silver_agg_df.write.mode(\"overwrite\").parquet(silver_agg_path)\n",
    "\n",
    "print(\"silver saved to:\")\n",
    "print(\"detail saved to:\", silver_detail_path)\n",
    "print(\"agg saved to:\", silver_agg_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4556b3e",
   "metadata": {},
   "source": [
    "# Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bd2f4ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "silver columns:\n",
      "detail cols: ['age', 'gender', 'education', 'jobposition', 'experienceyears', 'salary', 'jobsatisfaction', 'worklifebalance', 'opportunities', 'certifications', 'freelancingexperience', 'companysize', 'remote', 'companylocation', 'employeeresidence', 'currency', 'year', 'employementstatus', 'overtimepay', 'senioritylevel', 'longivitypay', 'technologyadoption', 'skillsgap', 'geographic_mobility', 'professional_networks', 'industry_growth_rate', 'job_security', 'field_of_study', 'salary_clean']\n",
      "agg cols: ['jobposition', 'companylocation', 'avg_salary', 'salary_records']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "silver_detail_loaded = spark.read.parquet(silver_detail_path)\n",
    "silver_agg_loaded = spark.read.parquet(silver_agg_path)\n",
    "\n",
    "print(\"silver columns:\")\n",
    "print(\"detail cols:\", silver_detail_loaded.columns)\n",
    "print(\"agg cols:\", silver_agg_loaded.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "206bd66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 01:13:58 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold agg features saved to:\n",
      "parquet: data/PART_5_fill_with_more_data/gold_features.parquet\n",
      "csv: data/PART_5_fill_with_more_data/gold_features_csv\n"
     ]
    }
   ],
   "source": [
    "# we choose role and location columns from aggregated Silver\n",
    "candidate_role_cols = [\"job_role\", \"jobposition\", \"occ_title\"]\n",
    "candidate_location_cols = [\"location\", \"companylocation\", \"employeeresidence\"]\n",
    "\n",
    "role_col = next((c for c in candidate_role_cols if c in silver_agg_loaded.columns), None)\n",
    "location_col = next((c for c in candidate_location_cols if c in silver_agg_loaded.columns), None)\n",
    "\n",
    "selected_cols = []\n",
    "if role_col: selected_cols.append(role_col)\n",
    "if location_col: selected_cols.append(location_col)\n",
    "\n",
    "for metric_col in [\"avg_salary\", \"salary_records\"]:\n",
    "    if metric_col in silver_agg_loaded.columns:\n",
    "        selected_cols.append(metric_col)\n",
    "\n",
    "if not selected_cols:\n",
    "    print(\"no columns in silver agg which is why we take all\")\n",
    "    gold_features_df = silver_agg_loaded\n",
    "else:\n",
    "    gold_features_df = silver_agg_loaded.select(*selected_cols)\n",
    "\n",
    "gold_features_path_parquet = \"data/PART_5_fill_with_more_data/gold_features.parquet\"\n",
    "gold_features_path_csv = \"data/PART_5_fill_with_more_data/gold_features_csv\"\n",
    "\n",
    "gold_features_df.write.mode(\"overwrite\").parquet(gold_features_path_parquet)\n",
    "gold_features_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(gold_features_path_csv)\n",
    "\n",
    "print(\"gold agg features saved to:\")\n",
    "print(\"parquet:\", gold_features_path_parquet)\n",
    "print(\"csv:\", gold_features_path_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "39520c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the gold: 19406438 6\n",
      "+--------------------------------------------------+---------------+------+--------------+-------------------+------------+\n",
      "|jobposition                                       |companylocation|year  |senioritylevel|experienceyears    |salary_clean|\n",
      "+--------------------------------------------------+---------------+------+--------------+-------------------+------------+\n",
      "|Industrial Process Machinery Mechanic             |LBR            |2024.0|Mid-level     |0.5159837966396452 |618160.0    |\n",
      "|Correctional Officer                              |NGA            |2021.0|Executive     |0.8234598102332119 |511970.0    |\n",
      "|Research Scientist                                |US             |2024.0|Senior        |0.12123162758559648|300000.0    |\n",
      "|Special Order Liquor and Wine Warehouse Operations|COD            |2021.0|Senior        |0.30060440808759425|605550.0    |\n",
      "|Software Engineer                                 |CA             |2025.0|Mid-level     |1.5243722584683221 |184800.0    |\n",
      "+--------------------------------------------------+---------------+------+--------------+-------------------+------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 01:14:01 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 01:14:01 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 01:14:01 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 01:14:02 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 01:14:02 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 01:14:02 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 01:14:02 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 01:14:02 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 01:14:02 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 01:14:02 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 01:14:02 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 01:14:02 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 01:14:02 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 01:14:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 01:14:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 01:14:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 01:14:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 01:14:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 01:14:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 01:14:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 01:14:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 01:14:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 01:14:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 01:14:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 01:14:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 01:14:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 01:14:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 01:14:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 01:14:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 01:14:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 01:14:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/11/26 01:14:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 01:14:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/11/26 01:14:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/11/26 01:14:03 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "[Stage 234:====================================================>  (24 + 1) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gold saved to: data/PART_5_fill_with_more_data/gold_detail.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ml_base_df = silver_detail_loaded\n",
    "detail_cols = []\n",
    "\n",
    "# even though we shouldn't have all these columns as we dropped them or combined them before, we still going to check for them just in case something happened.\n",
    "for c in [\"job_role\", \"jobposition\", \"occ_title\"]:\n",
    "    if not c in ml_base_df.columns: continue\n",
    "    detail_cols.append(c)\n",
    "    break\n",
    "\n",
    "for c in [\"location\", \"companylocation\", \"employeeresidence\"]:\n",
    "    if not c in ml_base_df.columns: continue\n",
    "    detail_cols.append(c)\n",
    "    break\n",
    "\n",
    "for c in [\"year\", \"fiscal_year\"]:\n",
    "    if not c in ml_base_df.columns: continue\n",
    "    detail_cols.append(c)\n",
    "    break\n",
    "\n",
    "for c in [\"seniority_level\", \"senioritylevel\"]:\n",
    "    if not c in ml_base_df.columns: continue\n",
    "    detail_cols.append(c)\n",
    "    break\n",
    "\n",
    "for c in [\"experience_years\", \"experienceyears\"]:\n",
    "    if not c in ml_base_df.columns: continue\n",
    "    detail_cols.append(c)\n",
    "    break\n",
    "\n",
    "detail_cols.append(\"salary_clean\")\n",
    "gold_detail_df = ml_base_df.select(*detail_cols)\n",
    "\n",
    "print(\"shape of the gold:\", gold_detail_df.count(), len(gold_detail_df.columns))\n",
    "gold_detail_df.show(5, truncate=False)\n",
    "\n",
    "gold_detail_path = \"data/PART_5_fill_with_more_data/gold_detail.parquet\"\n",
    "gold_detail_df.write.mode(\"overwrite\").parquet(gold_detail_path)\n",
    "print(\"gold saved to:\", gold_detail_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8050a719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML dataframe after dropna: 19312037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 01:09:34 WARN MemoryStore: Not enough space to cache rdd_500_8 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:09:34 WARN BlockManager: Persisting block rdd_500_8 to disk instead.\n",
      "25/11/26 01:09:34 WARN MemoryStore: Not enough space to cache rdd_500_2 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:09:34 WARN BlockManager: Persisting block rdd_500_2 to disk instead.\n",
      "25/11/26 01:09:34 WARN MemoryStore: Not enough space to cache rdd_500_4 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:09:34 WARN BlockManager: Persisting block rdd_500_4 to disk instead.\n",
      "25/11/26 01:09:34 WARN MemoryStore: Not enough space to cache rdd_500_0 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:09:34 WARN BlockManager: Persisting block rdd_500_0 to disk instead.\n",
      "25/11/26 01:09:34 WARN MemoryStore: Not enough space to cache rdd_500_1 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:09:34 WARN BlockManager: Persisting block rdd_500_1 to disk instead.\n",
      "25/11/26 01:09:34 WARN MemoryStore: Not enough space to cache rdd_500_5 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:09:34 WARN BlockManager: Persisting block rdd_500_5 to disk instead.\n",
      "25/11/26 01:09:34 WARN MemoryStore: Not enough space to cache rdd_500_3 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:09:34 WARN BlockManager: Persisting block rdd_500_3 to disk instead.\n",
      "25/11/26 01:09:34 WARN MemoryStore: Not enough space to cache rdd_500_6 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:09:34 WARN BlockManager: Persisting block rdd_500_6 to disk instead.\n",
      "25/11/26 01:09:34 WARN MemoryStore: Not enough space to cache rdd_500_9 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:09:34 WARN BlockManager: Persisting block rdd_500_9 to disk instead.\n",
      "25/11/26 01:09:34 WARN MemoryStore: Not enough space to cache rdd_500_7 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:09:34 WARN BlockManager: Persisting block rdd_500_7 to disk instead.\n",
      "25/11/26 01:09:34 WARN MemoryStore: Not enough space to cache rdd_500_8 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:09:34 WARN BlockManager: Persisting block rdd_500_8 to disk instead.\n",
      "25/11/26 01:09:34 WARN MemoryStore: Not enough space to cache rdd_500_2 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:09:34 WARN BlockManager: Persisting block rdd_500_2 to disk instead.\n",
      "25/11/26 01:09:34 WARN MemoryStore: Not enough space to cache rdd_500_4 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:09:34 WARN BlockManager: Persisting block rdd_500_4 to disk instead.\n",
      "25/11/26 01:09:34 WARN MemoryStore: Not enough space to cache rdd_500_0 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:09:34 WARN BlockManager: Persisting block rdd_500_0 to disk instead.\n",
      "25/11/26 01:09:34 WARN MemoryStore: Not enough space to cache rdd_500_1 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:09:34 WARN BlockManager: Persisting block rdd_500_1 to disk instead.\n",
      "25/11/26 01:09:34 WARN MemoryStore: Not enough space to cache rdd_500_5 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:09:34 WARN BlockManager: Persisting block rdd_500_5 to disk instead.\n",
      "25/11/26 01:09:34 WARN MemoryStore: Not enough space to cache rdd_500_3 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:09:34 WARN BlockManager: Persisting block rdd_500_3 to disk instead.\n",
      "25/11/26 01:09:34 WARN MemoryStore: Not enough space to cache rdd_500_6 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:09:34 WARN BlockManager: Persisting block rdd_500_6 to disk instead.\n",
      "25/11/26 01:09:34 WARN MemoryStore: Not enough space to cache rdd_500_9 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:09:34 WARN BlockManager: Persisting block rdd_500_9 to disk instead.\n",
      "25/11/26 01:09:34 WARN MemoryStore: Not enough space to cache rdd_500_7 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:09:34 WARN BlockManager: Persisting block rdd_500_7 to disk instead.\n",
      "25/11/26 01:09:37 WARN MemoryStore: Not enough space to cache rdd_500_7 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:09:37 WARN MemoryStore: Not enough space to cache rdd_500_2 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:09:37 WARN MemoryStore: Not enough space to cache rdd_500_8 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:09:37 WARN MemoryStore: Not enough space to cache rdd_500_1 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:09:37 WARN MemoryStore: Not enough space to cache rdd_500_6 in memory! (computed 18.0 MiB so far)\n",
      "25/11/26 01:09:37 WARN MemoryStore: Not enough space to cache rdd_500_3 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:09:37 WARN MemoryStore: Not enough space to cache rdd_500_9 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:09:37 WARN MemoryStore: Not enough space to cache rdd_500_4 in memory! (computed 1030.0 KiB so far)\n",
      "25/11/26 01:09:37 WARN MemoryStore: Not enough space to cache rdd_500_0 in memory! (computed 18.0 MiB so far)\n",
      "25/11/26 01:09:37 WARN MemoryStore: Not enough space to cache rdd_500_5 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:09:37 WARN MemoryStore: Not enough space to cache rdd_500_7 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:09:37 WARN MemoryStore: Not enough space to cache rdd_500_2 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:09:37 WARN MemoryStore: Not enough space to cache rdd_500_8 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:09:37 WARN MemoryStore: Not enough space to cache rdd_500_1 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:09:37 WARN MemoryStore: Not enough space to cache rdd_500_6 in memory! (computed 18.0 MiB so far)\n",
      "25/11/26 01:09:37 WARN MemoryStore: Not enough space to cache rdd_500_3 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:09:37 WARN MemoryStore: Not enough space to cache rdd_500_9 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:09:37 WARN MemoryStore: Not enough space to cache rdd_500_4 in memory! (computed 1030.0 KiB so far)\n",
      "25/11/26 01:09:37 WARN MemoryStore: Not enough space to cache rdd_500_0 in memory! (computed 18.0 MiB so far)\n",
      "25/11/26 01:09:37 WARN MemoryStore: Not enough space to cache rdd_500_5 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:09:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_500_18 in memory.\n",
      "25/11/26 01:09:40 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_500_18 in memory.\n",
      "25/11/26 01:09:41 WARN MemoryStore: Not enough space to cache rdd_500_10 in memory! (computed 1030.0 KiB so far)\n",
      "25/11/26 01:09:41 WARN BlockManager: Persisting block rdd_500_10 to disk instead.\n",
      "25/11/26 01:09:41 WARN MemoryStore: Not enough space to cache rdd_500_10 in memory! (computed 1030.0 KiB so far)\n",
      "25/11/26 01:09:41 WARN BlockManager: Persisting block rdd_500_10 to disk instead.\n",
      "25/11/26 01:09:42 WARN MemoryStore: Not enough space to cache rdd_500_14 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:09:42 WARN BlockManager: Persisting block rdd_500_14 to disk instead.\n",
      "25/11/26 01:09:42 WARN MemoryStore: Not enough space to cache rdd_500_12 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:09:42 WARN BlockManager: Persisting block rdd_500_12 to disk instead.\n",
      "25/11/26 01:09:42 WARN MemoryStore: Not enough space to cache rdd_500_15 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:09:42 WARN BlockManager: Persisting block rdd_500_15 to disk instead.\n",
      "25/11/26 01:09:42 WARN MemoryStore: Not enough space to cache rdd_500_16 in memory! (computed 8.0 MiB so far)\n",
      "25/11/26 01:09:42 WARN BlockManager: Persisting block rdd_500_16 to disk instead.\n",
      "25/11/26 01:09:42 WARN MemoryStore: Not enough space to cache rdd_500_11 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:09:42 WARN BlockManager: Persisting block rdd_500_11 to disk instead.\n",
      "25/11/26 01:09:42 WARN MemoryStore: Not enough space to cache rdd_500_14 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:09:42 WARN BlockManager: Persisting block rdd_500_14 to disk instead.\n",
      "25/11/26 01:09:42 WARN MemoryStore: Not enough space to cache rdd_500_12 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:09:42 WARN BlockManager: Persisting block rdd_500_12 to disk instead.\n",
      "25/11/26 01:09:42 WARN MemoryStore: Not enough space to cache rdd_500_15 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:09:42 WARN BlockManager: Persisting block rdd_500_15 to disk instead.\n",
      "25/11/26 01:09:42 WARN MemoryStore: Not enough space to cache rdd_500_16 in memory! (computed 8.0 MiB so far)\n",
      "25/11/26 01:09:42 WARN BlockManager: Persisting block rdd_500_16 to disk instead.\n",
      "25/11/26 01:09:42 WARN MemoryStore: Not enough space to cache rdd_500_11 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:09:42 WARN BlockManager: Persisting block rdd_500_11 to disk instead.\n",
      "25/11/26 01:09:42 WARN MemoryStore: Not enough space to cache rdd_500_18 in memory! (computed 504.0 B so far)\n",
      "25/11/26 01:09:42 WARN BlockManager: Persisting block rdd_500_18 to disk instead.\n",
      "25/11/26 01:09:42 WARN MemoryStore: Not enough space to cache rdd_500_18 in memory! (computed 504.0 B so far)\n",
      "25/11/26 01:09:42 WARN BlockManager: Persisting block rdd_500_18 to disk instead.\n",
      "25/11/26 01:09:43 WARN MemoryStore: Not enough space to cache rdd_500_19 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:09:43 WARN BlockManager: Persisting block rdd_500_19 to disk instead.\n",
      "25/11/26 01:09:43 WARN MemoryStore: Not enough space to cache rdd_500_17 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:09:43 WARN BlockManager: Persisting block rdd_500_17 to disk instead.\n",
      "25/11/26 01:09:43 WARN MemoryStore: Not enough space to cache rdd_500_19 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:09:43 WARN BlockManager: Persisting block rdd_500_19 to disk instead.\n",
      "25/11/26 01:09:43 WARN MemoryStore: Not enough space to cache rdd_500_17 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:09:43 WARN BlockManager: Persisting block rdd_500_17 to disk instead.\n",
      "25/11/26 01:09:45 WARN MemoryStore: Not enough space to cache rdd_500_15 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:09:45 WARN MemoryStore: Not enough space to cache rdd_500_14 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:09:45 WARN MemoryStore: Not enough space to cache rdd_500_10 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:09:45 WARN MemoryStore: Not enough space to cache rdd_500_11 in memory! (computed 12.0 MiB so far)\n",
      "25/11/26 01:09:45 WARN MemoryStore: Not enough space to cache rdd_500_12 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:09:45 WARN MemoryStore: Not enough space to cache rdd_500_15 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:09:45 WARN MemoryStore: Not enough space to cache rdd_500_14 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:09:45 WARN MemoryStore: Not enough space to cache rdd_500_10 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:09:45 WARN MemoryStore: Not enough space to cache rdd_500_11 in memory! (computed 12.0 MiB so far)\n",
      "25/11/26 01:09:45 WARN MemoryStore: Not enough space to cache rdd_500_12 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:09:46 WARN MemoryStore: Not enough space to cache rdd_500_16 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:09:46 WARN MemoryStore: Not enough space to cache rdd_500_16 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:09:47 WARN MemoryStore: Not enough space to cache rdd_500_19 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:09:47 WARN MemoryStore: Not enough space to cache rdd_500_17 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:09:47 WARN MemoryStore: Not enough space to cache rdd_500_18 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:09:47 WARN MemoryStore: Not enough space to cache rdd_500_19 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:09:47 WARN MemoryStore: Not enough space to cache rdd_500_17 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:09:47 WARN MemoryStore: Not enough space to cache rdd_500_18 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:09:50 WARN MemoryStore: Not enough space to cache rdd_500_23 in memory! (computed 1549.4 KiB so far)\n",
      "25/11/26 01:09:50 WARN BlockManager: Persisting block rdd_500_23 to disk instead.\n",
      "25/11/26 01:09:50 WARN MemoryStore: Not enough space to cache rdd_500_22 in memory! (computed 2.3 MiB so far)\n",
      "25/11/26 01:09:50 WARN BlockManager: Persisting block rdd_500_22 to disk instead.\n",
      "25/11/26 01:09:50 WARN MemoryStore: Not enough space to cache rdd_500_20 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:09:50 WARN BlockManager: Persisting block rdd_500_20 to disk instead.\n",
      "25/11/26 01:09:50 WARN MemoryStore: Not enough space to cache rdd_500_23 in memory! (computed 1549.4 KiB so far)\n",
      "25/11/26 01:09:50 WARN BlockManager: Persisting block rdd_500_23 to disk instead.\n",
      "25/11/26 01:09:50 WARN MemoryStore: Not enough space to cache rdd_500_22 in memory! (computed 2.3 MiB so far)\n",
      "25/11/26 01:09:50 WARN BlockManager: Persisting block rdd_500_22 to disk instead.\n",
      "25/11/26 01:09:50 WARN MemoryStore: Not enough space to cache rdd_500_20 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:09:50 WARN BlockManager: Persisting block rdd_500_20 to disk instead.\n",
      "25/11/26 01:09:50 WARN MemoryStore: Not enough space to cache rdd_500_21 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:09:50 WARN BlockManager: Persisting block rdd_500_21 to disk instead.\n",
      "25/11/26 01:09:50 WARN MemoryStore: Not enough space to cache rdd_500_21 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:09:50 WARN BlockManager: Persisting block rdd_500_21 to disk instead.\n",
      "25/11/26 01:09:52 WARN MemoryStore: Not enough space to cache rdd_500_20 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:09:52 WARN MemoryStore: Not enough space to cache rdd_500_20 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:09:52 WARN MemoryStore: Not enough space to cache rdd_500_24 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:09:52 WARN BlockManager: Persisting block rdd_500_24 to disk instead.\n",
      "25/11/26 01:09:52 WARN MemoryStore: Not enough space to cache rdd_500_21 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:09:52 WARN MemoryStore: Not enough space to cache rdd_500_24 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:09:52 WARN BlockManager: Persisting block rdd_500_24 to disk instead.\n",
      "25/11/26 01:09:52 WARN MemoryStore: Not enough space to cache rdd_500_21 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:09:53 WARN MemoryStore: Not enough space to cache rdd_500_22 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:09:53 WARN MemoryStore: Not enough space to cache rdd_500_22 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:09:58 WARN MemoryStore: Not enough space to cache rdd_500_24 in memory! (computed 216.8 MiB so far)\n",
      "25/11/26 01:09:58 WARN MemoryStore: Not enough space to cache rdd_500_24 in memory! (computed 216.8 MiB so far)\n",
      "25/11/26 01:10:01 WARN MemoryStore: Not enough space to cache rdd_500_2 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:01 WARN MemoryStore: Not enough space to cache rdd_500_0 in memory! (computed 18.0 MiB so far)\n",
      "25/11/26 01:10:01 WARN MemoryStore: Not enough space to cache rdd_500_7 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:01 WARN MemoryStore: Not enough space to cache rdd_500_4 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:01 WARN MemoryStore: Not enough space to cache rdd_500_8 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:01 WARN MemoryStore: Not enough space to cache rdd_500_5 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:01 WARN MemoryStore: Not enough space to cache rdd_500_6 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:01 WARN MemoryStore: Not enough space to cache rdd_500_9 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:01 WARN MemoryStore: Not enough space to cache rdd_500_1 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:01 WARN MemoryStore: Not enough space to cache rdd_500_3 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:01 WARN MemoryStore: Not enough space to cache rdd_500_2 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:01 WARN MemoryStore: Not enough space to cache rdd_500_0 in memory! (computed 18.0 MiB so far)\n",
      "25/11/26 01:10:01 WARN MemoryStore: Not enough space to cache rdd_500_7 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:01 WARN MemoryStore: Not enough space to cache rdd_500_4 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:01 WARN MemoryStore: Not enough space to cache rdd_500_8 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:01 WARN MemoryStore: Not enough space to cache rdd_500_5 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:01 WARN MemoryStore: Not enough space to cache rdd_500_6 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:01 WARN MemoryStore: Not enough space to cache rdd_500_9 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:01 WARN MemoryStore: Not enough space to cache rdd_500_1 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:01 WARN MemoryStore: Not enough space to cache rdd_500_3 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:04 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_500_16 in memory.\n",
      "25/11/26 01:10:04 WARN MemoryStore: Not enough space to cache rdd_500_16 in memory! (computed 504.0 B so far)\n",
      "25/11/26 01:10:04 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_500_17 in memory.\n",
      "25/11/26 01:10:04 WARN MemoryStore: Not enough space to cache rdd_500_17 in memory! (computed 504.0 B so far)\n",
      "25/11/26 01:10:04 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_500_18 in memory.\n",
      "25/11/26 01:10:04 WARN MemoryStore: Not enough space to cache rdd_500_18 in memory! (computed 504.0 B so far)\n",
      "25/11/26 01:10:04 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_500_16 in memory.\n",
      "25/11/26 01:10:04 WARN MemoryStore: Not enough space to cache rdd_500_16 in memory! (computed 504.0 B so far)\n",
      "25/11/26 01:10:04 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_500_17 in memory.\n",
      "25/11/26 01:10:04 WARN MemoryStore: Not enough space to cache rdd_500_17 in memory! (computed 504.0 B so far)\n",
      "25/11/26 01:10:04 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_500_18 in memory.\n",
      "25/11/26 01:10:04 WARN MemoryStore: Not enough space to cache rdd_500_18 in memory! (computed 504.0 B so far)\n",
      "25/11/26 01:10:04 WARN MemoryStore: Not enough space to cache rdd_500_13 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:10:04 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_500_19 in memory.\n",
      "25/11/26 01:10:04 WARN MemoryStore: Not enough space to cache rdd_500_19 in memory! (computed 504.0 B so far)\n",
      "25/11/26 01:10:04 WARN MemoryStore: Not enough space to cache rdd_500_15 in memory! (computed 1549.4 KiB so far)\n",
      "25/11/26 01:10:04 WARN MemoryStore: Not enough space to cache rdd_500_14 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:04 WARN MemoryStore: Not enough space to cache rdd_500_12 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:04 WARN MemoryStore: Not enough space to cache rdd_500_10 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:04 WARN MemoryStore: Not enough space to cache rdd_500_11 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:04 WARN MemoryStore: Not enough space to cache rdd_500_13 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:10:04 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_500_19 in memory.\n",
      "25/11/26 01:10:04 WARN MemoryStore: Not enough space to cache rdd_500_19 in memory! (computed 504.0 B so far)\n",
      "25/11/26 01:10:04 WARN MemoryStore: Not enough space to cache rdd_500_15 in memory! (computed 1549.4 KiB so far)\n",
      "25/11/26 01:10:04 WARN MemoryStore: Not enough space to cache rdd_500_14 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:04 WARN MemoryStore: Not enough space to cache rdd_500_12 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:04 WARN MemoryStore: Not enough space to cache rdd_500_10 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:04 WARN MemoryStore: Not enough space to cache rdd_500_11 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:07 WARN MemoryStore: Not enough space to cache rdd_500_20 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:07 WARN MemoryStore: Not enough space to cache rdd_500_21 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:07 WARN MemoryStore: Not enough space to cache rdd_500_22 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:07 WARN MemoryStore: Not enough space to cache rdd_500_24 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:07 WARN MemoryStore: Not enough space to cache rdd_500_20 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:07 WARN MemoryStore: Not enough space to cache rdd_500_21 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:07 WARN MemoryStore: Not enough space to cache rdd_500_22 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:07 WARN MemoryStore: Not enough space to cache rdd_500_24 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:12 WARN MemoryStore: Not enough space to cache rdd_500_0 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:12 WARN MemoryStore: Not enough space to cache rdd_500_2 in memory! (computed 18.0 MiB so far)\n",
      "25/11/26 01:10:12 WARN MemoryStore: Not enough space to cache rdd_500_6 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:12 WARN MemoryStore: Not enough space to cache rdd_500_7 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:12 WARN MemoryStore: Not enough space to cache rdd_500_3 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:13 WARN MemoryStore: Not enough space to cache rdd_500_4 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:13 WARN MemoryStore: Not enough space to cache rdd_500_5 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:13 WARN MemoryStore: Not enough space to cache rdd_500_9 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:13 WARN MemoryStore: Not enough space to cache rdd_500_1 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:13 WARN MemoryStore: Not enough space to cache rdd_500_8 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:12 WARN MemoryStore: Not enough space to cache rdd_500_0 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:12 WARN MemoryStore: Not enough space to cache rdd_500_2 in memory! (computed 18.0 MiB so far)\n",
      "25/11/26 01:10:12 WARN MemoryStore: Not enough space to cache rdd_500_6 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:12 WARN MemoryStore: Not enough space to cache rdd_500_7 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:12 WARN MemoryStore: Not enough space to cache rdd_500_3 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:13 WARN MemoryStore: Not enough space to cache rdd_500_4 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:13 WARN MemoryStore: Not enough space to cache rdd_500_5 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:13 WARN MemoryStore: Not enough space to cache rdd_500_9 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:13 WARN MemoryStore: Not enough space to cache rdd_500_1 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:13 WARN MemoryStore: Not enough space to cache rdd_500_8 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:17 WARN MemoryStore: Not enough space to cache rdd_500_13 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:17 WARN MemoryStore: Not enough space to cache rdd_500_16 in memory! (computed 12.0 MiB so far)\n",
      "25/11/26 01:10:17 WARN MemoryStore: Not enough space to cache rdd_500_17 in memory! (computed 5.3 MiB so far)\n",
      "25/11/26 01:10:17 WARN MemoryStore: Not enough space to cache rdd_500_18 in memory! (computed 3.4 MiB so far)\n",
      "25/11/26 01:10:17 WARN MemoryStore: Not enough space to cache rdd_500_15 in memory! (computed 18.0 MiB so far)\n",
      "25/11/26 01:10:17 WARN MemoryStore: Not enough space to cache rdd_500_12 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:10:17 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_500_19 in memory.\n",
      "25/11/26 01:10:17 WARN MemoryStore: Not enough space to cache rdd_500_19 in memory! (computed 504.0 B so far)\n",
      "25/11/26 01:10:17 WARN MemoryStore: Not enough space to cache rdd_500_14 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:17 WARN MemoryStore: Not enough space to cache rdd_500_13 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:17 WARN MemoryStore: Not enough space to cache rdd_500_16 in memory! (computed 12.0 MiB so far)\n",
      "25/11/26 01:10:17 WARN MemoryStore: Not enough space to cache rdd_500_17 in memory! (computed 5.3 MiB so far)\n",
      "25/11/26 01:10:17 WARN MemoryStore: Not enough space to cache rdd_500_18 in memory! (computed 3.4 MiB so far)\n",
      "25/11/26 01:10:17 WARN MemoryStore: Not enough space to cache rdd_500_15 in memory! (computed 18.0 MiB so far)\n",
      "25/11/26 01:10:17 WARN MemoryStore: Not enough space to cache rdd_500_12 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:10:17 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_500_19 in memory.\n",
      "25/11/26 01:10:17 WARN MemoryStore: Not enough space to cache rdd_500_19 in memory! (computed 504.0 B so far)\n",
      "25/11/26 01:10:17 WARN MemoryStore: Not enough space to cache rdd_500_14 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:17 WARN MemoryStore: Not enough space to cache rdd_500_10 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:17 WARN MemoryStore: Not enough space to cache rdd_500_11 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:17 WARN MemoryStore: Not enough space to cache rdd_500_10 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:17 WARN MemoryStore: Not enough space to cache rdd_500_11 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:21 WARN MemoryStore: Not enough space to cache rdd_500_24 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:21 WARN MemoryStore: Not enough space to cache rdd_500_24 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:21 WARN MemoryStore: Not enough space to cache rdd_500_21 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:21 WARN MemoryStore: Not enough space to cache rdd_500_22 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:21 WARN MemoryStore: Not enough space to cache rdd_500_20 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:10:21 WARN MemoryStore: Not enough space to cache rdd_500_21 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:21 WARN MemoryStore: Not enough space to cache rdd_500_22 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:21 WARN MemoryStore: Not enough space to cache rdd_500_20 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:10:28 WARN MemoryStore: Not enough space to cache rdd_500_1 in memory! (computed 18.0 MiB so far)\n",
      "25/11/26 01:10:28 WARN MemoryStore: Not enough space to cache rdd_500_2 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:28 WARN MemoryStore: Not enough space to cache rdd_500_4 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:28 WARN MemoryStore: Not enough space to cache rdd_500_7 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:28 WARN MemoryStore: Not enough space to cache rdd_500_0 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:28 WARN MemoryStore: Not enough space to cache rdd_500_8 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:28 WARN MemoryStore: Not enough space to cache rdd_500_5 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:28 WARN MemoryStore: Not enough space to cache rdd_500_9 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:28 WARN MemoryStore: Not enough space to cache rdd_500_3 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:28 WARN MemoryStore: Not enough space to cache rdd_500_6 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:28 WARN MemoryStore: Not enough space to cache rdd_500_1 in memory! (computed 18.0 MiB so far)\n",
      "25/11/26 01:10:28 WARN MemoryStore: Not enough space to cache rdd_500_2 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:28 WARN MemoryStore: Not enough space to cache rdd_500_4 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:28 WARN MemoryStore: Not enough space to cache rdd_500_7 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:28 WARN MemoryStore: Not enough space to cache rdd_500_0 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:28 WARN MemoryStore: Not enough space to cache rdd_500_8 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:28 WARN MemoryStore: Not enough space to cache rdd_500_5 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:28 WARN MemoryStore: Not enough space to cache rdd_500_9 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:28 WARN MemoryStore: Not enough space to cache rdd_500_3 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:28 WARN MemoryStore: Not enough space to cache rdd_500_6 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:33 WARN MemoryStore: Not enough space to cache rdd_500_12 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:10:33 WARN MemoryStore: Not enough space to cache rdd_500_16 in memory! (computed 5.3 MiB so far)\n",
      "25/11/26 01:10:33 WARN MemoryStore: Not enough space to cache rdd_500_12 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:10:33 WARN MemoryStore: Not enough space to cache rdd_500_16 in memory! (computed 5.3 MiB so far)\n",
      "25/11/26 01:10:33 WARN MemoryStore: Not enough space to cache rdd_500_14 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:33 WARN MemoryStore: Not enough space to cache rdd_500_15 in memory! (computed 18.0 MiB so far)\n",
      "25/11/26 01:10:33 WARN MemoryStore: Not enough space to cache rdd_500_13 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:10:33 WARN MemoryStore: Not enough space to cache rdd_500_10 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:33 WARN MemoryStore: Not enough space to cache rdd_500_17 in memory! (computed 1549.4 KiB so far)\n",
      "25/11/26 01:10:33 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_500_18 in memory.\n",
      "25/11/26 01:10:33 WARN MemoryStore: Not enough space to cache rdd_500_18 in memory! (computed 504.0 B so far)\n",
      "25/11/26 01:10:33 WARN MemoryStore: Not enough space to cache rdd_500_11 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:33 WARN MemoryStore: Not enough space to cache rdd_500_14 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:33 WARN MemoryStore: Not enough space to cache rdd_500_15 in memory! (computed 18.0 MiB so far)\n",
      "25/11/26 01:10:33 WARN MemoryStore: Not enough space to cache rdd_500_13 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:10:33 WARN MemoryStore: Not enough space to cache rdd_500_10 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:33 WARN MemoryStore: Not enough space to cache rdd_500_17 in memory! (computed 1549.4 KiB so far)\n",
      "25/11/26 01:10:33 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_500_18 in memory.\n",
      "25/11/26 01:10:33 WARN MemoryStore: Not enough space to cache rdd_500_18 in memory! (computed 504.0 B so far)\n",
      "25/11/26 01:10:33 WARN MemoryStore: Not enough space to cache rdd_500_11 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:33 WARN MemoryStore: Not enough space to cache rdd_500_19 in memory! (computed 5.3 MiB so far)\n",
      "25/11/26 01:10:33 WARN MemoryStore: Not enough space to cache rdd_500_19 in memory! (computed 5.3 MiB so far)\n",
      "25/11/26 01:10:37 WARN MemoryStore: Not enough space to cache rdd_500_22 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:37 WARN MemoryStore: Not enough space to cache rdd_500_21 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:37 WARN MemoryStore: Not enough space to cache rdd_500_20 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:37 WARN MemoryStore: Not enough space to cache rdd_500_24 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:37 WARN MemoryStore: Not enough space to cache rdd_500_22 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:37 WARN MemoryStore: Not enough space to cache rdd_500_21 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:37 WARN MemoryStore: Not enough space to cache rdd_500_20 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:37 WARN MemoryStore: Not enough space to cache rdd_500_24 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:44 WARN MemoryStore: Not enough space to cache rdd_500_1 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:44 WARN MemoryStore: Not enough space to cache rdd_500_4 in memory! (computed 18.0 MiB so far)\n",
      "25/11/26 01:10:44 WARN MemoryStore: Not enough space to cache rdd_500_9 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:44 WARN MemoryStore: Not enough space to cache rdd_500_0 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:44 WARN MemoryStore: Not enough space to cache rdd_500_8 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:44 WARN MemoryStore: Not enough space to cache rdd_500_6 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:44 WARN MemoryStore: Not enough space to cache rdd_500_3 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:44 WARN MemoryStore: Not enough space to cache rdd_500_7 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:44 WARN MemoryStore: Not enough space to cache rdd_500_2 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:44 WARN MemoryStore: Not enough space to cache rdd_500_5 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:44 WARN MemoryStore: Not enough space to cache rdd_500_1 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:44 WARN MemoryStore: Not enough space to cache rdd_500_4 in memory! (computed 18.0 MiB so far)\n",
      "25/11/26 01:10:44 WARN MemoryStore: Not enough space to cache rdd_500_9 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:44 WARN MemoryStore: Not enough space to cache rdd_500_0 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:44 WARN MemoryStore: Not enough space to cache rdd_500_8 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:44 WARN MemoryStore: Not enough space to cache rdd_500_6 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:44 WARN MemoryStore: Not enough space to cache rdd_500_3 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:44 WARN MemoryStore: Not enough space to cache rdd_500_7 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:44 WARN MemoryStore: Not enough space to cache rdd_500_2 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:44 WARN MemoryStore: Not enough space to cache rdd_500_5 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:48 WARN MemoryStore: Not enough space to cache rdd_500_14 in memory! (computed 3.4 MiB so far)\n",
      "25/11/26 01:10:48 WARN MemoryStore: Not enough space to cache rdd_500_15 in memory! (computed 5.3 MiB so far)\n",
      "25/11/26 01:10:48 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_500_17 in memory.\n",
      "25/11/26 01:10:48 WARN MemoryStore: Not enough space to cache rdd_500_17 in memory! (computed 504.0 B so far)\n",
      "25/11/26 01:10:48 WARN MemoryStore: Not enough space to cache rdd_500_13 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:48 WARN MemoryStore: Not enough space to cache rdd_500_11 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:48 WARN MemoryStore: Not enough space to cache rdd_500_16 in memory! (computed 1549.4 KiB so far)\n",
      "25/11/26 01:10:48 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_500_18 in memory.\n",
      "25/11/26 01:10:48 WARN MemoryStore: Not enough space to cache rdd_500_18 in memory! (computed 504.0 B so far)\n",
      "25/11/26 01:10:49 WARN MemoryStore: Not enough space to cache rdd_500_12 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:49 WARN MemoryStore: Not enough space to cache rdd_500_19 in memory! (computed 1549.4 KiB so far)\n",
      "25/11/26 01:10:49 WARN MemoryStore: Not enough space to cache rdd_500_10 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:10:48 WARN MemoryStore: Not enough space to cache rdd_500_14 in memory! (computed 3.4 MiB so far)\n",
      "25/11/26 01:10:48 WARN MemoryStore: Not enough space to cache rdd_500_15 in memory! (computed 5.3 MiB so far)\n",
      "25/11/26 01:10:48 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_500_17 in memory.\n",
      "25/11/26 01:10:48 WARN MemoryStore: Not enough space to cache rdd_500_17 in memory! (computed 504.0 B so far)\n",
      "25/11/26 01:10:48 WARN MemoryStore: Not enough space to cache rdd_500_13 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:48 WARN MemoryStore: Not enough space to cache rdd_500_11 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:48 WARN MemoryStore: Not enough space to cache rdd_500_16 in memory! (computed 1549.4 KiB so far)\n",
      "25/11/26 01:10:48 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_500_18 in memory.\n",
      "25/11/26 01:10:48 WARN MemoryStore: Not enough space to cache rdd_500_18 in memory! (computed 504.0 B so far)\n",
      "25/11/26 01:10:49 WARN MemoryStore: Not enough space to cache rdd_500_12 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:49 WARN MemoryStore: Not enough space to cache rdd_500_19 in memory! (computed 1549.4 KiB so far)\n",
      "25/11/26 01:10:49 WARN MemoryStore: Not enough space to cache rdd_500_10 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:10:53 WARN MemoryStore: Not enough space to cache rdd_500_21 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:54 WARN MemoryStore: Not enough space to cache rdd_500_22 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:54 WARN MemoryStore: Not enough space to cache rdd_500_24 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:54 WARN MemoryStore: Not enough space to cache rdd_500_20 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:10:53 WARN MemoryStore: Not enough space to cache rdd_500_21 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:54 WARN MemoryStore: Not enough space to cache rdd_500_22 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:10:54 WARN MemoryStore: Not enough space to cache rdd_500_24 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:10:54 WARN MemoryStore: Not enough space to cache rdd_500_20 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:11:02 WARN MemoryStore: Not enough space to cache rdd_500_2 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:11:02 WARN MemoryStore: Not enough space to cache rdd_500_6 in memory! (computed 18.0 MiB so far)\n",
      "25/11/26 01:11:02 WARN MemoryStore: Not enough space to cache rdd_500_2 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:11:02 WARN MemoryStore: Not enough space to cache rdd_500_6 in memory! (computed 18.0 MiB so far)\n",
      "25/11/26 01:11:02 WARN MemoryStore: Not enough space to cache rdd_500_1 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:11:02 WARN MemoryStore: Not enough space to cache rdd_500_3 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:11:02 WARN MemoryStore: Not enough space to cache rdd_500_4 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:11:02 WARN MemoryStore: Not enough space to cache rdd_500_5 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:11:02 WARN MemoryStore: Not enough space to cache rdd_500_9 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:11:02 WARN MemoryStore: Not enough space to cache rdd_500_8 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:11:02 WARN MemoryStore: Not enough space to cache rdd_500_7 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:11:02 WARN MemoryStore: Not enough space to cache rdd_500_0 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:11:02 WARN MemoryStore: Not enough space to cache rdd_500_1 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:11:02 WARN MemoryStore: Not enough space to cache rdd_500_3 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:11:02 WARN MemoryStore: Not enough space to cache rdd_500_4 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:11:02 WARN MemoryStore: Not enough space to cache rdd_500_5 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:11:02 WARN MemoryStore: Not enough space to cache rdd_500_9 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:11:02 WARN MemoryStore: Not enough space to cache rdd_500_8 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:11:02 WARN MemoryStore: Not enough space to cache rdd_500_7 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:11:02 WARN MemoryStore: Not enough space to cache rdd_500_0 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:11:06 WARN MemoryStore: Not enough space to cache rdd_500_13 in memory! (computed 12.0 MiB so far)\n",
      "25/11/26 01:11:06 WARN MemoryStore: Not enough space to cache rdd_500_17 in memory! (computed 12.0 MiB so far)\n",
      "25/11/26 01:11:06 WARN MemoryStore: Not enough space to cache rdd_500_16 in memory! (computed 18.0 MiB so far)\n",
      "25/11/26 01:11:06 WARN MemoryStore: Not enough space to cache rdd_500_19 in memory! (computed 1030.0 KiB so far)\n",
      "25/11/26 01:11:06 WARN MemoryStore: Not enough space to cache rdd_500_14 in memory! (computed 18.0 MiB so far)\n",
      "25/11/26 01:11:06 WARN MemoryStore: Not enough space to cache rdd_500_11 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:06 WARN MemoryStore: Not enough space to cache rdd_500_15 in memory! (computed 18.0 MiB so far)\n",
      "25/11/26 01:11:06 WARN MemoryStore: Not enough space to cache rdd_500_18 in memory! (computed 5.3 MiB so far)\n",
      "25/11/26 01:11:06 WARN MemoryStore: Not enough space to cache rdd_500_12 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:06 WARN MemoryStore: Not enough space to cache rdd_500_10 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:11:06 WARN MemoryStore: Not enough space to cache rdd_500_13 in memory! (computed 12.0 MiB so far)\n",
      "25/11/26 01:11:06 WARN MemoryStore: Not enough space to cache rdd_500_17 in memory! (computed 12.0 MiB so far)\n",
      "25/11/26 01:11:06 WARN MemoryStore: Not enough space to cache rdd_500_16 in memory! (computed 18.0 MiB so far)\n",
      "25/11/26 01:11:06 WARN MemoryStore: Not enough space to cache rdd_500_19 in memory! (computed 1030.0 KiB so far)\n",
      "25/11/26 01:11:06 WARN MemoryStore: Not enough space to cache rdd_500_14 in memory! (computed 18.0 MiB so far)\n",
      "25/11/26 01:11:06 WARN MemoryStore: Not enough space to cache rdd_500_11 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:06 WARN MemoryStore: Not enough space to cache rdd_500_15 in memory! (computed 18.0 MiB so far)\n",
      "25/11/26 01:11:06 WARN MemoryStore: Not enough space to cache rdd_500_18 in memory! (computed 5.3 MiB so far)\n",
      "25/11/26 01:11:06 WARN MemoryStore: Not enough space to cache rdd_500_12 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:06 WARN MemoryStore: Not enough space to cache rdd_500_10 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:11:11 WARN MemoryStore: Not enough space to cache rdd_500_22 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:11 WARN MemoryStore: Not enough space to cache rdd_500_20 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:11:11 WARN MemoryStore: Not enough space to cache rdd_500_24 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:11:11 WARN MemoryStore: Not enough space to cache rdd_500_21 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:11:11 WARN MemoryStore: Not enough space to cache rdd_500_22 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:11 WARN MemoryStore: Not enough space to cache rdd_500_20 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:11:11 WARN MemoryStore: Not enough space to cache rdd_500_24 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:11:11 WARN MemoryStore: Not enough space to cache rdd_500_21 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:11:19 WARN MemoryStore: Not enough space to cache rdd_500_4 in memory! (computed 18.0 MiB so far)\n",
      "25/11/26 01:11:19 WARN MemoryStore: Not enough space to cache rdd_500_1 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:19 WARN MemoryStore: Not enough space to cache rdd_500_5 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:19 WARN MemoryStore: Not enough space to cache rdd_500_0 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:19 WARN MemoryStore: Not enough space to cache rdd_500_8 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:19 WARN MemoryStore: Not enough space to cache rdd_500_9 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:19 WARN MemoryStore: Not enough space to cache rdd_500_7 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:19 WARN MemoryStore: Not enough space to cache rdd_500_2 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:19 WARN MemoryStore: Not enough space to cache rdd_500_3 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:19 WARN MemoryStore: Not enough space to cache rdd_500_4 in memory! (computed 18.0 MiB so far)\n",
      "25/11/26 01:11:19 WARN MemoryStore: Not enough space to cache rdd_500_1 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:19 WARN MemoryStore: Not enough space to cache rdd_500_5 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:19 WARN MemoryStore: Not enough space to cache rdd_500_0 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:19 WARN MemoryStore: Not enough space to cache rdd_500_8 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:19 WARN MemoryStore: Not enough space to cache rdd_500_9 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:19 WARN MemoryStore: Not enough space to cache rdd_500_7 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:19 WARN MemoryStore: Not enough space to cache rdd_500_2 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:19 WARN MemoryStore: Not enough space to cache rdd_500_3 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:19 WARN MemoryStore: Not enough space to cache rdd_500_6 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:19 WARN MemoryStore: Not enough space to cache rdd_500_6 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:23 WARN MemoryStore: Not enough space to cache rdd_500_15 in memory! (computed 18.0 MiB so far)\n",
      "25/11/26 01:11:23 WARN MemoryStore: Not enough space to cache rdd_500_16 in memory! (computed 18.0 MiB so far)\n",
      "25/11/26 01:11:23 WARN MemoryStore: Not enough space to cache rdd_500_10 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:11:23 WARN MemoryStore: Not enough space to cache rdd_500_18 in memory! (computed 1030.0 KiB so far)\n",
      "25/11/26 01:11:23 WARN MemoryStore: Not enough space to cache rdd_500_17 in memory! (computed 12.0 MiB so far)\n",
      "25/11/26 01:11:23 WARN MemoryStore: Not enough space to cache rdd_500_13 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:23 WARN MemoryStore: Not enough space to cache rdd_500_12 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:23 WARN MemoryStore: Not enough space to cache rdd_500_14 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:23 WARN MemoryStore: Not enough space to cache rdd_500_11 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:11:23 WARN MemoryStore: Not enough space to cache rdd_500_19 in memory! (computed 1030.0 KiB so far)\n",
      "25/11/26 01:11:23 WARN MemoryStore: Not enough space to cache rdd_500_15 in memory! (computed 18.0 MiB so far)\n",
      "25/11/26 01:11:23 WARN MemoryStore: Not enough space to cache rdd_500_16 in memory! (computed 18.0 MiB so far)\n",
      "25/11/26 01:11:23 WARN MemoryStore: Not enough space to cache rdd_500_10 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:11:23 WARN MemoryStore: Not enough space to cache rdd_500_18 in memory! (computed 1030.0 KiB so far)\n",
      "25/11/26 01:11:23 WARN MemoryStore: Not enough space to cache rdd_500_17 in memory! (computed 12.0 MiB so far)\n",
      "25/11/26 01:11:23 WARN MemoryStore: Not enough space to cache rdd_500_13 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:23 WARN MemoryStore: Not enough space to cache rdd_500_12 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:23 WARN MemoryStore: Not enough space to cache rdd_500_14 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:23 WARN MemoryStore: Not enough space to cache rdd_500_11 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:11:23 WARN MemoryStore: Not enough space to cache rdd_500_19 in memory! (computed 1030.0 KiB so far)\n",
      "25/11/26 01:11:27 WARN MemoryStore: Not enough space to cache rdd_500_21 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:11:27 WARN MemoryStore: Not enough space to cache rdd_500_22 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:27 WARN MemoryStore: Not enough space to cache rdd_500_23 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:27 WARN MemoryStore: Not enough space to cache rdd_500_20 in memory! (computed 144.4 MiB so far)\n",
      "25/11/26 01:11:28 WARN MemoryStore: Not enough space to cache rdd_500_24 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:27 WARN MemoryStore: Not enough space to cache rdd_500_21 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:11:27 WARN MemoryStore: Not enough space to cache rdd_500_22 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:27 WARN MemoryStore: Not enough space to cache rdd_500_23 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:27 WARN MemoryStore: Not enough space to cache rdd_500_20 in memory! (computed 144.4 MiB so far)\n",
      "25/11/26 01:11:28 WARN MemoryStore: Not enough space to cache rdd_500_24 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:35 WARN MemoryStore: Not enough space to cache rdd_500_3 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:35 WARN MemoryStore: Not enough space to cache rdd_500_2 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:35 WARN MemoryStore: Not enough space to cache rdd_500_5 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:35 WARN MemoryStore: Not enough space to cache rdd_500_7 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:35 WARN MemoryStore: Not enough space to cache rdd_500_9 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:35 WARN MemoryStore: Not enough space to cache rdd_500_4 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:35 WARN MemoryStore: Not enough space to cache rdd_500_0 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:35 WARN MemoryStore: Not enough space to cache rdd_500_8 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:35 WARN MemoryStore: Not enough space to cache rdd_500_1 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:35 WARN MemoryStore: Not enough space to cache rdd_500_6 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:35 WARN MemoryStore: Not enough space to cache rdd_500_3 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:35 WARN MemoryStore: Not enough space to cache rdd_500_2 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:35 WARN MemoryStore: Not enough space to cache rdd_500_5 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:35 WARN MemoryStore: Not enough space to cache rdd_500_7 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:35 WARN MemoryStore: Not enough space to cache rdd_500_9 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:35 WARN MemoryStore: Not enough space to cache rdd_500_4 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:35 WARN MemoryStore: Not enough space to cache rdd_500_0 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:35 WARN MemoryStore: Not enough space to cache rdd_500_8 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:35 WARN MemoryStore: Not enough space to cache rdd_500_1 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:35 WARN MemoryStore: Not enough space to cache rdd_500_6 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:39 WARN MemoryStore: Not enough space to cache rdd_500_13 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:39 WARN MemoryStore: Not enough space to cache rdd_500_12 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:39 WARN MemoryStore: Not enough space to cache rdd_500_18 in memory! (computed 12.0 MiB so far)\n",
      "25/11/26 01:11:39 WARN MemoryStore: Not enough space to cache rdd_500_14 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:39 WARN MemoryStore: Not enough space to cache rdd_500_19 in memory! (computed 12.0 MiB so far)\n",
      "25/11/26 01:11:39 WARN MemoryStore: Not enough space to cache rdd_500_16 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:11:39 WARN MemoryStore: Not enough space to cache rdd_500_15 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:39 WARN MemoryStore: Not enough space to cache rdd_500_17 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:11:39 WARN MemoryStore: Not enough space to cache rdd_500_13 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:39 WARN MemoryStore: Not enough space to cache rdd_500_12 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:39 WARN MemoryStore: Not enough space to cache rdd_500_18 in memory! (computed 12.0 MiB so far)\n",
      "25/11/26 01:11:39 WARN MemoryStore: Not enough space to cache rdd_500_14 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:39 WARN MemoryStore: Not enough space to cache rdd_500_19 in memory! (computed 12.0 MiB so far)\n",
      "25/11/26 01:11:39 WARN MemoryStore: Not enough space to cache rdd_500_16 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:11:39 WARN MemoryStore: Not enough space to cache rdd_500_15 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:39 WARN MemoryStore: Not enough space to cache rdd_500_17 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:11:39 WARN MemoryStore: Not enough space to cache rdd_500_10 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:11:39 WARN MemoryStore: Not enough space to cache rdd_500_11 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:11:39 WARN MemoryStore: Not enough space to cache rdd_500_10 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:11:39 WARN MemoryStore: Not enough space to cache rdd_500_11 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:11:43 WARN MemoryStore: Not enough space to cache rdd_500_22 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:43 WARN MemoryStore: Not enough space to cache rdd_500_24 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:43 WARN MemoryStore: Not enough space to cache rdd_500_23 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:43 WARN MemoryStore: Not enough space to cache rdd_500_21 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:11:43 WARN MemoryStore: Not enough space to cache rdd_500_20 in memory! (computed 144.4 MiB so far)\n",
      "25/11/26 01:11:43 WARN MemoryStore: Not enough space to cache rdd_500_22 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:43 WARN MemoryStore: Not enough space to cache rdd_500_24 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:43 WARN MemoryStore: Not enough space to cache rdd_500_23 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:43 WARN MemoryStore: Not enough space to cache rdd_500_21 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:11:43 WARN MemoryStore: Not enough space to cache rdd_500_20 in memory! (computed 144.4 MiB so far)\n",
      "25/11/26 01:11:50 WARN MemoryStore: Not enough space to cache rdd_500_1 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:50 WARN MemoryStore: Not enough space to cache rdd_500_3 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:50 WARN MemoryStore: Not enough space to cache rdd_500_6 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:50 WARN MemoryStore: Not enough space to cache rdd_500_4 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:50 WARN MemoryStore: Not enough space to cache rdd_500_0 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:50 WARN MemoryStore: Not enough space to cache rdd_500_9 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:50 WARN MemoryStore: Not enough space to cache rdd_500_5 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:50 WARN MemoryStore: Not enough space to cache rdd_500_7 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:50 WARN MemoryStore: Not enough space to cache rdd_500_8 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:50 WARN MemoryStore: Not enough space to cache rdd_500_2 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:50 WARN MemoryStore: Not enough space to cache rdd_500_1 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:50 WARN MemoryStore: Not enough space to cache rdd_500_3 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:50 WARN MemoryStore: Not enough space to cache rdd_500_6 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:50 WARN MemoryStore: Not enough space to cache rdd_500_4 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:50 WARN MemoryStore: Not enough space to cache rdd_500_0 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:50 WARN MemoryStore: Not enough space to cache rdd_500_9 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:50 WARN MemoryStore: Not enough space to cache rdd_500_5 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:50 WARN MemoryStore: Not enough space to cache rdd_500_7 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:50 WARN MemoryStore: Not enough space to cache rdd_500_8 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:50 WARN MemoryStore: Not enough space to cache rdd_500_2 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:54 WARN MemoryStore: Not enough space to cache rdd_500_13 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:54 WARN MemoryStore: Not enough space to cache rdd_500_13 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:54 WARN MemoryStore: Not enough space to cache rdd_500_14 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:54 WARN MemoryStore: Not enough space to cache rdd_500_18 in memory! (computed 12.0 MiB so far)\n",
      "25/11/26 01:11:54 WARN MemoryStore: Not enough space to cache rdd_500_15 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:54 WARN MemoryStore: Not enough space to cache rdd_500_17 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:54 WARN MemoryStore: Not enough space to cache rdd_500_19 in memory! (computed 2.3 MiB so far)\n",
      "25/11/26 01:11:54 WARN MemoryStore: Not enough space to cache rdd_500_10 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:54 WARN MemoryStore: Not enough space to cache rdd_500_12 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:54 WARN MemoryStore: Not enough space to cache rdd_500_11 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:54 WARN MemoryStore: Not enough space to cache rdd_500_16 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:54 WARN MemoryStore: Not enough space to cache rdd_500_14 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:54 WARN MemoryStore: Not enough space to cache rdd_500_18 in memory! (computed 12.0 MiB so far)\n",
      "25/11/26 01:11:54 WARN MemoryStore: Not enough space to cache rdd_500_15 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:54 WARN MemoryStore: Not enough space to cache rdd_500_17 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:11:54 WARN MemoryStore: Not enough space to cache rdd_500_19 in memory! (computed 2.3 MiB so far)\n",
      "25/11/26 01:11:54 WARN MemoryStore: Not enough space to cache rdd_500_10 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:54 WARN MemoryStore: Not enough space to cache rdd_500_12 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:54 WARN MemoryStore: Not enough space to cache rdd_500_11 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:54 WARN MemoryStore: Not enough space to cache rdd_500_16 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:57 WARN MemoryStore: Not enough space to cache rdd_500_23 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:57 WARN MemoryStore: Not enough space to cache rdd_500_24 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:57 WARN MemoryStore: Not enough space to cache rdd_500_21 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:11:57 WARN MemoryStore: Not enough space to cache rdd_500_20 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:11:57 WARN MemoryStore: Not enough space to cache rdd_500_22 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:11:57 WARN MemoryStore: Not enough space to cache rdd_500_23 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:57 WARN MemoryStore: Not enough space to cache rdd_500_24 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:11:57 WARN MemoryStore: Not enough space to cache rdd_500_21 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:11:57 WARN MemoryStore: Not enough space to cache rdd_500_20 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:11:57 WARN MemoryStore: Not enough space to cache rdd_500_22 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:12:03 WARN MemoryStore: Not enough space to cache rdd_500_7 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:12:03 WARN MemoryStore: Not enough space to cache rdd_500_5 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:12:03 WARN MemoryStore: Not enough space to cache rdd_500_1 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:12:03 WARN MemoryStore: Not enough space to cache rdd_500_6 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:12:03 WARN MemoryStore: Not enough space to cache rdd_500_8 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:12:03 WARN MemoryStore: Not enough space to cache rdd_500_2 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:12:03 WARN MemoryStore: Not enough space to cache rdd_500_9 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:12:03 WARN MemoryStore: Not enough space to cache rdd_500_4 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:12:03 WARN MemoryStore: Not enough space to cache rdd_500_0 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:12:03 WARN MemoryStore: Not enough space to cache rdd_500_3 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:12:03 WARN MemoryStore: Not enough space to cache rdd_500_7 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:12:03 WARN MemoryStore: Not enough space to cache rdd_500_5 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:12:03 WARN MemoryStore: Not enough space to cache rdd_500_1 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:12:03 WARN MemoryStore: Not enough space to cache rdd_500_6 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:12:03 WARN MemoryStore: Not enough space to cache rdd_500_8 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:12:03 WARN MemoryStore: Not enough space to cache rdd_500_2 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:12:03 WARN MemoryStore: Not enough space to cache rdd_500_9 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:12:03 WARN MemoryStore: Not enough space to cache rdd_500_4 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:12:03 WARN MemoryStore: Not enough space to cache rdd_500_0 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:12:03 WARN MemoryStore: Not enough space to cache rdd_500_3 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:12:06 WARN MemoryStore: Not enough space to cache rdd_500_16 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:12:06 WARN MemoryStore: Not enough space to cache rdd_500_12 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:12:06 WARN MemoryStore: Not enough space to cache rdd_500_10 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:12:06 WARN MemoryStore: Not enough space to cache rdd_500_19 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:12:06 WARN MemoryStore: Not enough space to cache rdd_500_17 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:12:06 WARN MemoryStore: Not enough space to cache rdd_500_14 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:12:06 WARN MemoryStore: Not enough space to cache rdd_500_15 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:12:06 WARN MemoryStore: Not enough space to cache rdd_500_18 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:12:06 WARN MemoryStore: Not enough space to cache rdd_500_11 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:12:06 WARN MemoryStore: Not enough space to cache rdd_500_13 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:12:06 WARN MemoryStore: Not enough space to cache rdd_500_16 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:12:06 WARN MemoryStore: Not enough space to cache rdd_500_12 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:12:06 WARN MemoryStore: Not enough space to cache rdd_500_10 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:12:06 WARN MemoryStore: Not enough space to cache rdd_500_19 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:12:06 WARN MemoryStore: Not enough space to cache rdd_500_17 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:12:06 WARN MemoryStore: Not enough space to cache rdd_500_14 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:12:06 WARN MemoryStore: Not enough space to cache rdd_500_15 in memory! (computed 42.8 MiB so far)\n",
      "25/11/26 01:12:06 WARN MemoryStore: Not enough space to cache rdd_500_18 in memory! (computed 27.0 MiB so far)\n",
      "25/11/26 01:12:06 WARN MemoryStore: Not enough space to cache rdd_500_11 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:12:06 WARN MemoryStore: Not enough space to cache rdd_500_13 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:12:09 WARN MemoryStore: Not enough space to cache rdd_500_23 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:12:09 WARN MemoryStore: Not enough space to cache rdd_500_24 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:12:09 WARN MemoryStore: Not enough space to cache rdd_500_22 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:12:09 WARN MemoryStore: Not enough space to cache rdd_500_20 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:12:09 WARN MemoryStore: Not enough space to cache rdd_500_21 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:12:09 WARN MemoryStore: Not enough space to cache rdd_500_23 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:12:09 WARN MemoryStore: Not enough space to cache rdd_500_24 in memory! (computed 64.2 MiB so far)\n",
      "25/11/26 01:12:09 WARN MemoryStore: Not enough space to cache rdd_500_22 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:12:09 WARN MemoryStore: Not enough space to cache rdd_500_20 in memory! (computed 96.3 MiB so far)\n",
      "25/11/26 01:12:09 WARN MemoryStore: Not enough space to cache rdd_500_21 in memory! (computed 96.3 MiB so far)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved to: data/PART_5_fill_with_more_data/salary_rf_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 224:====================================================>  (24 + 1) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test [How much trained data is off by]: 2219722012191425.25\n",
      "Test [R^2 (accuracy) of the model]: 0.272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import FeatureHasher, VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "ml_df = gold_detail_df\n",
    "\n",
    "# same like above, even though we shouldn't have all these columns as we dropped them or combined them before, we still going to check for them just in case something happened.\n",
    "if \"occ_title\" in ml_df.columns and \"job_role\" not in ml_df.columns: ml_df = ml_df.withColumnRenamed(\"occ_title\", \"job_role\")\n",
    "if \"jobposition\" in ml_df.columns and \"job_role\" not in ml_df.columns: ml_df = ml_df.withColumnRenamed(\"jobposition\", \"job_role\")\n",
    "if \"companylocation\" in ml_df.columns and \"location\" not in ml_df.columns: ml_df = ml_df.withColumnRenamed(\"companylocation\", \"location\")\n",
    "if \"employeeresidence\" in ml_df.columns and \"location\" not in ml_df.columns: ml_df = ml_df.withColumnRenamed(\"employeeresidence\", \"location\")\n",
    "if \"fiscal_year\" in ml_df.columns and \"year\" not in ml_df.columns: ml_df = ml_df.withColumnRenamed(\"fiscal_year\", \"year\")\n",
    "if \"senioritylevel\" in ml_df.columns and \"seniority_level\" not in ml_df.columns: ml_df = ml_df.withColumnRenamed(\"senioritylevel\", \"seniority_level\")\n",
    "if \"experienceyears\" in ml_df.columns and \"experience_years\" not in ml_df.columns: ml_df = ml_df.withColumnRenamed(\"experienceyears\", \"experience_years\")\n",
    "\n",
    "# we fill missing values in case we have any\n",
    "if \"seniority_level\" in ml_df.columns: ml_df = ml_df.fillna({\"seniority_level\": \"Unknown\"})\n",
    "if \"experience_years\" in ml_df.columns: ml_df = ml_df.withColumn(\"experience_years\", F.col(\"experience_years\").cast(\"double\"))\n",
    "\n",
    "ml_df = ml_df.dropna(subset=[\"job_role\", \"location\", \"year\", \"salary_clean\"])\n",
    "\n",
    "print(\"ML dataframe after dropna:\", ml_df.count())\n",
    "\n",
    "hasher_input_cols = []\n",
    "if \"job_role\" in ml_df.columns: hasher_input_cols.append(\"job_role\")\n",
    "if \"location\" in ml_df.columns: hasher_input_cols.append(\"location\")\n",
    "if \"seniority_level\" in ml_df.columns: hasher_input_cols.append(\"seniority_level\")\n",
    "\n",
    "hasher = FeatureHasher(\n",
    "    inputCols=hasher_input_cols,\n",
    "    outputCol=\"cat_features\",\n",
    "    numFeatures=2048 # 512\n",
    ")\n",
    "\n",
    "assembler_input_cols = []\n",
    "if \"cat_features\" in ml_df.columns: assembler_input_cols.append(\"cat_features\")\n",
    "if \"year\" in ml_df.columns: assembler_input_cols.append(\"year\")\n",
    "if \"experience_years\" in ml_df.columns: assembler_input_cols.append(\"experience_years\")\n",
    "\n",
    "assembler = VectorAssembler(inputCols=assembler_input_cols, outputCol=\"features\")\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"salary_clean\", predictionCol=\"prediction_salary\", numTrees=50, maxDepth=10)\n",
    "pipeline = Pipeline(stages=[hasher, assembler, rf])\n",
    "\n",
    "train_df, test_df = ml_df.randomSplit([0.8, 0.2], seed=42)\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "model_path = \"data/PART_5_fill_with_more_data/salary_rf_model\"\n",
    "model.write().overwrite().save(model_path) # we overwrite any existing model in order to prevent errors about already existing model, I guess another possible way is to either comment it out or use try and except or maybe have a variable to control overwriting\n",
    "print(\"model saved to:\", model_path)\n",
    "\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "evaluator_rmse = RegressionEvaluator(labelCol=\"salary_clean\", predictionCol=\"prediction_salary\", metricName=\"rmse\")\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"salary_clean\", predictionCol=\"prediction_salary\", metricName=\"r2\")\n",
    "\n",
    "trained_off_by = evaluator_rmse.evaluate(predictions)\n",
    "trained_percentage = evaluator_r2.evaluate(predictions)\n",
    "\n",
    "print(f\"Test [How much trained data is off by]: {trained_off_by:.2f}\")\n",
    "print(f\"Test [R^2 (accuracy) of the model]: {trained_percentage:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ab7ad6",
   "metadata": {},
   "source": [
    "We use RMSE and R^2 to tell how well the model is doing\n",
    "\n",
    "RMSE (Root mean squared error) -> trained_off_by --> Lower is better. RMSE basically tells us how much it can be off by.\n",
    "\n",
    "R^2 (Coefficient of determination) -> trained_percentage  --> closer to 1 the better it is. It explains variation in our data by used features.\n",
    "\n",
    "How ML works:\n",
    "- Standardize column names\n",
    "- Feature encoding (featurehasher) --> job_role and location are high-cardinality (basically with lots of unique values) strings --> turns these to numeric vector of length of 512 (cat_features / category_features)\n",
    "- VectorAssembler combines cat_features and year into features\n",
    "- We use RandomForestRegressor in order to learn mapping so that would be from features to salary_clean --> it tries to learn patterns could be that that role + location + year --> x salary.\n",
    "- We train 80% of data and use 20% for testing and then we evaluate using RMSE and R^2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8fd0e385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+----+-----------------+\n",
      "|job_role      |location|year|prediction_salary|\n",
      "+--------------+--------+----+-----------------+\n",
      "|data_scientist|US      |2025|535140.2358435399|\n",
      "+--------------+--------+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Location: ISO code\n",
    "\n",
    "# 'EN': 'Entry-level',\n",
    "# 'MI': 'Mid-level',\n",
    "# 'SE': 'Senior',\n",
    "# 'EX': 'Executive'\n",
    "example_df = spark.createDataFrame([(\"data_scientist\", \"US\", 2025)],[\"job_role\", \"location\", \"year\"])\n",
    "example_pred = model.transform(example_df)\n",
    "example_pred.select(\"job_role\", \"location\", \"year\", \"prediction_salary\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f3c78188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+----+-----------------+\n",
      "|job_role      |location|year|prediction_salary|\n",
      "+--------------+--------+----+-----------------+\n",
      "|data_scientist|US      |2025|535140.2358435399|\n",
      "+--------------+--------+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load from the trained model file\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "\n",
    "loaded_model = PipelineModel.load(\"data/PART_5_fill_with_more_data/salary_rf_model\")\n",
    "\n",
    "example_df = spark.createDataFrame([(\"data_scientist\", \"US\", 2025)],[\"job_role\", \"location\", \"year\"])\n",
    "example_pred = loaded_model.transform(example_df)\n",
    "example_pred.select(\"job_role\", \"location\", \"year\", \"prediction_salary\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
